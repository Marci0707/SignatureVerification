{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5b90538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\"\"\"\n",
    "A transformator class which can be fitted to an online signature and used to transform that signature into \n",
    "a list of global signature features\n",
    "\"\"\"\n",
    "\n",
    "class FeatureAnalyzer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"data shape:\n",
    "        84 <--number of points, first line\n",
    "        2933 5678 31275775 0 1550 710 439  <-- data point [x,y,times stamp, pen down/up , azimuth, altitude, pressure]\n",
    "        2933 5678 31275785 1 1480 770 420\n",
    "        3001 5851 31275795 1 1350 830 433\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, log_progress=False):\n",
    "        self.log_progress = log_progress\n",
    "\n",
    "    def fit(self, data):\n",
    "        features = [\"x\", \"y\", \"t\", \"state\", \"azimuth\", \"altitude\", \"pressure\"]\n",
    "\n",
    "        self.df = pd.DataFrame(data, columns=features)\n",
    "\n",
    "        self.make_time_labels_distinct()\n",
    "\n",
    "        self.df[\"y_d\"] = (self.df[\"y\"] - self.df[\"y\"].shift(1)) / (self.df[\"t\"] - self.df[\"t\"].shift(1))\n",
    "        self.df[\"x_d\"] = (self.df[\"x\"] - self.df[\"x\"].shift(1)) / (self.df[\"t\"] - self.df[\"t\"].shift(1))\n",
    "        self.df[\"y_d_d\"] = (self.df[\"y_d\"] - self.df[\"y_d\"].shift(1)) / (self.df[\"t\"] - self.df[\"t\"].shift(1))\n",
    "        self.df[\"x_d_d\"] = (self.df[\"x_d\"] - self.df[\"x_d\"].shift(1)) / (self.df[\"t\"] - self.df[\"t\"].shift(1))\n",
    "        self.df[\"y_d_d_d\"] = (self.df[\"y_d_d\"] - self.df[\"y_d_d\"].shift(1)) / (self.df[\"t\"] - self.df[\"t\"].shift(1))\n",
    "        self.df[\"x_d_d_d\"] = (self.df[\"x_d_d\"] - self.df[\"x_d_d\"].shift(1)) / (self.df[\"t\"] - self.df[\"t\"].shift(1))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def make_time_labels_distinct(self):\n",
    "        \"\"\"\n",
    "        avoid same timestamps in the data\n",
    "        \"\"\"\n",
    "        same_time_values_as_previous_row = self.df.loc[self.df['t'] == self.df['t'].shift(1)].index.tolist()\n",
    "\n",
    "        self.df[\"id\"] = self.df.index\n",
    "        self.df.loc[self.df['id'].isin(same_time_values_as_previous_row), 't'] += 1\n",
    "        self.df.drop([\"id\"], axis = 1, inplace=True)\n",
    "\n",
    "        if len(self.df.loc[self.df['t'] == self.df['t'].shift(1)].index.tolist()) != 0:\n",
    "            self.make_time_labels_distinct()\n",
    "\n",
    "\n",
    "    def log_progression(self, feature_id):\n",
    "        if self.log_progress:\n",
    "            print(\"calculating feature: %s\", feature_id)\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        :param1: data parameter is not used anymore, the fit function saves the data to self.df which is a pandas Dataframe of the data\n",
    "        returns: a 20-long list of global features\n",
    "        \"\"\"\n",
    "        values = np.zeros(20)\n",
    "\n",
    "        self.log_progression(1)\n",
    "        values[0] = self.calc_duration()\n",
    "        self.log_progression(2)\n",
    "        values[1] = self.calc_pen_ups()\n",
    "        self.log_progression(3)\n",
    "        values[2] = self.calc_sign_changes_in_col(\"x\") + self.calc_sign_changes_in_col(\"y\")\n",
    "        self.log_progression(4)\n",
    "        values[3] = self.calc_avg_jerk()\n",
    "        self.log_progression(5)\n",
    "        values[4] = self.calc_std_dev_in_col(\"y_d_d\")\n",
    "        self.log_progression(6)\n",
    "        values[5] = self.calc_std_dev_in_col(\"y_d\")\n",
    "        self.log_progression(7)\n",
    "        values[6] = self.f7()\n",
    "        self.log_progression(8)\n",
    "        values[7] = self.count_local_maxima_in_col(\"x\")\n",
    "        self.log_progression(9)\n",
    "        values[8] = self.calc_std_dev_in_col(\"x_d_d\")\n",
    "        self.log_progression(10)\n",
    "        values[9] = self.calc_std_dev_in_col(\"x_d\")\n",
    "        self.log_progression(11)\n",
    "        values[10] = self.f11()\n",
    "        self.log_progression(12)\n",
    "        values[11] = self.count_local_maxima_in_col(\"y\")\n",
    "        self.log_progression(13)\n",
    "        values[12] = self.f13()\n",
    "        self.log_progression(14)\n",
    "        values[13] = self.f14()\n",
    "        self.log_progression(15)\n",
    "        values[14] = self.f15()\n",
    "        self.log_progression(\"16 and 18\")\n",
    "        values[15], values[17] = self.f16_and_18()\n",
    "        self.log_progression(\"17 and 19\")\n",
    "        values[16], values[18] = self.f17_and_f19()\n",
    "        self.log_progression(20)\n",
    "        values[19] = self.f22()\n",
    "\n",
    "        return values\n",
    "\n",
    "    def calc_duration(self):\n",
    "        return self.df[\"t\"].iloc[-1] - self.df[\"t\"].iloc[0]\n",
    "\n",
    "    def calc_avg_jerk(self):\n",
    "        self.df[\"abs_jerk\"] = np.sqrt(self.df[\"x_d_d_d\"] * self.df[\"x_d_d_d\"] + self.df[\"y_d_d_d\"] * self.df[\"y_d_d_d\"])\n",
    "        mean = self.df[\"abs_jerk\"].mean()\n",
    "        return mean\n",
    "\n",
    "    def calc_std_dev_in_col(self, col_name):\n",
    "        return self.df[col_name].std()\n",
    "\n",
    "    def count_local_maxima_in_col(self, col_name):\n",
    "        c = 0\n",
    "        col = self.df[col_name]\n",
    "        for i in range(1, len(col) - 1):\n",
    "            if col.iloc[i + 1] < col.iloc[i] > col.iloc[i - 1]:\n",
    "                c += 1\n",
    "        if col.iloc[0] > col.iloc[1]:\n",
    "            c += 1\n",
    "        if col.iloc[-1] > col.iloc[-2]:\n",
    "            c += 2\n",
    "        return c\n",
    "\n",
    "    def f11(self):  # Jerk rms\n",
    "        return (self.df[\"abs_jerk\"] ** 2).mean() ** 0.5\n",
    "\n",
    "    def f13(self):  # t(2nd pen_down) / duration\n",
    "        pen_downs_found = 0\n",
    "        for row_idx in range(len(self.df)):\n",
    "            if self.df[\"state\"].iloc[row_idx] == 0:\n",
    "                pen_downs_found += 1\n",
    "            if pen_downs_found == 2:\n",
    "                return self.df[\"t\"].iloc[row_idx] / self.calc_duration()\n",
    "\n",
    "        return 1  # just in case\n",
    "\n",
    "    def f14(self):  # (avg abs(velocity)) / (maximum of Vx)\n",
    "        return ((self.df[\"x_d\"].iloc[1:] ** 2 + self.df[\"y_d\"].iloc[1:] ** 2) ** 0.5).mean() / self.df[\"x_d\"].iloc[\n",
    "                                                                                               1:].max()\n",
    "\n",
    "    def delta(self, col_idx):\n",
    "        sum_ = 0\n",
    "        state_idx = 3\n",
    "        curr_max = self.df.loc[1][col_idx]\n",
    "        curr_min = self.df.loc[1][col_idx]\n",
    "        for record in self.df.iterrows():\n",
    "            if record[1][state_idx] == 0:\n",
    "                sum_ += curr_max - curr_min\n",
    "                curr_max = -np.inf\n",
    "                curr_min = np.inf\n",
    "            if record[1][col_idx] > curr_max:\n",
    "                curr_max = record[1][col_idx]\n",
    "            if record[1][col_idx] < curr_min:\n",
    "                curr_min = record[1][col_idx]\n",
    "        \n",
    "        if sum_== 0:\n",
    "            sum_= 1\n",
    "        return sum_\n",
    "\n",
    "    def f15(self):\n",
    "        A_min = (self.df['y'].max() - self.df['y'].min()) * (self.df['x'].max() - self.df['x'].min())\n",
    "        denominator = self.delta(0) * self.delta(1)\n",
    "        return A_min / denominator\n",
    "\n",
    "    def f7(self):\n",
    "        return self.calc_std_dev_in_col('y') / self.delta(1)\n",
    "\n",
    "    def f16_and_18(self):\n",
    "        for i in range(len(self.df) - 1, -1, -1):\n",
    "            if self.df['state'].iloc[i] == 0:\n",
    "                x_last_penup = self.df['x'].iloc[i]\n",
    "                y_last_penup = self.df['y'].iloc[i]\n",
    "                x_max = self.df['x'].max()\n",
    "                y_min = self.df['y'].min()\n",
    "                f16 = (x_last_penup - x_max) / self.delta(0)\n",
    "                f18 = (y_last_penup - y_min) / self.delta(1)\n",
    "                return f16, f18\n",
    "\n",
    "    def f17_and_f19(self):\n",
    "        for i in range(len(self.df)):\n",
    "            if self.df['state'].iloc[i] == 1:\n",
    "                x_first_pendown = self.df['x'].iloc[i]\n",
    "                y_first_pendown = self.df['y'].iloc[i]\n",
    "                x_max = self.df['x'].max()\n",
    "                y_min = self.df['y'].min()\n",
    "                f17 = (x_first_pendown - x_max) / self.delta(0)\n",
    "                f19 = (y_first_pendown - y_min) / self.delta(1)\n",
    "                return f17, f19\n",
    "\n",
    "    def f22(self):\n",
    "        indices = self.df.index[self.df['state'] == 0].tolist()\n",
    "        pendown_duration = 0\n",
    "        for i in indices:\n",
    "            pendown_duration += self.df['t'].loc[i + 1] - self.df['t'].loc[i]\n",
    "\n",
    "        return pendown_duration / self.calc_duration()\n",
    "\n",
    "    def calc_sign_changes_in_col(self, col_name):\n",
    "        col = self.df[col_name]\n",
    "        c = 0\n",
    "        for i in range(1, len(col) - 1):\n",
    "            if col[i] < col[i + 1] != col[i - 1] < col[i]:\n",
    "                c += 1\n",
    "        return c\n",
    "\n",
    "    def get_feature_list(self):\n",
    "        return self.analyze()\n",
    "\n",
    "    def calc_pen_ups(self):\n",
    "        return len(self.df.index[self.df['state'] == 0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc46af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_user_data(user_id):\n",
    "    #takes a user id and reads it into an array from the folder /SVC2004_Online\n",
    "    all_signatures = np.zeros(40, object)\n",
    "\n",
    "    for signature_id in range(1, 41):\n",
    "        filename = \"U\" + str(user_id) + \"S\" + str(signature_id) + \".txt\"\n",
    "        path = os.path.join(os.getcwd(), \"SVC2004_Online\", filename)\n",
    "\n",
    "        with open(path) as f:\n",
    "            lines = (line for line in f)\n",
    "            sig_data = np.loadtxt(lines, delimiter=' ', skiprows=1)\n",
    "\n",
    "            all_signatures[signature_id - 1] = sig_data\n",
    "\n",
    "    return np.array(all_signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84c0c69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cluster_feature_vectors(number_of_global_features, feature_vectors):\n",
    "    #clusters feature vectors using Kmeans and returns the clusterer object which stores and clusters \n",
    "    clusterer = KMeans(n_clusters=int(number_of_global_features * 0.6))\n",
    "    clusterer.fit(feature_vectors)\n",
    "\n",
    "    return clusterer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "133e7325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from FeatureAnalyzer import FeatureAnalyzer\n",
    "from model import create_model\n",
    "import os\n",
    "\n",
    "number_of_global_features = 20 #that I calculate and use for training\n",
    "\n",
    "def get_signatures_and_labels(user_id):\n",
    "    #reads a user ID and returns the data and the labels for supervised learning\n",
    "    user_signatures = fetch_user_data(user_id)\n",
    "\n",
    "    labels = np.array([x // 20 for x in range(40)]).astype(\"float32\")  # first 20 valid, second 20 forgery. forgery = 1\n",
    "\n",
    "    return user_signatures,labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b30796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_signature_features(user_signatures):\n",
    "    #calculates the global features of the signature and returns them in a 20-long array\n",
    "    analyzer = FeatureAnalyzer()\n",
    "    signature_features = np.zeros(shape=(len(user_signatures), number_of_global_features))\n",
    "    for idx, sig in enumerate(user_signatures):\n",
    "        signature_features[idx] = analyzer.fit_transform(sig)\n",
    "        \n",
    "    return signature_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9848e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_vectors(signature_features):\n",
    "    #creates the feature vectors described in the source research paper\n",
    "    feature_vectors = signature_features.transpose()\n",
    "    feature_vectors = np.reshape(feature_vectors, (40, 20))\n",
    "    \n",
    "    return feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfe187e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_representatives(clusterer, feature_vectors):\n",
    "    #calculates cluster representatives based on the Meadian of Medians algorithm which is mentioned and used\n",
    "    #in the source research paper\n",
    "    labels = clusterer.labels_\n",
    "    groups = dict()\n",
    "    \n",
    "    for idx, vector in enumerate(feature_vectors):\n",
    "        label = labels[idx]\n",
    "        if label not in groups:\n",
    "            groups[label] = [vector]\n",
    "        else:\n",
    "            groups[label].append(vector)\n",
    "        \n",
    "    cluster_heads = []\n",
    "    for group in groups:\n",
    "        vectors_in_group = groups[group]\n",
    "        \n",
    "        vector_medians = []\n",
    "        for vector in vectors_in_group:\n",
    "            vector_medians = []\n",
    "            for idx,feature in enumerate(vector):\n",
    "                feature_medians = []\n",
    "                for other_feature in vector:\n",
    "                    feature_medians.append(abs(feature-other_feature))\n",
    "                    \n",
    "                feature_median = np.median(feature_medians)\n",
    "                feature_medians.append(feature_median)\n",
    "\n",
    "            vector_median = np.median(feature_medians)\n",
    "            vector_medians.append(vector_median)\n",
    "        \n",
    "        representative = np.median(vector_medians)\n",
    "        cluster_heads.append(representative)\n",
    "    \n",
    "    return cluster_heads\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1236290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clusters_and_representatives(feature_vectors):\n",
    "    #creates the clusters and returns the cluster representatives of the clusters. The representatives are calculated\n",
    "    #using the Median of Medians algorithm\n",
    "    clusterer = cluster_feature_vectors(number_of_global_features, feature_vectors) #12 clusters from 20 vectors\n",
    "    clusterer.labels_\n",
    "\n",
    "    representatives = get_cluster_representatives(clusterer,feature_vectors)\n",
    "    representatives = np.array(representatives)\n",
    "    \n",
    "    return representatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1352ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_vectors(signature_features,representatives):\n",
    "    #Creates the input vectors to be fed to the neural network. It consists of the global features\n",
    "    #plus the feature representatives\n",
    "    input_vectors = np.zeros(shape=(40,32))\n",
    "    for idx,signature_feature in enumerate(signature_features):\n",
    "        input_vectors[idx] = np.append(signature_feature,representatives, axis = 0)\n",
    "\n",
    "    ipnut_vectors = signature_features\n",
    "    \n",
    "    return input_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c81be7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def split_and_scale_data(input_vectors,labels,scale=True):\n",
    "    #data preprocessing. returns 3 splits to be used for training.\n",
    "    X = input_vectors\n",
    "    y = labels\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "    if(scale):\n",
    "        std_scaler = StandardScaler()\n",
    "        X_train[:,:20] = std_scaler.fit_transform(X_train[:,:20])\n",
    "        X_valid[:,:20] = std_scaler.transform(X_valid[:,:20])\n",
    "        X_test[:,:20] = std_scaler.transform(X_test[:,:20])\n",
    "    \n",
    "    return (X_train,y_train), (X_valid,y_valid), (X_test,y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f53ba54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_input(X_train,X_valid,X_test):\n",
    "    #reshaping to match the required shape by the first layer in the model\n",
    "    X_train = np.reshape(X_train,(X_train.shape[0],-1,X_train.shape[1]))\n",
    "    X_valid = np.reshape(X_valid,(X_valid.shape[0],-1,X_valid.shape[1]))\n",
    "    X_test = np.reshape(X_test,(X_test.shape[0],-1,X_test.shape[1]))\n",
    "    \n",
    "    return X_train,X_valid,X_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7bb09cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_of_user(user_id):\n",
    "    #reads a user's data and fully prepares if for training\n",
    "    signatures,labels = get_signatures_and_labels(user_id)\n",
    "\n",
    "    signature_features = generate_signature_features(signatures)\n",
    "\n",
    "    feature_vectors = generate_feature_vectors(signature_features)\n",
    "\n",
    "    representatives = make_clusters_and_representatives(feature_vectors)\n",
    "\n",
    "    input_vectors = get_input_vectors(signature_features,representatives)\n",
    "\n",
    "    (X_train,y_train), (X_valid,y_valid), (X_test,y_test) = split_and_scale_data(input_vectors,labels,scale=True)\n",
    "    X_train, X_valid, X_test = reshape_to_input(X_train, X_valid, X_test)\n",
    "    \n",
    "    return (X_train,y_train), (X_valid,y_valid), (X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47286d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import SeparableConv1D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "#learning rate scheduler for training\n",
    "def scheduler(epoch,lr):\n",
    "    return 0.001\n",
    "    if(epoch < 5000):\n",
    "        return 0.000005\n",
    "    else:\n",
    "        return 0.0000005\n",
    "\n",
    "\n",
    "\n",
    "#return a model with the recommended architecture and calculated hyperparameters\n",
    "def create_model(lr):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(SeparableConv1D(filters=96, kernel_size=7, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(SeparableConv1D(filters=32, kernel_size=5, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(LSTM(96,recurrent_dropout=0.2, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(32, kernel_initializer=\"random_uniform\",bias_initializer=\"random_uniform\",activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.6))\n",
    "\n",
    "    model.add(Dense(64, kernel_initializer=\"random_uniform\",bias_initializer=\"random_uniform\",activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.0))\n",
    "\n",
    "    model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=lr) ,loss=\"binary_crossentropy\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f9191d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_model(user_id, X_train,y_train, X_valid,y_valid, learning_rate=0.001):\n",
    "    #trains a model on a user\n",
    "    #returns the model and the trainig history\n",
    "\n",
    "    model = create_model(learning_rate)\n",
    "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "    early_stopping = EarlyStopping(patience=500, verbose=2,monitor='val_loss', mode='min')\n",
    "    history = model.fit(X_train,y_train, validation_data=(X_valid,y_valid),epochs = 10000,batch_size=8,verbose = 1,\n",
    "                       callbacks=[early_stopping])\n",
    "\n",
    "    \n",
    "    \n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e62438cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt_rows = 7 #for pyplot subplots\n",
    "plt_cols = 7 #for pyplot subplots\n",
    "\n",
    "def evaluate_model(user_id,model,X_test,y_test,history,axs):\n",
    "    #evaluates the model on the test set, prints the results and adds \n",
    "    #the training history to a pyplot subplot\n",
    "    print(\"Evaluating predictions for user\",user_id)\n",
    "    preds = model.predict(X_test)\n",
    "    print(\"prediction for test:\\n\",preds)\n",
    "    print(\"labels:\",y_test)\n",
    "    \n",
    "    err = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    positives = 0\n",
    "    negatives = 0\n",
    "    for idx,pred in enumerate(model.predict(X_test)):\n",
    "        \n",
    "        if y_test[idx] == 1:\n",
    "            positives+=1\n",
    "        else:\n",
    "            negatives+=1\n",
    "        \n",
    "        \n",
    "        if y_test[idx] == 1.0 and pred[0] < 0.5:\n",
    "            false_negatives += 1\n",
    "        elif y_test[idx] == 0.0 and pred[0] > 0.5:\n",
    "            false_positives+=1\n",
    "        \n",
    "        \n",
    "        err+= round(abs(pred[0]-y_test[idx]))\n",
    "    \n",
    "    errs.append(err)\n",
    "    print(\"Misclassified:\",err,\"out of\",len(preds))\n",
    "    \n",
    "    \n",
    "    user_id -=1 #to 0-39 from 1-40\n",
    "    \n",
    "    plt_row = user_id//plt_rows\n",
    "    plt_col = user_id%plt_cols\n",
    "\n",
    "    \n",
    "    axs[plt_row, plt_col].plot(history.history[\"loss\"])\n",
    "    \n",
    "    return (false_positives,false_negatives,positives,negatives)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c9e2833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import SeparableConv1D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#creates and returns a model for hyperparameter optimization using keras_tuner\n",
    "def create_model_hiperopti(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(SeparableConv1D(filters=hp.Choice(name=\"conv_1_filter\", values=[32,96]), \n",
    "                              kernel_size=hp.Choice(name=\"conv_1_kernel\", values=[3,5,7]), \n",
    "                              padding=\"same\", \n",
    "                              activation=\"relu\",\n",
    "                              input_shape=(1, 32)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(hp.Choice(name=\"dropout_1\", values=[0.1,0.5,0.7])))\n",
    "\n",
    "    model.add(SeparableConv1D(filters=hp.Choice(name=\"conv_2_filter\", values=[32,96]), \n",
    "                              kernel_size=hp.Choice(name=\"conv_2_kernel\", values=[3,5,7]), \n",
    "                              padding=\"same\", \n",
    "                              activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(hp.Choice(name=\"dropout_2\", values=[0.2,0.4,0.6])))\n",
    "\n",
    "    model.add(LSTM(hp.Choice(name=\"lstm_nodes\", values=[64,96]),recurrent_dropout=0.2, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(hp.Choice(name=\"dense_1_nodes\", values=[64,32]), \n",
    "                    kernel_initializer=\"random_uniform\",\n",
    "                    bias_initializer=\"random_uniform\",\n",
    "                    activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(hp.Choice(name=\"dropout_3\", values=[0.0,0.3,0.6])))\n",
    "\n",
    "    model.add(Dense(hp.Choice(name=\"dense_2_nodes\", values=[64,32]), \n",
    "                    kernel_initializer=\"random_uniform\",\n",
    "                    bias_initializer=\"random_uniform\",\n",
    "                    activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(hp.Choice(name=\"dropout_4\", values=[0.0,0.5])))\n",
    "\n",
    "    model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Choice(name=\"learning_rate\", values=[0.001,0.00001])) ,\n",
    "                                 loss=\"binary_crossentropy\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4d5e29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner.tuners import RandomSearch\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "#Does a hpyerparameter optimization on the test and validitaion sets given\n",
    "#Using keras_tuner and randomsearch. returns a model built with the best parameters\n",
    "def hyperopti(X_train,y_train, X_valid,y_valid):\n",
    "    tuner = RandomSearch(\n",
    "        create_model_hiperopti,\n",
    "        objective='val_loss',\n",
    "        max_trials=50,\n",
    "        directory=\"tuner_runs\")\n",
    "\n",
    "    print(tuner.search_space_summary())\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=500, verbose=2,monitor='val_loss', mode='min')\n",
    "    \n",
    "\n",
    "    tuner.search(X_train, y_train,\n",
    "                 epochs=2000, validation_data=(X_valid,y_valid),\n",
    "                 callbacks=[early_stopping,keras.callbacks.TensorBoard(\"tuner_runs/tb_logs\")]\n",
    "                 )\n",
    "\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    params_best = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    print(\"best params\\n\",params_best.get_config()['values'])\n",
    "    \n",
    "    return tuner.hypermodel.build(params_best)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e254ce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#launches a hyperparameter optimization on a user. returns the best model found.\n",
    "def hyperopti_on_user(id_):\n",
    "    (X_train,y_train), (X_valid,y_valid), (X_test,y_test) = prepare_data_of_user(id_)\n",
    "    print(X_valid.shape)\n",
    "    best_model = hyperopti(X_train,y_train, X_valid,y_valid)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1380e170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 22 Complete [00h 00m 58s]\n",
      "val_loss: 0.0002452078042551875\n",
      "\n",
      "Best val_loss So Far: 1.2334548955550417e-06\n",
      "Total elapsed time: 00h 30m 49s\n",
      "\n",
      "Search: Running Trial #23\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "conv_1_filter     |96                |96                \n",
      "conv_1_kernel     |3                 |7                 \n",
      "dropout_1         |0.7               |0.1               \n",
      "conv_2_filter     |32                |32                \n",
      "conv_2_kernel     |3                 |5                 \n",
      "dropout_2         |0.6               |0.4               \n",
      "lstm_nodes        |96                |96                \n",
      "dense_1_nodes     |64                |32                \n",
      "dropout_3         |0                 |0.6               \n",
      "dense_2_nodes     |32                |64                \n",
      "dropout_4         |0                 |0                 \n",
      "learning_rate     |0.001             |0.001             \n",
      "\n",
      "Epoch 1/2000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6353 - val_loss: 0.6932\n",
      "Epoch 2/2000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.6067 - val_loss: 0.6931\n",
      "Epoch 3/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.7295 - val_loss: 0.6932\n",
      "Epoch 4/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7415 - val_loss: 0.6932\n",
      "Epoch 5/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.6450 - val_loss: 0.6932\n",
      "Epoch 6/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4583 - val_loss: 0.6933\n",
      "Epoch 7/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4321 - val_loss: 0.6934\n",
      "Epoch 8/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5266 - val_loss: 0.6935\n",
      "Epoch 9/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4417 - val_loss: 0.6936\n",
      "Epoch 10/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3247 - val_loss: 0.6937\n",
      "Epoch 11/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5534 - val_loss: 0.6939\n",
      "Epoch 12/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3817 - val_loss: 0.6940\n",
      "Epoch 13/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3956 - val_loss: 0.6942\n",
      "Epoch 14/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4241 - val_loss: 0.6943\n",
      "Epoch 15/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4129 - val_loss: 0.6945\n",
      "Epoch 16/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3836 - val_loss: 0.6946\n",
      "Epoch 17/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4745 - val_loss: 0.6946\n",
      "Epoch 18/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5257 - val_loss: 0.6947\n",
      "Epoch 19/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4575 - val_loss: 0.6948\n",
      "Epoch 20/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4145 - val_loss: 0.6948\n",
      "Epoch 21/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2646 - val_loss: 0.6949\n",
      "Epoch 22/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2927 - val_loss: 0.6949\n",
      "Epoch 23/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2636 - val_loss: 0.6950\n",
      "Epoch 24/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5036 - val_loss: 0.6951\n",
      "Epoch 25/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3851 - val_loss: 0.6951\n",
      "Epoch 26/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6037 - val_loss: 0.6952\n",
      "Epoch 27/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3015 - val_loss: 0.6953\n",
      "Epoch 28/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2512 - val_loss: 0.6953\n",
      "Epoch 29/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4956 - val_loss: 0.6954\n",
      "Epoch 30/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4035 - val_loss: 0.6954\n",
      "Epoch 31/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4432 - val_loss: 0.6953\n",
      "Epoch 32/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4073 - val_loss: 0.6953\n",
      "Epoch 33/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2474 - val_loss: 0.6954\n",
      "Epoch 34/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5467 - val_loss: 0.6954\n",
      "Epoch 35/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2218 - val_loss: 0.6954\n",
      "Epoch 36/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3747 - val_loss: 0.6954\n",
      "Epoch 37/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1808 - val_loss: 0.6955\n",
      "Epoch 38/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3916 - val_loss: 0.6955\n",
      "Epoch 39/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3242 - val_loss: 0.6956\n",
      "Epoch 40/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2921 - val_loss: 0.6957\n",
      "Epoch 41/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2967 - val_loss: 0.6958\n",
      "Epoch 42/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3257 - val_loss: 0.6959\n",
      "Epoch 43/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1630 - val_loss: 0.6960\n",
      "Epoch 44/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4115 - val_loss: 0.6960\n",
      "Epoch 45/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2340 - val_loss: 0.6962\n",
      "Epoch 46/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2359 - val_loss: 0.6963\n",
      "Epoch 47/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2248 - val_loss: 0.6964\n",
      "Epoch 48/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2293 - val_loss: 0.6964\n",
      "Epoch 49/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4534 - val_loss: 0.6965\n",
      "Epoch 50/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1616 - val_loss: 0.6967\n",
      "Epoch 51/2000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.1713 - val_loss: 0.6968\n",
      "Epoch 52/2000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.2506 - val_loss: 0.6969\n",
      "Epoch 53/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1494 - val_loss: 0.6971\n",
      "Epoch 54/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1457 - val_loss: 0.6972\n",
      "Epoch 55/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2310 - val_loss: 0.6973\n",
      "Epoch 56/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2227 - val_loss: 0.6974\n",
      "Epoch 57/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2253 - val_loss: 0.6975\n",
      "Epoch 58/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4138 - val_loss: 0.6976\n",
      "Epoch 59/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3917 - val_loss: 0.6977\n",
      "Epoch 60/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2190 - val_loss: 0.6978\n",
      "Epoch 61/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2628 - val_loss: 0.6979\n",
      "Epoch 62/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3293 - val_loss: 0.6980\n",
      "Epoch 63/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3190 - val_loss: 0.6981\n",
      "Epoch 64/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1407 - val_loss: 0.6983\n",
      "Epoch 65/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1306 - val_loss: 0.6984\n",
      "Epoch 66/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0971 - val_loss: 0.6985\n",
      "Epoch 67/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2289 - val_loss: 0.6987\n",
      "Epoch 68/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1813 - val_loss: 0.6988\n",
      "Epoch 69/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2524 - val_loss: 0.6990\n",
      "Epoch 70/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3095 - val_loss: 0.6992\n",
      "Epoch 71/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1151 - val_loss: 0.6994\n",
      "Epoch 72/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2402 - val_loss: 0.6995\n",
      "Epoch 73/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2303 - val_loss: 0.6996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1389 - val_loss: 0.6997\n",
      "Epoch 75/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2165 - val_loss: 0.6998\n",
      "Epoch 76/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1080 - val_loss: 0.6999\n",
      "Epoch 77/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1299 - val_loss: 0.6999\n",
      "Epoch 78/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1794 - val_loss: 0.7000\n",
      "Epoch 79/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3360 - val_loss: 0.6999\n",
      "Epoch 80/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1163 - val_loss: 0.7000\n",
      "Epoch 81/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0931 - val_loss: 0.7000\n",
      "Epoch 82/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1722 - val_loss: 0.7000\n",
      "Epoch 83/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2133 - val_loss: 0.6999\n",
      "Epoch 84/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0642 - val_loss: 0.6999\n",
      "Epoch 85/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0961 - val_loss: 0.6999\n",
      "Epoch 86/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1100 - val_loss: 0.6998\n",
      "Epoch 87/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0495 - val_loss: 0.6997\n",
      "Epoch 88/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1055 - val_loss: 0.6995\n",
      "Epoch 89/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0942 - val_loss: 0.6993\n",
      "Epoch 90/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0828 - val_loss: 0.6992\n",
      "Epoch 91/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2210 - val_loss: 0.6990\n",
      "Epoch 92/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1035 - val_loss: 0.6989\n",
      "Epoch 93/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0940 - val_loss: 0.6988\n",
      "Epoch 94/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1476 - val_loss: 0.6987\n",
      "Epoch 95/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0640 - val_loss: 0.6987\n",
      "Epoch 96/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1107 - val_loss: 0.6986\n",
      "Epoch 97/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0577 - val_loss: 0.6985\n",
      "Epoch 98/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1961 - val_loss: 0.6984\n",
      "Epoch 99/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1192 - val_loss: 0.6983\n",
      "Epoch 100/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2333 - val_loss: 0.6984\n",
      "Epoch 101/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1496 - val_loss: 0.6984\n",
      "Epoch 102/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1887 - val_loss: 0.6983\n",
      "Epoch 103/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1506 - val_loss: 0.6984\n",
      "Epoch 104/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1153 - val_loss: 0.6983\n",
      "Epoch 105/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0324 - val_loss: 0.6982\n",
      "Epoch 106/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1513 - val_loss: 0.6981\n",
      "Epoch 107/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2572 - val_loss: 0.6982\n",
      "Epoch 108/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1416 - val_loss: 0.6981\n",
      "Epoch 109/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0671 - val_loss: 0.6980\n",
      "Epoch 110/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0874 - val_loss: 0.6979\n",
      "Epoch 111/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1228 - val_loss: 0.6979\n",
      "Epoch 112/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0194 - val_loss: 0.6978\n",
      "Epoch 113/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0331 - val_loss: 0.6978\n",
      "Epoch 114/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0616 - val_loss: 0.6978\n",
      "Epoch 115/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0297 - val_loss: 0.6979\n",
      "Epoch 116/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1052 - val_loss: 0.6979\n",
      "Epoch 117/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1566 - val_loss: 0.6978\n",
      "Epoch 118/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1144 - val_loss: 0.6978\n",
      "Epoch 119/2000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0588 - val_loss: 0.6977\n",
      "Epoch 120/2000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.1826 - val_loss: 0.6976\n",
      "Epoch 121/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2052 - val_loss: 0.6975\n",
      "Epoch 122/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0262 - val_loss: 0.6973\n",
      "Epoch 123/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2687 - val_loss: 0.6970\n",
      "Epoch 124/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0654 - val_loss: 0.6966\n",
      "Epoch 125/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0890 - val_loss: 0.6961\n",
      "Epoch 126/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1438 - val_loss: 0.6957\n",
      "Epoch 127/2000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0890 - val_loss: 0.6952\n",
      "Epoch 128/2000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0929 - val_loss: 0.6949\n",
      "Epoch 129/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0425 - val_loss: 0.6945\n",
      "Epoch 130/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1269 - val_loss: 0.6943\n",
      "Epoch 131/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1142 - val_loss: 0.6942\n",
      "Epoch 132/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0240 - val_loss: 0.6941\n",
      "Epoch 133/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0339 - val_loss: 0.6939\n",
      "Epoch 134/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0396 - val_loss: 0.6938\n",
      "Epoch 135/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3100 - val_loss: 0.6936\n",
      "Epoch 136/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0608 - val_loss: 0.6934\n",
      "Epoch 137/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0659 - val_loss: 0.6933\n",
      "Epoch 138/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1322 - val_loss: 0.6931\n",
      "Epoch 139/2000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0527 - val_loss: 0.6930\n",
      "Epoch 140/2000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0624 - val_loss: 0.6928\n",
      "Epoch 141/2000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0565 - val_loss: 0.6927\n",
      "Epoch 142/2000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0538 - val_loss: 0.6925\n",
      "Epoch 143/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0513 - val_loss: 0.6924\n",
      "Epoch 144/2000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0559 - val_loss: 0.6922\n",
      "Epoch 145/2000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0161 - val_loss: 0.6920\n",
      "Epoch 146/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0374 - val_loss: 0.6919\n",
      "Epoch 147/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0436 - val_loss: 0.6917\n",
      "Epoch 148/2000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0326 - val_loss: 0.6916\n",
      "Epoch 149/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0127 - val_loss: 0.6914\n",
      "Epoch 150/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0230 - val_loss: 0.6913\n",
      "Epoch 151/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0382 - val_loss: 0.6912\n",
      "Epoch 152/2000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0542 - val_loss: 0.6912\n",
      "Epoch 153/2000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0397 - val_loss: 0.6911\n",
      "Epoch 154/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0554 - val_loss: 0.6911\n",
      "Epoch 155/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0230 - val_loss: 0.6911\n",
      "Epoch 156/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0199 - val_loss: 0.6910\n",
      "Epoch 157/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0331 - val_loss: 0.6910\n",
      "Epoch 158/2000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0259 - val_loss: 0.6909\n",
      "Epoch 159/2000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0220 - val_loss: 0.6909\n",
      "Epoch 160/2000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0243 - val_loss: 0.6909\n",
      "Epoch 161/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0106 - val_loss: 0.6909\n",
      "Epoch 162/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0396 - val_loss: 0.6908\n",
      "Epoch 163/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0890 - val_loss: 0.6908\n",
      "Epoch 164/2000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0505 - val_loss: 0.6908\n",
      "Epoch 165/2000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0878 - val_loss: 0.6908\n",
      "Epoch 166/2000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0177 - val_loss: 0.6907\n",
      "Epoch 167/2000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0074 - val_loss: 0.6907\n",
      "Epoch 168/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0489 - val_loss: 0.6907\n",
      "Epoch 169/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0516 - val_loss: 0.6907\n",
      "Epoch 170/2000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0236 - val_loss: 0.6907\n",
      "Epoch 171/2000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0424 - val_loss: 0.6906\n",
      "Epoch 172/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0399 - val_loss: 0.6906\n",
      "Epoch 173/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0704 - val_loss: 0.6906\n",
      "Epoch 174/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0531 - val_loss: 0.6906\n",
      "Epoch 175/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0414 - val_loss: 0.6906\n",
      "Epoch 176/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0245 - val_loss: 0.6907\n",
      "Epoch 177/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1538 - val_loss: 0.6907\n",
      "Epoch 178/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0082 - val_loss: 0.6907\n",
      "Epoch 179/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0360 - val_loss: 0.6907\n",
      "Epoch 180/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0710 - val_loss: 0.6908\n",
      "Epoch 181/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0064 - val_loss: 0.6909\n",
      "Epoch 182/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0530 - val_loss: 0.6908\n",
      "Epoch 183/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0086 - val_loss: 0.6906\n",
      "Epoch 184/2000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2905 - val_loss: 0.6905\n",
      "Epoch 185/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0175 - val_loss: 0.6905\n",
      "Epoch 186/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0088 - val_loss: 0.6904\n",
      "Epoch 187/2000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0084 - val_loss: 0.6904\n",
      "Epoch 188/2000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0494 - val_loss: 0.6903\n",
      "Epoch 189/2000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0243 - val_loss: 0.6902\n",
      "Epoch 190/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0821 - val_loss: 0.6901\n",
      "Epoch 191/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0422 - val_loss: 0.6899\n",
      "Epoch 192/2000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0164 - val_loss: 0.6898\n",
      "Epoch 193/2000\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0129 - val_loss: 0.6896\n",
      "Epoch 194/2000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0225 - val_loss: 0.6894\n",
      "Epoch 195/2000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0121 - val_loss: 0.6892\n",
      "Epoch 196/2000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0086 - val_loss: 0.6891\n",
      "Epoch 197/2000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0047 - val_loss: 0.6890\n",
      "Epoch 198/2000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0098 - val_loss: 0.6888\n",
      "Epoch 199/2000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0111 - val_loss: 0.6888\n",
      "Epoch 200/2000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0066 - val_loss: 0.6888\n",
      "Epoch 201/2000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0045 - val_loss: 0.6887\n",
      "Epoch 202/2000\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0179 - val_loss: 0.6887\n",
      "Epoch 203/2000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0074 - val_loss: 0.6887\n",
      "Epoch 204/2000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0061 - val_loss: 0.6887\n",
      "Epoch 205/2000\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0263 - val_loss: 0.6887\n",
      "Epoch 206/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0312 - val_loss: 0.6885\n",
      "Epoch 207/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0099 - val_loss: 0.6884\n",
      "Epoch 208/2000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0109 - val_loss: 0.6882\n",
      "Epoch 209/2000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0094 - val_loss: 0.6880\n",
      "Epoch 210/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0172 - val_loss: 0.6878\n",
      "Epoch 211/2000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0311 - val_loss: 0.6878\n",
      "Epoch 212/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0824 - val_loss: 0.6883\n",
      "Epoch 213/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0058 - val_loss: 0.6887\n",
      "Epoch 214/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0145 - val_loss: 0.6892\n",
      "Epoch 215/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0043 - val_loss: 0.6897\n",
      "Epoch 216/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0088 - val_loss: 0.6902\n",
      "Epoch 217/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0325 - val_loss: 0.6907\n",
      "Epoch 218/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0259 - val_loss: 0.6912\n",
      "Epoch 219/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0032 - val_loss: 0.6918\n",
      "Epoch 220/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0394 - val_loss: 0.6928\n",
      "Epoch 221/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0172 - val_loss: 0.6935\n",
      "Epoch 222/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0127 - val_loss: 0.6942\n",
      "Epoch 223/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.6950\n",
      "Epoch 224/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.6957\n",
      "Epoch 225/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0647 - val_loss: 0.6966\n",
      "Epoch 226/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0130 - val_loss: 0.6974\n",
      "Epoch 227/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1400 - val_loss: 0.6980\n",
      "Epoch 228/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0072 - val_loss: 0.6987\n",
      "Epoch 229/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0118 - val_loss: 0.6996\n",
      "Epoch 230/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0099 - val_loss: 0.7004\n",
      "Epoch 231/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0098 - val_loss: 0.7011\n",
      "Epoch 232/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0039 - val_loss: 0.7017\n",
      "Epoch 233/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0476 - val_loss: 0.7028\n",
      "Epoch 234/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0265 - val_loss: 0.7033\n",
      "Epoch 235/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0060 - val_loss: 0.7039\n",
      "Epoch 236/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0067 - val_loss: 0.7046\n",
      "Epoch 237/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0087 - val_loss: 0.7051\n",
      "Epoch 238/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0042 - val_loss: 0.7052\n",
      "Epoch 239/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0029 - val_loss: 0.7053\n",
      "Epoch 240/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0132 - val_loss: 0.7056\n",
      "Epoch 241/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0025 - val_loss: 0.7061\n",
      "Epoch 242/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0304 - val_loss: 0.7066\n",
      "Epoch 243/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0057 - val_loss: 0.7072\n",
      "Epoch 244/2000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0171 - val_loss: 0.7075\n",
      "Epoch 245/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0949 - val_loss: 0.7078\n",
      "Epoch 246/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0045 - val_loss: 0.7083\n",
      "Epoch 247/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0056 - val_loss: 0.7090\n",
      "Epoch 248/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.7097\n",
      "Epoch 249/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0081 - val_loss: 0.7105\n",
      "Epoch 250/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0068 - val_loss: 0.7112\n",
      "Epoch 251/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0049 - val_loss: 0.7120\n",
      "Epoch 252/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0236 - val_loss: 0.7129\n",
      "Epoch 253/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0122 - val_loss: 0.7138\n",
      "Epoch 254/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0240 - val_loss: 0.7141\n",
      "Epoch 255/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0167 - val_loss: 0.7142\n",
      "Epoch 256/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0058 - val_loss: 0.7143\n",
      "Epoch 257/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0416 - val_loss: 0.7142\n",
      "Epoch 258/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0046 - val_loss: 0.7142\n",
      "Epoch 259/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0229 - val_loss: 0.7160\n",
      "Epoch 260/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0070 - val_loss: 0.7180\n",
      "Epoch 261/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0190 - val_loss: 0.7198\n",
      "Epoch 262/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0086 - val_loss: 0.7217\n",
      "Epoch 263/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0020 - val_loss: 0.7236\n",
      "Epoch 264/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0206 - val_loss: 0.7248\n",
      "Epoch 265/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0026 - val_loss: 0.7260\n",
      "Epoch 266/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0022 - val_loss: 0.7272\n",
      "Epoch 267/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0027 - val_loss: 0.7283\n",
      "Epoch 268/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0134 - val_loss: 0.7298\n",
      "Epoch 269/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0044 - val_loss: 0.7315\n",
      "Epoch 270/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0078 - val_loss: 0.7328\n",
      "Epoch 271/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0075 - val_loss: 0.7343\n",
      "Epoch 272/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0027 - val_loss: 0.7361\n",
      "Epoch 273/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0077 - val_loss: 0.7376\n",
      "Epoch 274/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0913 - val_loss: 0.7393\n",
      "Epoch 275/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0046 - val_loss: 0.7416\n",
      "Epoch 276/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0039 - val_loss: 0.7437\n",
      "Epoch 277/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0027 - val_loss: 0.7456\n",
      "Epoch 278/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1374 - val_loss: 0.7466\n",
      "Epoch 279/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0362 - val_loss: 0.7480\n",
      "Epoch 280/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0022 - val_loss: 0.7492\n",
      "Epoch 281/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0017 - val_loss: 0.7510\n",
      "Epoch 282/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0022 - val_loss: 0.7523\n",
      "Epoch 283/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0031 - val_loss: 0.7527\n",
      "Epoch 284/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0024 - val_loss: 0.7533\n",
      "Epoch 285/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0202 - val_loss: 0.7541\n",
      "Epoch 286/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1218 - val_loss: 0.7535\n",
      "Epoch 287/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0045 - val_loss: 0.7527\n",
      "Epoch 288/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0163 - val_loss: 0.7544\n",
      "Epoch 289/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0044 - val_loss: 0.7559\n",
      "Epoch 290/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.7572\n",
      "Epoch 291/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0309 - val_loss: 0.7585\n",
      "Epoch 292/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.7594\n",
      "Epoch 293/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0024 - val_loss: 0.7600\n",
      "Epoch 294/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0037 - val_loss: 0.7609\n",
      "Epoch 295/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0023 - val_loss: 0.7616\n",
      "Epoch 296/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0072 - val_loss: 0.7628\n",
      "Epoch 297/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0131 - val_loss: 0.7653\n",
      "Epoch 298/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2006 - val_loss: 0.7843\n",
      "Epoch 299/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0098 - val_loss: 0.8032\n",
      "Epoch 300/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0067 - val_loss: 0.8218\n",
      "Epoch 301/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0104 - val_loss: 0.8358\n",
      "Epoch 302/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0080 - val_loss: 0.8460\n",
      "Epoch 303/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0021 - val_loss: 0.8550\n",
      "Epoch 304/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0025 - val_loss: 0.8627\n",
      "Epoch 305/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1828 - val_loss: 0.8702\n",
      "Epoch 306/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0044 - val_loss: 0.8766\n",
      "Epoch 307/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0137 - val_loss: 0.8835\n",
      "Epoch 308/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0109 - val_loss: 0.8909\n",
      "Epoch 309/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.8979\n",
      "Epoch 310/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0081 - val_loss: 0.9046\n",
      "Epoch 311/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0068 - val_loss: 0.9114\n",
      "Epoch 312/2000\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0038 - val_loss: 0.9180\n",
      "Epoch 313/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0450 - val_loss: 0.9237\n",
      "Epoch 314/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0123 - val_loss: 0.9285\n",
      "Epoch 315/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0036 - val_loss: 0.9336\n",
      "Epoch 316/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0355 - val_loss: 0.9383\n",
      "Epoch 317/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0209 - val_loss: 0.9444\n",
      "Epoch 318/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0034 - val_loss: 0.9504\n",
      "Epoch 319/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0121 - val_loss: 0.9570\n",
      "Epoch 320/2000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0699 - val_loss: 0.9623\n",
      "Epoch 321/2000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0334 - val_loss: 0.9673\n",
      "Epoch 322/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0025 - val_loss: 0.9727\n",
      "Epoch 323/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0020 - val_loss: 0.9777\n",
      "Epoch 324/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0571 - val_loss: 0.9851\n",
      "Epoch 325/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0019 - val_loss: 0.9921\n",
      "Epoch 326/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0033 - val_loss: 0.9989\n",
      "Epoch 327/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0048 - val_loss: 1.0056\n",
      "Epoch 328/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0029 - val_loss: 1.0121\n",
      "Epoch 329/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0308 - val_loss: 1.0189\n",
      "Epoch 330/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0018 - val_loss: 1.0245\n",
      "Epoch 331/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0155 - val_loss: 1.0291\n",
      "Epoch 332/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0482 - val_loss: 1.0356\n",
      "Epoch 333/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0333 - val_loss: 1.0460\n",
      "Epoch 334/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0761 - val_loss: 1.0486\n",
      "Epoch 335/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0022 - val_loss: 1.0501\n",
      "Epoch 336/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0045 - val_loss: 1.0513\n",
      "Epoch 337/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0025 - val_loss: 1.0523\n",
      "Epoch 338/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0076 - val_loss: 1.0539\n",
      "Epoch 339/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0028 - val_loss: 1.0559\n",
      "Epoch 340/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0040 - val_loss: 1.0574\n",
      "Epoch 341/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0035 - val_loss: 1.0589\n",
      "Epoch 342/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0966 - val_loss: 1.0553\n",
      "Epoch 343/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1067 - val_loss: 1.0535\n",
      "Epoch 344/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0072 - val_loss: 1.0535\n",
      "Epoch 345/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0043 - val_loss: 1.0533\n",
      "Epoch 346/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0061 - val_loss: 1.0500\n",
      "Epoch 347/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0040 - val_loss: 1.0460\n",
      "Epoch 348/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0042 - val_loss: 1.0442\n",
      "Epoch 349/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0870 - val_loss: 1.0464\n",
      "Epoch 350/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0156 - val_loss: 1.0512\n",
      "Epoch 351/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0038 - val_loss: 1.0565\n",
      "Epoch 352/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0036 - val_loss: 1.0621\n",
      "Epoch 353/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0035 - val_loss: 1.0682\n",
      "Epoch 354/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0026 - val_loss: 1.0737\n",
      "Epoch 355/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0285 - val_loss: 1.0773\n",
      "Epoch 356/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0197 - val_loss: 1.0823\n",
      "Epoch 357/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0023 - val_loss: 1.0871\n",
      "Epoch 358/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0023 - val_loss: 1.0921\n",
      "Epoch 359/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0034 - val_loss: 1.0966\n",
      "Epoch 360/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0054 - val_loss: 1.1012\n",
      "Epoch 361/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0052 - val_loss: 1.1059\n",
      "Epoch 362/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0019 - val_loss: 1.1106\n",
      "Epoch 363/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0015 - val_loss: 1.1156\n",
      "Epoch 364/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0674 - val_loss: 1.1231\n",
      "Epoch 365/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0019 - val_loss: 1.1306\n",
      "Epoch 366/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0017 - val_loss: 1.1384\n",
      "Epoch 367/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0024 - val_loss: 1.1468\n",
      "Epoch 368/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0047 - val_loss: 1.1541\n",
      "Epoch 369/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0068 - val_loss: 1.1618\n",
      "Epoch 370/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0016 - val_loss: 1.1696\n",
      "Epoch 371/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 1.1757\n",
      "Epoch 372/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0030 - val_loss: 1.1823\n",
      "Epoch 373/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0014 - val_loss: 1.1889\n",
      "Epoch 374/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0051 - val_loss: 1.1960\n",
      "Epoch 375/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0016 - val_loss: 1.2040\n",
      "Epoch 376/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0105 - val_loss: 1.2128\n",
      "Epoch 377/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0166 - val_loss: 1.2276\n",
      "Epoch 378/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0021 - val_loss: 1.2427\n",
      "Epoch 379/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0031 - val_loss: 1.2559\n",
      "Epoch 380/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0019 - val_loss: 1.2679\n",
      "Epoch 381/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0044 - val_loss: 1.2799\n",
      "Epoch 382/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0031 - val_loss: 1.2911\n",
      "Epoch 383/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0137 - val_loss: 1.3040\n",
      "Epoch 384/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2068 - val_loss: 1.3200\n",
      "Epoch 385/2000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0025 - val_loss: 1.3355\n",
      "Epoch 386/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1188 - val_loss: 1.3489\n",
      "Epoch 387/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0016 - val_loss: 1.3617\n",
      "Epoch 388/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0037 - val_loss: 1.3707\n",
      "Epoch 389/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0122 - val_loss: 1.3771\n",
      "Epoch 390/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0038 - val_loss: 1.3840\n",
      "Epoch 391/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0036 - val_loss: 1.3910\n",
      "Epoch 392/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0030 - val_loss: 1.3978\n",
      "Epoch 393/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0027 - val_loss: 1.4053\n",
      "Epoch 394/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0020 - val_loss: 1.4114\n",
      "Epoch 395/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0018 - val_loss: 1.4188\n",
      "Epoch 396/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0019 - val_loss: 1.4249\n",
      "Epoch 397/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0070 - val_loss: 1.4312\n",
      "Epoch 398/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0032 - val_loss: 1.4387\n",
      "Epoch 399/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0019 - val_loss: 1.4461\n",
      "Epoch 400/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0190 - val_loss: 1.4533\n",
      "Epoch 401/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0026 - val_loss: 1.4584\n",
      "Epoch 402/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0039 - val_loss: 1.4634\n",
      "Epoch 403/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0030 - val_loss: 1.4677\n",
      "Epoch 404/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0020 - val_loss: 1.4730\n",
      "Epoch 405/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0098 - val_loss: 1.4799\n",
      "Epoch 406/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0028 - val_loss: 1.4871\n",
      "Epoch 407/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0146 - val_loss: 1.4972\n",
      "Epoch 408/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0024 - val_loss: 1.5072\n",
      "Epoch 409/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0041 - val_loss: 1.5172\n",
      "Epoch 410/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0023 - val_loss: 1.5255\n",
      "Epoch 411/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0032 - val_loss: 1.5344\n",
      "Epoch 412/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0030 - val_loss: 1.5430\n",
      "Epoch 413/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0044 - val_loss: 1.5518\n",
      "Epoch 414/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0189 - val_loss: 1.5581\n",
      "Epoch 415/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0019 - val_loss: 1.5656\n",
      "Epoch 416/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0015 - val_loss: 1.5745\n",
      "Epoch 417/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0012 - val_loss: 1.5837\n",
      "Epoch 418/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0015 - val_loss: 1.5914\n",
      "Epoch 419/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0023 - val_loss: 1.6007\n",
      "Epoch 420/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0018 - val_loss: 1.6092\n",
      "Epoch 421/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0042 - val_loss: 1.6156\n",
      "Epoch 422/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0018 - val_loss: 1.6217\n",
      "Epoch 423/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0097 - val_loss: 1.6307\n",
      "Epoch 424/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0012 - val_loss: 1.6392\n",
      "Epoch 425/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0726 - val_loss: 1.6313\n",
      "Epoch 426/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0021 - val_loss: 1.6195\n",
      "Epoch 427/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0024 - val_loss: 1.6089\n",
      "Epoch 428/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0033 - val_loss: 1.6008\n",
      "Epoch 429/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0088 - val_loss: 1.5948\n",
      "Epoch 430/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0035 - val_loss: 1.5891\n",
      "Epoch 431/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0030 - val_loss: 1.5867\n",
      "Epoch 432/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0019 - val_loss: 1.5862\n",
      "Epoch 433/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0050 - val_loss: 1.5884\n",
      "Epoch 434/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0034 - val_loss: 1.5912\n",
      "Epoch 435/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0019 - val_loss: 1.5942\n",
      "Epoch 436/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0036 - val_loss: 1.5975\n",
      "Epoch 437/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0012 - val_loss: 1.6018\n",
      "Epoch 438/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0294 - val_loss: 1.5925\n",
      "Epoch 439/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0020 - val_loss: 1.5864\n",
      "Epoch 440/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 1.5828\n",
      "Epoch 441/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0014 - val_loss: 1.5782\n",
      "Epoch 442/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0022 - val_loss: 1.5701\n",
      "Epoch 443/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0018 - val_loss: 1.5653\n",
      "Epoch 444/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0019 - val_loss: 1.5592\n",
      "Epoch 445/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0017 - val_loss: 1.5547\n",
      "Epoch 446/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0015 - val_loss: 1.5489\n",
      "Epoch 447/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0014 - val_loss: 1.5446\n",
      "Epoch 448/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0137 - val_loss: 1.5299\n",
      "Epoch 449/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0036 - val_loss: 1.5141\n",
      "Epoch 450/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0017 - val_loss: 1.5009\n",
      "Epoch 451/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0014 - val_loss: 1.4908\n",
      "Epoch 452/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0090 - val_loss: 1.4793\n",
      "Epoch 453/2000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0079 - val_loss: 1.4866\n",
      "Epoch 454/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0221 - val_loss: 1.4619\n",
      "Epoch 455/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0182 - val_loss: 1.4625\n",
      "Epoch 456/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0018 - val_loss: 1.4680\n",
      "Epoch 457/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0574 - val_loss: 1.5017\n",
      "Epoch 458/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0013 - val_loss: 1.5352\n",
      "Epoch 459/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0162 - val_loss: 1.5712\n",
      "Epoch 460/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0015 - val_loss: 1.6035\n",
      "Epoch 461/2000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0096 - val_loss: 1.6307\n",
      "Epoch 462/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0069 - val_loss: 1.6509\n",
      "Epoch 463/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0017 - val_loss: 1.6707\n",
      "Epoch 464/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0010 - val_loss: 1.6902\n",
      "Epoch 465/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0010 - val_loss: 1.7113\n",
      "Epoch 466/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0035 - val_loss: 1.7370\n",
      "Epoch 467/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0017 - val_loss: 1.7644\n",
      "Epoch 468/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0017 - val_loss: 1.7887\n",
      "Epoch 469/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0517 - val_loss: 1.8713\n",
      "Epoch 470/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0043 - val_loss: 1.9075\n",
      "Epoch 471/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0044 - val_loss: 1.9336\n",
      "Epoch 472/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.2773e-04 - val_loss: 1.9731\n",
      "Epoch 473/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0077 - val_loss: 2.0067\n",
      "Epoch 474/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0014 - val_loss: 2.0458\n",
      "Epoch 475/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0043 - val_loss: 2.0797\n",
      "Epoch 476/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0040 - val_loss: 2.0933\n",
      "Epoch 477/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0013 - val_loss: 2.0987\n",
      "Epoch 478/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0080 - val_loss: 2.1052\n",
      "Epoch 479/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0202 - val_loss: 2.1377\n",
      "Epoch 480/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0014 - val_loss: 2.1643\n",
      "Epoch 481/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0032 - val_loss: 2.1882\n",
      "Epoch 482/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0011 - val_loss: 2.2099\n",
      "Epoch 483/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0880 - val_loss: 2.2245\n",
      "Epoch 484/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0015 - val_loss: 2.2409\n",
      "Epoch 485/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0016 - val_loss: 2.2574\n",
      "Epoch 486/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0012 - val_loss: 2.2663\n",
      "Epoch 487/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0013 - val_loss: 2.2779\n",
      "Epoch 488/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0021 - val_loss: 2.2877\n",
      "Epoch 489/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0014 - val_loss: 2.2956\n",
      "Epoch 490/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 9.0320e-04 - val_loss: 2.3023\n",
      "Epoch 491/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0011 - val_loss: 2.3098\n",
      "Epoch 492/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0012 - val_loss: 2.3177\n",
      "Epoch 493/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0059 - val_loss: 2.3268\n",
      "Epoch 494/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.3669e-04 - val_loss: 2.3362\n",
      "Epoch 495/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0019 - val_loss: 2.3417\n",
      "Epoch 496/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0033 - val_loss: 2.3503\n",
      "Epoch 497/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0014 - val_loss: 2.3565\n",
      "Epoch 498/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0060 - val_loss: 2.3594\n",
      "Epoch 499/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.8654e-04 - val_loss: 2.3607\n",
      "Epoch 500/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0069 - val_loss: 2.3612\n",
      "Epoch 501/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.8115e-04 - val_loss: 2.3632\n",
      "Epoch 502/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 9.6119e-04 - val_loss: 2.3653\n",
      "Epoch 503/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0023 - val_loss: 2.3631\n",
      "Epoch 504/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0016 - val_loss: 2.3605\n",
      "Epoch 505/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.9491e-04 - val_loss: 2.3608\n",
      "Epoch 506/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0011 - val_loss: 2.3597\n",
      "Epoch 507/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.3350e-04 - val_loss: 2.3594\n",
      "Epoch 508/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0010 - val_loss: 2.3573\n",
      "Epoch 509/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0011 - val_loss: 2.3601\n",
      "Epoch 510/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0016 - val_loss: 2.3628\n",
      "Epoch 511/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0027 - val_loss: 2.3616\n",
      "Epoch 512/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0022 - val_loss: 2.3638\n",
      "Epoch 513/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.8687e-04 - val_loss: 2.3644\n",
      "Epoch 514/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0014 - val_loss: 2.3663\n",
      "Epoch 515/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.6795e-04 - val_loss: 2.3677\n",
      "Epoch 516/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.6860e-04 - val_loss: 2.3696\n",
      "Epoch 517/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8.2193e-04 - val_loss: 2.3709\n",
      "Epoch 518/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0010 - val_loss: 2.3732\n",
      "Epoch 519/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0032 - val_loss: 2.3842\n",
      "Epoch 520/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.0678e-04 - val_loss: 2.3958\n",
      "Epoch 521/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0011 - val_loss: 2.4088\n",
      "Epoch 522/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0013 - val_loss: 2.4216\n",
      "Epoch 523/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0015 - val_loss: 2.4327\n",
      "Epoch 524/2000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.1401e-04 - val_loss: 2.4399\n",
      "Epoch 525/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0013 - val_loss: 2.4487\n",
      "Epoch 526/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0142 - val_loss: 2.4144\n",
      "Epoch 527/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1594 - val_loss: 2.5222\n",
      "Epoch 528/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.9463e-04 - val_loss: 2.5549\n",
      "Epoch 529/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0018 - val_loss: 2.5971\n",
      "Epoch 530/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0017 - val_loss: 2.6353\n",
      "Epoch 531/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0755 - val_loss: 2.5916\n",
      "Epoch 532/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 9.5069e-04 - val_loss: 2.5487\n",
      "Epoch 533/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0099 - val_loss: 2.5041\n",
      "Epoch 534/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.9263e-04 - val_loss: 2.4631\n",
      "Epoch 535/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0055 - val_loss: 2.4172\n",
      "Epoch 536/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0577 - val_loss: 2.3719\n",
      "Epoch 537/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.7748e-04 - val_loss: 2.3300\n",
      "Epoch 538/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.2215e-04 - val_loss: 2.2906\n",
      "Epoch 539/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0012 - val_loss: 2.2312\n",
      "Epoch 540/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0039 - val_loss: 2.1886\n",
      "Epoch 541/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0010 - val_loss: 2.1407\n",
      "Epoch 542/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.5162e-04 - val_loss: 2.0985\n",
      "Epoch 543/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0015 - val_loss: 2.0671\n",
      "Epoch 544/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.6419e-04 - val_loss: 2.0411\n",
      "Epoch 545/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 9.5500e-04 - val_loss: 2.0170\n",
      "Epoch 546/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0018 - val_loss: 1.9944\n",
      "Epoch 547/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.4128e-04 - val_loss: 1.9767\n",
      "Epoch 548/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0015 - val_loss: 1.9620\n",
      "Epoch 549/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0068 - val_loss: 1.9375\n",
      "Epoch 550/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.7908e-04 - val_loss: 1.9234\n",
      "Epoch 551/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2127 - val_loss: 1.9659\n",
      "Epoch 552/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0026 - val_loss: 2.0064\n",
      "Epoch 553/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 9.1423e-04 - val_loss: 2.0348\n",
      "Epoch 554/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0240 - val_loss: 2.0938\n",
      "Epoch 555/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0033 - val_loss: 2.1372\n",
      "Epoch 556/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0158 - val_loss: 2.2390\n",
      "Epoch 557/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.4938e-04 - val_loss: 2.3126\n",
      "Epoch 558/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0014 - val_loss: 2.3763\n",
      "Epoch 559/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0011 - val_loss: 2.4411\n",
      "Epoch 560/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0028 - val_loss: 2.4904\n",
      "Epoch 561/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0011 - val_loss: 2.5255\n",
      "Epoch 562/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0011 - val_loss: 2.5566\n",
      "Epoch 563/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0028 - val_loss: 2.5877\n",
      "Epoch 564/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0023 - val_loss: 2.6148\n",
      "Epoch 565/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0011 - val_loss: 2.6381\n",
      "Epoch 566/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 9.8227e-04 - val_loss: 2.6540\n",
      "Epoch 567/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0025 - val_loss: 2.6764\n",
      "Epoch 568/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0012 - val_loss: 2.6940\n",
      "Epoch 569/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0010 - val_loss: 2.7131\n",
      "Epoch 570/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0039 - val_loss: 2.7234\n",
      "Epoch 571/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.2351e-04 - val_loss: 2.7345\n",
      "Epoch 572/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0011 - val_loss: 2.7457\n",
      "Epoch 573/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0026 - val_loss: 2.7648\n",
      "Epoch 574/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.8069e-04 - val_loss: 2.7786\n",
      "Epoch 575/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0038 - val_loss: 2.7896\n",
      "Epoch 576/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0017 - val_loss: 2.7944\n",
      "Epoch 577/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0030 - val_loss: 2.8001\n",
      "Epoch 578/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0016 - val_loss: 2.8180\n",
      "Epoch 579/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0019 - val_loss: 2.8339\n",
      "Epoch 580/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0015 - val_loss: 2.8472\n",
      "Epoch 581/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0011 - val_loss: 2.8649\n",
      "Epoch 582/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.6842e-04 - val_loss: 2.8770\n",
      "Epoch 583/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0022 - val_loss: 2.8895\n",
      "Epoch 584/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.0249e-04 - val_loss: 2.9018\n",
      "Epoch 585/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0022 - val_loss: 2.9131\n",
      "Epoch 586/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0019 - val_loss: 2.9138\n",
      "Epoch 587/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0011 - val_loss: 2.9170\n",
      "Epoch 588/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.1831e-04 - val_loss: 2.9247\n",
      "Epoch 589/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0013 - val_loss: 2.9345\n",
      "Epoch 590/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.0109e-04 - val_loss: 2.9455\n",
      "Epoch 591/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.7500e-04 - val_loss: 2.9598\n",
      "Epoch 592/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0084 - val_loss: 2.9676\n",
      "Epoch 593/2000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 9.6034e-04 - val_loss: 2.9781\n",
      "Epoch 594/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.0207e-04 - val_loss: 2.9849\n",
      "Epoch 595/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.8968e-04 - val_loss: 2.9979\n",
      "Epoch 596/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.4107e-04 - val_loss: 3.0130\n",
      "Epoch 597/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0314 - val_loss: 2.9483\n",
      "Epoch 598/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.7483e-04 - val_loss: 2.8890\n",
      "Epoch 599/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0704 - val_loss: 2.8891\n",
      "Epoch 600/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0014 - val_loss: 2.8844\n",
      "Epoch 601/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.7986e-04 - val_loss: 2.8779\n",
      "Epoch 602/2000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0013 - val_loss: 2.8599\n",
      "Epoch 603/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0036 - val_loss: 2.8410\n",
      "Epoch 604/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.8027e-04 - val_loss: 2.8071\n",
      "Epoch 605/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.9478e-04 - val_loss: 2.7707\n",
      "Epoch 606/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0011 - val_loss: 2.7205\n",
      "Epoch 607/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0074 - val_loss: 2.7194\n",
      "Epoch 608/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.5240e-04 - val_loss: 2.7225\n",
      "Epoch 609/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0018 - val_loss: 2.7158\n",
      "Epoch 610/2000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0039 - val_loss: 2.6979\n",
      "Epoch 611/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.8962e-04 - val_loss: 2.6785\n",
      "Epoch 612/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0072 - val_loss: 2.6936\n",
      "Epoch 613/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.5980e-04 - val_loss: 2.7081\n",
      "Epoch 614/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0023 - val_loss: 2.7185\n",
      "Epoch 615/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0010 - val_loss: 2.7258\n",
      "Epoch 616/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.5431e-04 - val_loss: 2.7223\n",
      "Epoch 617/2000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0067 - val_loss: 2.7277\n",
      "Epoch 618/2000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 6.8439e-04 - val_loss: 2.7319\n",
      "Epoch 619/2000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.2748e-04 - val_loss: 2.7376\n",
      "Epoch 620/2000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.3519e-04 - val_loss: 2.7379\n",
      "Epoch 621/2000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.4684e-04 - val_loss: 2.7389\n",
      "Epoch 622/2000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.1705e-04 - val_loss: 2.7454\n",
      "Epoch 623/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0162 - val_loss: 2.7614\n",
      "Epoch 624/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.9937e-04 - val_loss: 2.7808\n",
      "Epoch 625/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.0392e-04 - val_loss: 2.7841\n",
      "Epoch 626/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0031 - val_loss: 2.7932\n",
      "Epoch 627/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0022 - val_loss: 2.8030\n",
      "Epoch 628/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0014 - val_loss: 2.8075\n",
      "Epoch 629/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.9102e-04 - val_loss: 2.8145\n",
      "Epoch 630/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.5964e-04 - val_loss: 2.8169\n",
      "Epoch 631/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0072 - val_loss: 2.7927\n",
      "Epoch 632/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.7922e-04 - val_loss: 2.7725\n",
      "Epoch 633/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.1615e-04 - val_loss: 2.7476\n",
      "Epoch 634/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.6822e-04 - val_loss: 2.7238\n",
      "Epoch 635/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.1441e-04 - val_loss: 2.7024\n",
      "Epoch 636/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1373 - val_loss: 1.5288\n",
      "Epoch 637/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0069 - val_loss: 0.5321\n",
      "Epoch 638/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.3728e-04 - val_loss: 0.1198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 639/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 6.5480e-04 - val_loss: 0.0539\n",
      "Epoch 640/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0014 - val_loss: 0.0461\n",
      "Epoch 641/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0029 - val_loss: 0.0823\n",
      "Epoch 642/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0011 - val_loss: 0.1938\n",
      "Epoch 643/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0010 - val_loss: 0.3650\n",
      "Epoch 644/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0081 - val_loss: 0.5594\n",
      "Epoch 645/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0188 - val_loss: 0.7376\n",
      "Epoch 646/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0023 - val_loss: 0.8860\n",
      "Epoch 647/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0010 - val_loss: 1.0219\n",
      "Epoch 648/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.3129e-04 - val_loss: 1.1332\n",
      "Epoch 649/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8.4204e-04 - val_loss: 1.2331\n",
      "Epoch 650/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0010 - val_loss: 1.3334\n",
      "Epoch 651/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0020 - val_loss: 1.3925\n",
      "Epoch 652/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.7260e-04 - val_loss: 1.4462\n",
      "Epoch 653/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0033 - val_loss: 1.5296\n",
      "Epoch 654/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.8032e-04 - val_loss: 1.5729\n",
      "Epoch 655/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.9855e-04 - val_loss: 1.5872\n",
      "Epoch 656/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0014 - val_loss: 1.5977\n",
      "Epoch 657/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.5497e-04 - val_loss: 1.6038\n",
      "Epoch 658/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.8675e-04 - val_loss: 1.5999\n",
      "Epoch 659/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.2840e-04 - val_loss: 1.5948\n",
      "Epoch 660/2000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0030 - val_loss: 1.6153\n",
      "Epoch 661/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0013 - val_loss: 1.6297\n",
      "Epoch 662/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 9.7388e-04 - val_loss: 1.6432\n",
      "Epoch 663/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0091 - val_loss: 1.6755\n",
      "Epoch 664/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.8155e-04 - val_loss: 1.6898\n",
      "Epoch 665/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 9.0446e-04 - val_loss: 1.7050\n",
      "Epoch 666/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0066 - val_loss: 1.6940\n",
      "Epoch 667/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.9694e-04 - val_loss: 1.6712\n",
      "Epoch 668/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.8596e-04 - val_loss: 1.6547\n",
      "Epoch 669/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.5949e-04 - val_loss: 1.6326\n",
      "Epoch 670/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.8963e-04 - val_loss: 1.6214\n",
      "Epoch 671/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0010 - val_loss: 1.6120\n",
      "Epoch 672/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.3475e-04 - val_loss: 1.5967\n",
      "Epoch 673/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.7894e-04 - val_loss: 1.5827\n",
      "Epoch 674/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.1205e-04 - val_loss: 1.5613\n",
      "Epoch 675/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 9.0962e-04 - val_loss: 1.5483\n",
      "Epoch 676/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.0236e-04 - val_loss: 1.5259\n",
      "Epoch 677/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.4624e-04 - val_loss: 1.5022\n",
      "Epoch 678/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0011 - val_loss: 1.4773\n",
      "Epoch 679/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0021 - val_loss: 1.4158\n",
      "Epoch 680/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.8339e-04 - val_loss: 1.3791\n",
      "Epoch 681/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.1193e-04 - val_loss: 1.3309\n",
      "Epoch 682/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.8211e-04 - val_loss: 1.2909\n",
      "Epoch 683/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0014 - val_loss: 1.2514\n",
      "Epoch 684/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.8308e-04 - val_loss: 1.2211\n",
      "Epoch 685/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8.0779e-04 - val_loss: 1.1826\n",
      "Epoch 686/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 9.8622e-04 - val_loss: 1.1654\n",
      "Epoch 687/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0056 - val_loss: 1.2137\n",
      "Epoch 688/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.9841e-04 - val_loss: 1.2806\n",
      "Epoch 689/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0011 - val_loss: 1.3037\n",
      "Epoch 690/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.2043e-04 - val_loss: 1.3542\n",
      "Epoch 691/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0017 - val_loss: 1.3911\n",
      "Epoch 692/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.6188e-04 - val_loss: 1.4227\n",
      "Epoch 693/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.1140e-04 - val_loss: 1.4381\n",
      "Epoch 694/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0137 - val_loss: 1.0567\n",
      "Epoch 695/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.8696e-04 - val_loss: 0.6797\n",
      "Epoch 696/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.9205e-04 - val_loss: 0.3809\n",
      "Epoch 697/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.5184e-04 - val_loss: 0.2082\n",
      "Epoch 698/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0035 - val_loss: 0.1232\n",
      "Epoch 699/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0015 - val_loss: 0.0901\n",
      "Epoch 700/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0011 - val_loss: 0.0876\n",
      "Epoch 701/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.8615e-04 - val_loss: 0.1011\n",
      "Epoch 702/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.0391e-04 - val_loss: 0.1249\n",
      "Epoch 703/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0010 - val_loss: 0.1638\n",
      "Epoch 704/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.7750e-04 - val_loss: 0.2171\n",
      "Epoch 705/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.0417e-04 - val_loss: 0.2940\n",
      "Epoch 706/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0012 - val_loss: 0.3890\n",
      "Epoch 707/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0020 - val_loss: 0.4722\n",
      "Epoch 708/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.0169e-04 - val_loss: 0.5585\n",
      "Epoch 709/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.3385e-04 - val_loss: 0.6384\n",
      "Epoch 710/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8.3770e-04 - val_loss: 0.7002\n",
      "Epoch 711/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.3089e-04 - val_loss: 0.7705\n",
      "Epoch 712/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.2737e-04 - val_loss: 0.8323\n",
      "Epoch 713/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.6509e-04 - val_loss: 0.8613\n",
      "Epoch 714/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0014 - val_loss: 0.8995\n",
      "Epoch 715/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0010 - val_loss: 0.9333\n",
      "Epoch 716/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0030 - val_loss: 0.9904\n",
      "Epoch 717/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.6589e-04 - val_loss: 1.0045\n",
      "Epoch 718/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.2966e-04 - val_loss: 1.0366\n",
      "Epoch 719/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.3445e-04 - val_loss: 1.0433\n",
      "Epoch 720/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.4129e-04 - val_loss: 1.0658\n",
      "Epoch 721/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.3308e-04 - val_loss: 1.0874\n",
      "Epoch 722/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0010 - val_loss: 1.0975\n",
      "Epoch 723/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.6683e-04 - val_loss: 1.1145\n",
      "Epoch 724/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0027 - val_loss: 1.1487\n",
      "Epoch 725/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.2426e-04 - val_loss: 1.1903\n",
      "Epoch 726/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.6740e-04 - val_loss: 1.2203\n",
      "Epoch 727/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.0807e-04 - val_loss: 1.2409\n",
      "Epoch 728/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.3949e-04 - val_loss: 1.2520\n",
      "Epoch 729/2000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 5.0188e-04 - val_loss: 1.2651\n",
      "Epoch 730/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.6781e-04 - val_loss: 1.2758\n",
      "Epoch 731/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 6.4515e-04 - val_loss: 1.2838\n",
      "Epoch 732/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.1521e-04 - val_loss: 1.2950\n",
      "Epoch 733/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.1184e-04 - val_loss: 1.2928\n",
      "Epoch 734/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0102 - val_loss: 1.2621\n",
      "Epoch 735/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.1087e-04 - val_loss: 1.2330\n",
      "Epoch 736/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.4748e-04 - val_loss: 1.2208\n",
      "Epoch 737/2000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5.0622e-04 - val_loss: 1.1941\n",
      "Epoch 738/2000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 5.5439e-04 - val_loss: 1.1723\n",
      "Epoch 739/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.9192e-04 - val_loss: 1.1592\n",
      "Epoch 740/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0020 - val_loss: 1.1360\n",
      "Epoch 741/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0014 - val_loss: 1.0691\n",
      "Epoch 742/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0326 - val_loss: 0.5097\n",
      "Epoch 743/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0042 - val_loss: 0.3326\n",
      "Epoch 744/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.3753e-04 - val_loss: 0.2448\n",
      "Epoch 745/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0012 - val_loss: 0.2053\n",
      "Epoch 746/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.7873e-04 - val_loss: 0.1935\n",
      "Epoch 747/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.4411e-04 - val_loss: 0.1805\n",
      "Epoch 748/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 6.9926e-04 - val_loss: 0.1730\n",
      "Epoch 749/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.9463e-04 - val_loss: 0.1530\n",
      "Epoch 750/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.3401e-04 - val_loss: 0.1337\n",
      "Epoch 751/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.8814e-04 - val_loss: 0.1160\n",
      "Epoch 752/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.9254e-04 - val_loss: 0.0999\n",
      "Epoch 753/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.7620e-04 - val_loss: 0.0862\n",
      "Epoch 754/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.0940e-04 - val_loss: 0.0747\n",
      "Epoch 755/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0014 - val_loss: 0.0668\n",
      "Epoch 756/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0034 - val_loss: 0.0617\n",
      "Epoch 757/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.2751e-04 - val_loss: 0.0576\n",
      "Epoch 758/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0051 - val_loss: 0.0536\n",
      "Epoch 759/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.9409e-04 - val_loss: 0.0514\n",
      "Epoch 760/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.8607e-04 - val_loss: 0.0505\n",
      "Epoch 761/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.2246e-04 - val_loss: 0.0507\n",
      "Epoch 762/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.0972e-04 - val_loss: 0.0524\n",
      "Epoch 763/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 9.2326e-04 - val_loss: 0.0548\n",
      "Epoch 764/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0016 - val_loss: 0.0576\n",
      "Epoch 765/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.7296e-04 - val_loss: 0.0604\n",
      "Epoch 766/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.1194e-04 - val_loss: 0.0636\n",
      "Epoch 767/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.1770e-04 - val_loss: 0.0661\n",
      "Epoch 768/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0017 - val_loss: 0.0695\n",
      "Epoch 769/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.4738e-04 - val_loss: 0.0722\n",
      "Epoch 770/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.3665e-04 - val_loss: 0.0750\n",
      "Epoch 771/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.3739e-04 - val_loss: 0.0781\n",
      "Epoch 772/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.9336e-04 - val_loss: 0.0818\n",
      "Epoch 773/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0016 - val_loss: 0.0820\n",
      "Epoch 774/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0013 - val_loss: 0.0832\n",
      "Epoch 775/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.3038e-04 - val_loss: 0.0819\n",
      "Epoch 776/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.0707e-04 - val_loss: 0.0815\n",
      "Epoch 777/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.5200e-04 - val_loss: 0.0807\n",
      "Epoch 778/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0010 - val_loss: 0.0795\n",
      "Epoch 779/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.7108e-04 - val_loss: 0.0793\n",
      "Epoch 780/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.4049e-04 - val_loss: 0.0787\n",
      "Epoch 781/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.4560e-04 - val_loss: 0.0790\n",
      "Epoch 782/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.3362e-04 - val_loss: 0.0792\n",
      "Epoch 783/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.5706e-04 - val_loss: 0.0800\n",
      "Epoch 784/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0011 - val_loss: 0.0819\n",
      "Epoch 785/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.1007e-04 - val_loss: 0.0827\n",
      "Epoch 786/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0091 - val_loss: 0.0875\n",
      "Epoch 787/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 9.3340e-04 - val_loss: 0.0900\n",
      "Epoch 788/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.5023e-04 - val_loss: 0.0949\n",
      "Epoch 789/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.6734e-04 - val_loss: 0.0984\n",
      "Epoch 790/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0099 - val_loss: 0.1264\n",
      "Epoch 791/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.8443e-04 - val_loss: 0.1585\n",
      "Epoch 792/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.3735e-04 - val_loss: 0.1923\n",
      "Epoch 793/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.4135e-04 - val_loss: 0.2259\n",
      "Epoch 794/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.5940e-04 - val_loss: 0.2622\n",
      "Epoch 795/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.5336e-04 - val_loss: 0.2976\n",
      "Epoch 796/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.3504e-04 - val_loss: 0.3310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 797/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.6094e-04 - val_loss: 0.3700\n",
      "Epoch 798/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0109 - val_loss: 0.3861\n",
      "Epoch 799/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.1672e-04 - val_loss: 0.4070\n",
      "Epoch 800/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0012 - val_loss: 0.4223\n",
      "Epoch 801/2000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 9.7440e-04 - val_loss: 0.4354\n",
      "Epoch 802/2000\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 5.7938e-04 - val_loss: 0.4466\n",
      "Epoch 803/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.6521e-04 - val_loss: 0.4591\n",
      "Epoch 804/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.3208e-04 - val_loss: 0.4695\n",
      "Epoch 805/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.2158e-04 - val_loss: 0.4767\n",
      "Epoch 806/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.1731e-04 - val_loss: 0.4770\n",
      "Epoch 807/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.1260e-04 - val_loss: 0.4908\n",
      "Epoch 808/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 5.6734e-04 - val_loss: 0.4923\n",
      "Epoch 809/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.0510e-04 - val_loss: 0.4895\n",
      "Epoch 810/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.2945e-04 - val_loss: 0.4944\n",
      "Epoch 811/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.9966e-04 - val_loss: 0.5010\n",
      "Epoch 812/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.0912e-04 - val_loss: 0.4954\n",
      "Epoch 813/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0011 - val_loss: 0.4962\n",
      "Epoch 814/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.8195e-04 - val_loss: 0.5059\n",
      "Epoch 815/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.1738e-04 - val_loss: 0.5100\n",
      "Epoch 816/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.8764e-04 - val_loss: 0.5145\n",
      "Epoch 817/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0053 - val_loss: 0.6110\n",
      "Epoch 818/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0022 - val_loss: 0.6815\n",
      "Epoch 819/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.3505e-04 - val_loss: 0.7484\n",
      "Epoch 820/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.5211e-04 - val_loss: 0.8030\n",
      "Epoch 821/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.4117e-04 - val_loss: 0.8516\n",
      "Epoch 822/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0026 - val_loss: 0.9014\n",
      "Epoch 823/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.9517e-04 - val_loss: 0.9529\n",
      "Epoch 824/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.8016e-04 - val_loss: 0.9959\n",
      "Epoch 825/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.5253e-04 - val_loss: 1.0223\n",
      "Epoch 826/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1587 - val_loss: 1.2903\n",
      "Epoch 827/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0016 - val_loss: 1.5400\n",
      "Epoch 828/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.6023e-04 - val_loss: 1.7197\n",
      "Epoch 829/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.7851e-04 - val_loss: 1.8500\n",
      "Epoch 830/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.0940e-04 - val_loss: 1.9190\n",
      "Epoch 831/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.6080e-04 - val_loss: 1.9493\n",
      "Epoch 832/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.5575e-04 - val_loss: 1.9568\n",
      "Epoch 833/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.9502e-04 - val_loss: 1.9649\n",
      "Epoch 834/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.1291e-04 - val_loss: 1.9679\n",
      "Epoch 835/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.8635e-04 - val_loss: 1.9715\n",
      "Epoch 836/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0016 - val_loss: 1.9477\n",
      "Epoch 837/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.3461e-04 - val_loss: 1.9144\n",
      "Epoch 838/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.9589e-04 - val_loss: 1.8847\n",
      "Epoch 839/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.9510e-04 - val_loss: 1.8517\n",
      "Epoch 840/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.6924e-04 - val_loss: 1.8272\n",
      "Epoch 841/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.2875e-04 - val_loss: 1.8035\n",
      "Epoch 842/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.7422e-04 - val_loss: 1.7722\n",
      "Epoch 843/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.1035e-04 - val_loss: 1.7434\n",
      "Epoch 844/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0014 - val_loss: 1.7417\n",
      "Epoch 845/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.8949e-04 - val_loss: 1.7521\n",
      "Epoch 846/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.9600e-04 - val_loss: 1.7540\n",
      "Epoch 847/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.4460e-04 - val_loss: 1.7560\n",
      "Epoch 848/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.9605e-04 - val_loss: 1.7511\n",
      "Epoch 849/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.1963e-04 - val_loss: 1.7436\n",
      "Epoch 850/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.9737e-04 - val_loss: 1.7325\n",
      "Epoch 851/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0011 - val_loss: 1.7405\n",
      "Epoch 852/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.0937e-04 - val_loss: 1.7360\n",
      "Epoch 853/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.0965e-04 - val_loss: 1.7332\n",
      "Epoch 854/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.2066e-04 - val_loss: 1.7231\n",
      "Epoch 855/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.3986e-04 - val_loss: 1.7127\n",
      "Epoch 856/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 1.5782\n",
      "Epoch 857/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0031 - val_loss: 1.4500\n",
      "Epoch 858/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.0235e-04 - val_loss: 1.3360\n",
      "Epoch 859/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.1483e-04 - val_loss: 1.2769\n",
      "Epoch 860/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0047 - val_loss: 1.1643\n",
      "Epoch 861/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0012 - val_loss: 1.0064\n",
      "Epoch 862/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.0119e-04 - val_loss: 0.8617\n",
      "Epoch 863/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.3847e-04 - val_loss: 0.7133\n",
      "Epoch 864/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.2141e-04 - val_loss: 0.5996\n",
      "Epoch 865/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.1790e-04 - val_loss: 0.5056\n",
      "Epoch 866/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.1485e-04 - val_loss: 0.4314\n",
      "Epoch 867/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.6111e-04 - val_loss: 0.3655\n",
      "Epoch 868/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.6110e-04 - val_loss: 0.3133\n",
      "Epoch 869/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.3618e-04 - val_loss: 0.2805\n",
      "Epoch 870/2000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.8526e-04 - val_loss: 0.2487\n",
      "Epoch 871/2000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 5.2954e-04 - val_loss: 0.2240\n",
      "Epoch 872/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.0386e-04 - val_loss: 0.2061\n",
      "Epoch 873/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.7060e-04 - val_loss: 0.1881\n",
      "Epoch 874/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0015 - val_loss: 0.1717\n",
      "Epoch 875/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.6808e-04 - val_loss: 0.1528\n",
      "Epoch 876/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.2966e-04 - val_loss: 0.1416\n",
      "Epoch 877/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.6138e-04 - val_loss: 0.1285\n",
      "Epoch 878/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.2830e-04 - val_loss: 0.1232\n",
      "Epoch 879/2000\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0230 - val_loss: 0.1227\n",
      "Epoch 880/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.7497e-04 - val_loss: 0.1330\n",
      "Epoch 881/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.4366e-04 - val_loss: 0.1475\n",
      "Epoch 882/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.2535e-04 - val_loss: 0.1644\n",
      "Epoch 883/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.8612e-04 - val_loss: 0.1723\n",
      "Epoch 884/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.5365e-04 - val_loss: 0.1473\n",
      "Epoch 885/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.9442e-04 - val_loss: 0.1269\n",
      "Epoch 886/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.1198e-04 - val_loss: 0.1235\n",
      "Epoch 887/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.4760e-04 - val_loss: 0.1093\n",
      "Epoch 888/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.9404e-04 - val_loss: 0.1071\n",
      "Epoch 889/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0018 - val_loss: 0.0962\n",
      "Epoch 890/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.1857e-04 - val_loss: 0.0884\n",
      "Epoch 891/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.4424e-04 - val_loss: 0.0806\n",
      "Epoch 892/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0271 - val_loss: 0.0579\n",
      "Epoch 893/2000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0014 - val_loss: 0.0395\n",
      "Epoch 894/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.7047e-04 - val_loss: 0.0234\n",
      "Epoch 895/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.5067e-04 - val_loss: 0.0256\n",
      "Epoch 896/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.0894e-04 - val_loss: 0.1097\n",
      "Epoch 897/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5.3801e-04 - val_loss: 0.3866\n",
      "Epoch 898/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.7901e-04 - val_loss: 0.9047\n",
      "Epoch 899/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0014 - val_loss: 1.4171\n",
      "Epoch 900/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.7205e-04 - val_loss: 1.8842\n",
      "Epoch 901/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0011 - val_loss: 2.2564\n",
      "Epoch 902/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0013 - val_loss: 2.5718\n",
      "Epoch 903/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.1526e-04 - val_loss: 2.8713\n",
      "Epoch 904/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.1601e-04 - val_loss: 3.0943\n",
      "Epoch 905/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.4431e-04 - val_loss: 3.2822\n",
      "Epoch 906/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8.8130e-04 - val_loss: 3.4245\n",
      "Epoch 907/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.7387e-04 - val_loss: 3.5338\n",
      "Epoch 908/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.6320e-04 - val_loss: 3.6058\n",
      "Epoch 909/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.4225e-04 - val_loss: 3.6727\n",
      "Epoch 910/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0012 - val_loss: 3.6947\n",
      "Epoch 911/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.9798e-04 - val_loss: 3.7372\n",
      "Epoch 912/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0041 - val_loss: 3.7892\n",
      "Epoch 913/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.5077e-04 - val_loss: 3.8359\n",
      "Epoch 914/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.0475e-04 - val_loss: 3.8723\n",
      "Epoch 915/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.2928e-04 - val_loss: 3.5151\n",
      "Epoch 916/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0011 - val_loss: 2.8730\n",
      "Epoch 917/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0014 - val_loss: 2.0015\n",
      "Epoch 918/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0045 - val_loss: 1.2181\n",
      "Epoch 919/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.9246e-04 - val_loss: 0.5435\n",
      "Epoch 920/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.5634e-04 - val_loss: 0.1419\n",
      "Epoch 921/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0028 - val_loss: 0.0474\n",
      "Epoch 922/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.9715e-04 - val_loss: 0.0539\n",
      "Epoch 923/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.4546e-04 - val_loss: 0.1441\n",
      "Epoch 924/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.9575e-04 - val_loss: 0.3521\n",
      "Epoch 925/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.1772e-04 - val_loss: 0.7116\n",
      "Epoch 926/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0011 - val_loss: 1.1749\n",
      "Epoch 927/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.8869e-04 - val_loss: 1.6144\n",
      "Epoch 928/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1072 - val_loss: 2.4645\n",
      "Epoch 929/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0098 - val_loss: 2.5569\n",
      "Epoch 930/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0017 - val_loss: 2.4702\n",
      "Epoch 931/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.6278e-04 - val_loss: 2.3988\n",
      "Epoch 932/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.2210e-04 - val_loss: 2.3408\n",
      "Epoch 933/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.5285e-04 - val_loss: 2.3138\n",
      "Epoch 934/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.5422e-04 - val_loss: 2.2406\n",
      "Epoch 935/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.8560e-04 - val_loss: 2.2022\n",
      "Epoch 936/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.9411e-04 - val_loss: 2.1238\n",
      "Epoch 937/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.6829e-04 - val_loss: 2.0296\n",
      "Epoch 938/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0137 - val_loss: 2.1491\n",
      "Epoch 939/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0021 - val_loss: 2.2073\n",
      "Epoch 940/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0012 - val_loss: 2.2451\n",
      "Epoch 941/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.5980e-04 - val_loss: 2.2697\n",
      "Epoch 942/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5.9244e-04 - val_loss: 2.2738\n",
      "Epoch 943/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0014 - val_loss: 2.2760\n",
      "Epoch 944/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.1016e-04 - val_loss: 2.2878\n",
      "Epoch 945/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7.7145e-04 - val_loss: 2.2848\n",
      "Epoch 946/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.9260e-04 - val_loss: 2.2862\n",
      "Epoch 947/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.3176e-04 - val_loss: 2.2917\n",
      "Epoch 948/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.2439e-04 - val_loss: 2.3088\n",
      "Epoch 949/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.5232e-04 - val_loss: 2.3088\n",
      "Epoch 950/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.9923e-04 - val_loss: 2.3118\n",
      "Epoch 951/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.3170e-04 - val_loss: 2.3190\n",
      "Epoch 952/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.4304e-04 - val_loss: 2.3300\n",
      "Epoch 953/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.8807e-04 - val_loss: 2.3330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 954/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.9873e-04 - val_loss: 2.3353\n",
      "Epoch 955/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0010 - val_loss: 2.3340\n",
      "Epoch 956/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.2834e-04 - val_loss: 2.3332\n",
      "Epoch 957/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.3133e-04 - val_loss: 2.3334\n",
      "Epoch 958/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.1036e-04 - val_loss: 2.3490\n",
      "Epoch 959/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.1016e-04 - val_loss: 2.3513\n",
      "Epoch 960/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.9513e-04 - val_loss: 2.3471\n",
      "Epoch 961/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0031 - val_loss: 2.3497\n",
      "Epoch 962/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.2304e-04 - val_loss: 2.3536\n",
      "Epoch 963/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0018 - val_loss: 2.3765\n",
      "Epoch 964/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.1223e-04 - val_loss: 2.3862\n",
      "Epoch 965/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.4987e-04 - val_loss: 2.3991\n",
      "Epoch 966/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.8665e-04 - val_loss: 2.4040\n",
      "Epoch 967/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.6705e-04 - val_loss: 2.4033\n",
      "Epoch 968/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.6590e-04 - val_loss: 2.4047\n",
      "Epoch 969/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0138 - val_loss: 2.3446\n",
      "Epoch 970/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0010 - val_loss: 2.2811\n",
      "Epoch 971/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0035 - val_loss: 2.2143\n",
      "Epoch 972/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.3396e-04 - val_loss: 2.1466\n",
      "Epoch 973/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7.4475e-04 - val_loss: 2.0761\n",
      "Epoch 974/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.1835e-04 - val_loss: 1.9914\n",
      "Epoch 975/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.8426e-04 - val_loss: 1.9112\n",
      "Epoch 976/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.7144e-04 - val_loss: 1.8426\n",
      "Epoch 977/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.3334e-04 - val_loss: 1.7865\n",
      "Epoch 978/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.5412e-04 - val_loss: 1.7241\n",
      "Epoch 979/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.6806e-04 - val_loss: 1.6671\n",
      "Epoch 980/2000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.7866e-04 - val_loss: 1.6330\n",
      "Epoch 981/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0027 - val_loss: 1.5927\n",
      "Epoch 982/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.1493e-04 - val_loss: 1.5681\n",
      "Epoch 983/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.9933e-04 - val_loss: 1.5363\n",
      "Epoch 984/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.7731e-04 - val_loss: 1.5403\n",
      "Epoch 985/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.0551e-04 - val_loss: 1.5104\n",
      "Epoch 986/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.4653e-04 - val_loss: 1.4808\n",
      "Epoch 987/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.6536e-04 - val_loss: 1.4583\n",
      "Epoch 988/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.9862e-04 - val_loss: 1.4282\n",
      "Epoch 989/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.4422e-04 - val_loss: 1.4041\n",
      "Epoch 990/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.1687e-04 - val_loss: 1.3780\n",
      "Epoch 991/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0012 - val_loss: 1.4005\n",
      "Epoch 992/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.8149e-04 - val_loss: 1.3780\n",
      "Epoch 993/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.2724e-04 - val_loss: 1.3623\n",
      "Epoch 994/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.3576e-04 - val_loss: 1.3474\n",
      "Epoch 995/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.5050e-04 - val_loss: 1.3378\n",
      "Epoch 996/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0027 - val_loss: 1.5048\n",
      "Epoch 997/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0011 - val_loss: 1.6516\n",
      "Epoch 998/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.0397e-04 - val_loss: 1.7794\n",
      "Epoch 999/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0047 - val_loss: 1.8888\n",
      "Epoch 1000/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.2046e-04 - val_loss: 1.9961\n",
      "Epoch 1001/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.1419e-04 - val_loss: 2.0819\n",
      "Epoch 1002/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.3154e-04 - val_loss: 2.1482\n",
      "Epoch 1003/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0012 - val_loss: 2.2169\n",
      "Epoch 1004/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.3409e-04 - val_loss: 2.2679\n",
      "Epoch 1005/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.6182e-04 - val_loss: 2.3117\n",
      "Epoch 1006/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.8261e-04 - val_loss: 2.3575\n",
      "Epoch 1007/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.0225e-04 - val_loss: 2.4065\n",
      "Epoch 1008/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.5583e-04 - val_loss: 2.4392\n",
      "Epoch 1009/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.1052e-04 - val_loss: 2.4944\n",
      "Epoch 1010/2000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 5.6895e-04 - val_loss: 2.5254\n",
      "Epoch 1011/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4.8822e-04 - val_loss: 2.5603\n",
      "Epoch 1012/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.4676e-04 - val_loss: 2.5898\n",
      "Epoch 1013/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.4581e-04 - val_loss: 2.6171\n",
      "Epoch 1014/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.2308e-04 - val_loss: 2.6601\n",
      "Epoch 1015/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.9746e-04 - val_loss: 2.6822\n",
      "Epoch 1016/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.0712e-04 - val_loss: 2.7036\n",
      "Epoch 1017/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.5085e-04 - val_loss: 2.7342\n",
      "Epoch 1018/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.8784e-04 - val_loss: 2.7532\n",
      "Epoch 1019/2000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0034 - val_loss: 2.7952\n",
      "Epoch 1020/2000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 6.1079e-04 - val_loss: 2.8356\n",
      "Epoch 1021/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1375 - val_loss: 2.8738\n",
      "Epoch 1022/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0137 - val_loss: 2.9129\n",
      "Epoch 1023/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.0020e-04 - val_loss: 2.9221\n",
      "Epoch 1024/2000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.0497e-04 - val_loss: 2.9224\n",
      "Epoch 1025/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 9.2735e-04 - val_loss: 2.9209\n",
      "Epoch 1026/2000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.5198e-04 - val_loss: 2.9131\n",
      "Epoch 1027/2000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.1010e-04 - val_loss: 2.9152\n",
      "Epoch 1028/2000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0044 - val_loss: 2.9276\n",
      "Epoch 1029/2000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0045 - val_loss: 2.8440\n",
      "Epoch 1030/2000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.6014e-04 - val_loss: 2.7763\n",
      "Epoch 1031/2000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0036 - val_loss: 2.7520\n",
      "Epoch 1032/2000\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.9073e-04 - val_loss: 2.7189\n",
      "Epoch 1033/2000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.3583e-04 - val_loss: 2.7235\n",
      "Epoch 1034/2000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.0374e-04 - val_loss: 2.6917\n",
      "Epoch 1035/2000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.8932e-04 - val_loss: 2.6552\n",
      "Epoch 1036/2000\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.8002e-04 - val_loss: 2.6199\n",
      "Epoch 1037/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.9137e-04 - val_loss: 2.5883\n",
      "Epoch 1038/2000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.1838e-04 - val_loss: 2.5718\n",
      "Epoch 1039/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0028 - val_loss: 2.5613\n",
      "Epoch 1040/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.9001e-04 - val_loss: 2.5383\n",
      "Epoch 1041/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.3616e-04 - val_loss: 2.5090\n",
      "Epoch 1042/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.1295e-04 - val_loss: 2.4900\n",
      "Epoch 1043/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.9681e-04 - val_loss: 2.4749\n",
      "Epoch 1044/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.1074e-04 - val_loss: 2.4476\n",
      "Epoch 1045/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0023 - val_loss: 2.4184\n",
      "Epoch 1046/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.3802e-04 - val_loss: 2.4054\n",
      "Epoch 1047/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.9509e-04 - val_loss: 2.3972\n",
      "Epoch 1048/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0015 - val_loss: 2.4265\n",
      "Epoch 1049/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.6831e-04 - val_loss: 2.4062\n",
      "Epoch 1050/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.0843e-04 - val_loss: 2.4019\n",
      "Epoch 1051/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.3746e-04 - val_loss: 2.4140\n",
      "Epoch 1052/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.8332e-04 - val_loss: 2.4705\n",
      "Epoch 1053/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.7001e-04 - val_loss: 2.4670\n",
      "Epoch 1054/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.2058e-04 - val_loss: 2.4549\n",
      "Epoch 1055/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7744e-04 - val_loss: 2.4526\n",
      "Epoch 1056/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.1025e-04 - val_loss: 2.4474\n",
      "Epoch 1057/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.1291e-04 - val_loss: 2.4327\n",
      "Epoch 1058/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.7463e-04 - val_loss: 2.4355\n",
      "Epoch 1059/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.4129e-04 - val_loss: 2.4321\n",
      "Epoch 1060/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.0704e-04 - val_loss: 2.4333\n",
      "Epoch 1061/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.6155e-04 - val_loss: 2.4362\n",
      "Epoch 1062/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0044 - val_loss: 2.5491\n",
      "Epoch 1063/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.3806e-04 - val_loss: 2.6068\n",
      "Epoch 1064/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.1557e-04 - val_loss: 2.6505\n",
      "Epoch 1065/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.0013e-04 - val_loss: 2.7232\n",
      "Epoch 1066/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.2924e-04 - val_loss: 2.7541\n",
      "Epoch 1067/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.9188e-04 - val_loss: 2.7927\n",
      "Epoch 1068/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.7134e-04 - val_loss: 2.8163\n",
      "Epoch 1069/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.8404e-04 - val_loss: 2.8446\n",
      "Epoch 1070/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.5970e-04 - val_loss: 2.8740\n",
      "Epoch 1071/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.4148e-04 - val_loss: 2.8978\n",
      "Epoch 1072/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.1659e-04 - val_loss: 2.9275\n",
      "Epoch 1073/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.2526e-04 - val_loss: 2.9431\n",
      "Epoch 1074/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.8825e-04 - val_loss: 2.9676\n",
      "Epoch 1075/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.3743e-04 - val_loss: 2.9894\n",
      "Epoch 1076/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.0792e-04 - val_loss: 3.0089\n",
      "Epoch 1077/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.7528e-04 - val_loss: 3.0322\n",
      "Epoch 1078/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.0332e-04 - val_loss: 3.0485\n",
      "Epoch 1079/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.4683e-04 - val_loss: 3.0642\n",
      "Epoch 1080/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0012 - val_loss: 3.0881\n",
      "Epoch 1081/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.0599e-04 - val_loss: 3.1067\n",
      "Epoch 1082/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.9087e-04 - val_loss: 3.1273\n",
      "Epoch 1083/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.6992e-04 - val_loss: 3.1498\n",
      "Epoch 1084/2000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.3118e-04 - val_loss: 3.1645\n",
      "Epoch 1085/2000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5190e-04 - val_loss: 3.1819\n",
      "Epoch 1086/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.3392e-04 - val_loss: 3.2010\n",
      "Epoch 1087/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.5606e-04 - val_loss: 3.2158\n",
      "Epoch 1088/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.8546e-04 - val_loss: 3.2269\n",
      "Epoch 1089/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.2071e-04 - val_loss: 3.2472\n",
      "Epoch 1090/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.7669e-04 - val_loss: 3.2972\n",
      "Epoch 1091/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.1821e-04 - val_loss: 3.3129\n",
      "Epoch 1092/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.2626e-04 - val_loss: 3.3286\n",
      "Epoch 1093/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0061 - val_loss: 3.3708\n",
      "Epoch 1094/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.2007e-04 - val_loss: 3.4261\n",
      "Epoch 1095/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.0686e-04 - val_loss: 3.4631\n",
      "Epoch 1096/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.5350e-04 - val_loss: 3.4946\n",
      "Epoch 1097/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.7553e-04 - val_loss: 3.5132\n",
      "Epoch 1098/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.2085e-04 - val_loss: 3.5465\n",
      "Epoch 1099/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5562e-04 - val_loss: 3.5594\n",
      "Epoch 1100/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.7069e-04 - val_loss: 3.5708\n",
      "Epoch 1101/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.2949e-04 - val_loss: 3.5855\n",
      "Epoch 1102/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.3450e-04 - val_loss: 3.6051\n",
      "Epoch 1103/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0138 - val_loss: 3.6919\n",
      "Epoch 1104/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.2197e-04 - val_loss: 3.7272\n",
      "Epoch 1105/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0025 - val_loss: 3.7553\n",
      "Epoch 1106/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.1983e-04 - val_loss: 3.7736\n",
      "Epoch 1107/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.9510e-04 - val_loss: 3.7925\n",
      "Epoch 1108/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.1990e-04 - val_loss: 3.8089\n",
      "Epoch 1109/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step - loss: 2.8424e-04 - val_loss: 3.8271\n",
      "Epoch 1110/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 9.0844e-04 - val_loss: 3.8522\n",
      "Epoch 1111/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.2446e-04 - val_loss: 3.8652\n",
      "Epoch 1112/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.3805e-04 - val_loss: 3.8808\n",
      "Epoch 1113/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5287e-04 - val_loss: 3.8949\n",
      "Epoch 1114/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.0902e-04 - val_loss: 3.9070\n",
      "Epoch 1115/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.7648e-04 - val_loss: 3.9171\n",
      "Epoch 1116/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.6390e-04 - val_loss: 3.9301\n",
      "Epoch 1117/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.6959e-04 - val_loss: 3.9450\n",
      "Epoch 1118/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.7709e-04 - val_loss: 3.9601\n",
      "Epoch 1119/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0023 - val_loss: 3.9624\n",
      "Epoch 1120/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0080 - val_loss: 4.0188\n",
      "Epoch 1121/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.0654e-04 - val_loss: 4.0647\n",
      "Epoch 1122/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.5301e-04 - val_loss: 4.1033\n",
      "Epoch 1123/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.9393e-04 - val_loss: 4.1384\n",
      "Epoch 1124/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.2113e-04 - val_loss: 4.1789\n",
      "Epoch 1125/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.6366e-04 - val_loss: 4.2240\n",
      "Epoch 1126/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.3329e-04 - val_loss: 4.2630\n",
      "Epoch 1127/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.7214e-04 - val_loss: 4.2957\n",
      "Epoch 1128/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.0236e-04 - val_loss: 4.3378\n",
      "Epoch 1129/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.4696e-04 - val_loss: 4.3573\n",
      "Epoch 1130/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.3107e-04 - val_loss: 4.3753\n",
      "Epoch 1131/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.9949e-04 - val_loss: 4.3885\n",
      "Epoch 1132/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.9565e-04 - val_loss: 4.4058\n",
      "Epoch 1133/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.0296e-04 - val_loss: 4.4160\n",
      "Epoch 1134/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.9131e-04 - val_loss: 4.4277\n",
      "Epoch 1135/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.6261e-04 - val_loss: 4.4378\n",
      "Epoch 1136/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.7327e-04 - val_loss: 4.4485\n",
      "Epoch 1137/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.5432e-04 - val_loss: 4.4591\n",
      "Epoch 1138/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.7590e-04 - val_loss: 4.4660\n",
      "Epoch 1139/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.6424e-04 - val_loss: 4.4733\n",
      "Epoch 1140/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.5233e-04 - val_loss: 4.4817\n",
      "Epoch 1141/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0144 - val_loss: 4.5692\n",
      "Epoch 1142/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.4886e-04 - val_loss: 4.6037\n",
      "Epoch 1143/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.5334e-04 - val_loss: 4.5995\n",
      "Epoch 1144/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.9891e-04 - val_loss: 4.5744\n",
      "Epoch 1145/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.7311e-04 - val_loss: 4.5482\n",
      "Epoch 1146/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.6341e-04 - val_loss: 4.5236\n",
      "Epoch 1147/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.7854e-04 - val_loss: 4.4946\n",
      "Epoch 1148/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0011 - val_loss: 4.4693\n",
      "Epoch 1149/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.2830e-04 - val_loss: 4.4444\n",
      "Epoch 1150/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.4995e-04 - val_loss: 4.4203\n",
      "Epoch 1151/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.6162e-04 - val_loss: 4.3982\n",
      "Epoch 1152/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.2094e-04 - val_loss: 4.3767\n",
      "Epoch 1153/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.1192e-04 - val_loss: 4.3581\n",
      "Epoch 1154/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0017 - val_loss: 4.3416\n",
      "Epoch 1155/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.3710e-04 - val_loss: 4.3235\n",
      "Epoch 1156/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.8281e-04 - val_loss: 4.3077\n",
      "Epoch 1157/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.1952e-04 - val_loss: 4.2894\n",
      "Epoch 1158/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.9746e-04 - val_loss: 4.2780\n",
      "Epoch 1159/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.8734e-04 - val_loss: 4.2653\n",
      "Epoch 1160/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5919e-04 - val_loss: 4.2538\n",
      "Epoch 1161/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.1730e-04 - val_loss: 4.2445\n",
      "Epoch 1162/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.5122e-04 - val_loss: 4.2364\n",
      "Epoch 1163/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.9723e-04 - val_loss: 4.2295\n",
      "Epoch 1164/2000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.4427e-04 - val_loss: 4.2242\n",
      "Epoch 1165/2000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.4763e-04 - val_loss: 4.2198\n",
      "Epoch 1166/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.5409e-04 - val_loss: 4.2150\n",
      "Epoch 1167/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.7894e-04 - val_loss: 4.2108\n",
      "Epoch 1168/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.1456e-04 - val_loss: 4.2088\n",
      "Epoch 1169/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.4413e-04 - val_loss: 4.2067\n",
      "Epoch 1170/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.8128e-04 - val_loss: 4.2057\n",
      "Epoch 1171/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.4042e-04 - val_loss: 4.2069\n",
      "Epoch 1172/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.3832e-04 - val_loss: 4.2068\n",
      "Epoch 1173/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.6469e-04 - val_loss: 4.2070\n",
      "Epoch 1174/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.2795e-04 - val_loss: 4.2092\n",
      "Epoch 1175/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7.2604e-04 - val_loss: 4.2047\n",
      "Epoch 1176/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.8743e-04 - val_loss: 4.2017\n",
      "Epoch 1177/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0020 - val_loss: 4.2045\n",
      "Epoch 1178/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.3020e-04 - val_loss: 4.2031\n",
      "Epoch 1179/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.6865e-04 - val_loss: 4.2017\n",
      "Epoch 1180/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.0621e-04 - val_loss: 4.1998\n",
      "Epoch 1181/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.8541e-04 - val_loss: 4.1984\n",
      "Epoch 1182/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.3888e-04 - val_loss: 4.1977\n",
      "Epoch 1183/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.4616e-04 - val_loss: 4.1973\n",
      "Epoch 1184/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.2828e-04 - val_loss: 4.1942\n",
      "Epoch 1185/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.7551e-04 - val_loss: 4.1928\n",
      "Epoch 1186/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.4533e-04 - val_loss: 4.1904\n",
      "Epoch 1187/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.2396e-04 - val_loss: 4.1891\n",
      "Epoch 1188/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0082 - val_loss: 4.2232\n",
      "Epoch 1189/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2275e-04 - val_loss: 4.2438\n",
      "Epoch 1190/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.5758e-04 - val_loss: 4.2494\n",
      "Epoch 1191/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.3344e-04 - val_loss: 4.2486\n",
      "Epoch 1192/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.0331e-04 - val_loss: 4.2475\n",
      "Epoch 1193/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.4231e-04 - val_loss: 4.2451\n",
      "Epoch 1194/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5372e-04 - val_loss: 4.2436\n",
      "Epoch 1195/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.3926e-04 - val_loss: 4.2458\n",
      "Epoch 1196/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.8886e-04 - val_loss: 4.2531\n",
      "Epoch 1197/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.3769e-04 - val_loss: 4.2573\n",
      "Epoch 1198/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.3958e-04 - val_loss: 4.2602\n",
      "Epoch 1199/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.3668e-04 - val_loss: 4.2617\n",
      "Epoch 1200/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.6535e-04 - val_loss: 4.2602\n",
      "Epoch 1201/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.7354e-04 - val_loss: 4.2570\n",
      "Epoch 1202/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.5898e-04 - val_loss: 4.2508\n",
      "Epoch 1203/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.6639e-04 - val_loss: 4.2423\n",
      "Epoch 1204/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.7690e-04 - val_loss: 4.2309\n",
      "Epoch 1205/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.1391e-04 - val_loss: 4.2217\n",
      "Epoch 1206/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.4877e-04 - val_loss: 4.2139\n",
      "Epoch 1207/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.3782e-04 - val_loss: 4.2037\n",
      "Epoch 1208/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.0069e-04 - val_loss: 4.1902\n",
      "Epoch 1209/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.3901e-04 - val_loss: 4.1780\n",
      "Epoch 1210/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1957e-04 - val_loss: 4.1630\n",
      "Epoch 1211/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1202e-04 - val_loss: 4.1473\n",
      "Epoch 1212/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2602e-04 - val_loss: 4.1328\n",
      "Epoch 1213/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0095 - val_loss: 4.0681\n",
      "Epoch 1214/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.0718e-04 - val_loss: 3.9852\n",
      "Epoch 1215/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.4755e-04 - val_loss: 3.8971\n",
      "Epoch 1216/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5450e-04 - val_loss: 3.8040\n",
      "Epoch 1217/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0010 - val_loss: 3.6759\n",
      "Epoch 1218/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.4330e-04 - val_loss: 3.5403\n",
      "Epoch 1219/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.8027e-04 - val_loss: 3.4042\n",
      "Epoch 1220/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.3593e-04 - val_loss: 3.2790\n",
      "Epoch 1221/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.2497e-04 - val_loss: 3.1560\n",
      "Epoch 1222/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.4201e-04 - val_loss: 3.0385\n",
      "Epoch 1223/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1516e-04 - val_loss: 2.9269\n",
      "Epoch 1224/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.9196e-04 - val_loss: 2.8114\n",
      "Epoch 1225/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.2534e-04 - val_loss: 2.7122\n",
      "Epoch 1226/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.4072e-04 - val_loss: 2.6241\n",
      "Epoch 1227/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.9577e-04 - val_loss: 2.5436\n",
      "Epoch 1228/2000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.2280e-04 - val_loss: 2.4744\n",
      "Epoch 1229/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.3385e-04 - val_loss: 2.4087\n",
      "Epoch 1230/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.1837e-04 - val_loss: 2.3492\n",
      "Epoch 1231/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.0139e-04 - val_loss: 2.3025\n",
      "Epoch 1232/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.8600e-04 - val_loss: 2.2662\n",
      "Epoch 1233/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.0189e-04 - val_loss: 2.2227\n",
      "Epoch 1234/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.4257e-04 - val_loss: 2.1872\n",
      "Epoch 1235/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.6111e-04 - val_loss: 2.1496\n",
      "Epoch 1236/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5287e-04 - val_loss: 2.1096\n",
      "Epoch 1237/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.3684e-04 - val_loss: 2.0731\n",
      "Epoch 1238/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.3554e-04 - val_loss: 2.0360\n",
      "Epoch 1239/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0732 - val_loss: 2.2395\n",
      "Epoch 1240/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.4595e-04 - val_loss: 3.1954\n",
      "Epoch 1241/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.6137e-04 - val_loss: 3.3997\n",
      "Epoch 1242/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0010 - val_loss: 3.1887\n",
      "Epoch 1243/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.4844e-04 - val_loss: 2.7011\n",
      "Epoch 1244/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.2048e-04 - val_loss: 1.9800\n",
      "Epoch 1245/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.7166e-04 - val_loss: 1.2940\n",
      "Epoch 1246/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2944e-04 - val_loss: 0.7335\n",
      "Epoch 1247/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.9839e-04 - val_loss: 0.3431\n",
      "Epoch 1248/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2481e-04 - val_loss: 0.1341\n",
      "Epoch 1249/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0032 - val_loss: 0.0620\n",
      "Epoch 1250/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.6732e-04 - val_loss: 0.0396\n",
      "Epoch 1251/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5754e-04 - val_loss: 0.0296\n",
      "Epoch 1252/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.6204e-04 - val_loss: 0.0254\n",
      "Epoch 1253/2000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.5028e-04 - val_loss: 0.0226\n",
      "Epoch 1254/2000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7253e-04 - val_loss: 0.0206\n",
      "Epoch 1255/2000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.2493e-04 - val_loss: 0.0197\n",
      "Epoch 1256/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0014 - val_loss: 0.0192\n",
      "Epoch 1257/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.4567e-04 - val_loss: 0.0195\n",
      "Epoch 1258/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.6625e-04 - val_loss: 0.0199\n",
      "Epoch 1259/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.9965e-04 - val_loss: 0.0210\n",
      "Epoch 1260/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.5896e-04 - val_loss: 0.0221\n",
      "Epoch 1261/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.4622e-04 - val_loss: 0.0235\n",
      "Epoch 1262/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0013 - val_loss: 0.0238\n",
      "Epoch 1263/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step - loss: 2.4484e-04 - val_loss: 0.0248\n",
      "Epoch 1264/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.8215e-04 - val_loss: 0.0257\n",
      "Epoch 1265/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.5765e-04 - val_loss: 0.0263\n",
      "Epoch 1266/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.0867e-04 - val_loss: 0.0272\n",
      "Epoch 1267/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.1861e-04 - val_loss: 0.0280\n",
      "Epoch 1268/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.1203e-04 - val_loss: 0.0269\n",
      "Epoch 1269/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0053 - val_loss: 0.0287\n",
      "Epoch 1270/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.7153e-04 - val_loss: 0.0306\n",
      "Epoch 1271/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.1548e-04 - val_loss: 0.0325\n",
      "Epoch 1272/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0032 - val_loss: 0.0367\n",
      "Epoch 1273/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.6616e-04 - val_loss: 0.0408\n",
      "Epoch 1274/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.3318e-04 - val_loss: 0.0461\n",
      "Epoch 1275/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.1052e-04 - val_loss: 0.0522\n",
      "Epoch 1276/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.0911e-04 - val_loss: 0.0552\n",
      "Epoch 1277/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.8862e-04 - val_loss: 0.0579\n",
      "Epoch 1278/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.3323e-04 - val_loss: 0.0621\n",
      "Epoch 1279/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7.6031e-04 - val_loss: 0.0662\n",
      "Epoch 1280/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0059 - val_loss: 0.0652\n",
      "Epoch 1281/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.8514e-04 - val_loss: 0.0656\n",
      "Epoch 1282/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.4662e-04 - val_loss: 0.0647\n",
      "Epoch 1283/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.7300e-04 - val_loss: 0.0623\n",
      "Epoch 1284/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.9390e-04 - val_loss: 0.0596\n",
      "Epoch 1285/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.5718e-04 - val_loss: 0.0566\n",
      "Epoch 1286/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.3924e-04 - val_loss: 0.0554\n",
      "Epoch 1287/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.4091e-04 - val_loss: 0.0539\n",
      "Epoch 1288/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.4227e-04 - val_loss: 0.0528\n",
      "Epoch 1289/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0191 - val_loss: 0.0592\n",
      "Epoch 1290/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.8599e-04 - val_loss: 0.0701\n",
      "Epoch 1291/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.9648e-04 - val_loss: 0.0885\n",
      "Epoch 1292/2000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6.2407e-04 - val_loss: 0.1121\n",
      "Epoch 1293/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.9540e-04 - val_loss: 0.1356\n",
      "Epoch 1294/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.0380e-04 - val_loss: 0.1685\n",
      "Epoch 1295/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.2219e-04 - val_loss: 0.2011\n",
      "Epoch 1296/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.3924e-04 - val_loss: 0.2268\n",
      "Epoch 1297/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.2606e-04 - val_loss: 0.2679\n",
      "Epoch 1298/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.4766e-04 - val_loss: 0.3069\n",
      "Epoch 1299/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.3783e-04 - val_loss: 0.3105\n",
      "Epoch 1300/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.1697e-04 - val_loss: 0.3388\n",
      "Epoch 1301/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.8430e-04 - val_loss: 0.3232\n",
      "Epoch 1302/2000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.9300e-04 - val_loss: 0.3371\n",
      "Epoch 1303/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.9389e-04 - val_loss: 0.3502\n",
      "Epoch 1304/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.1801e-04 - val_loss: 0.3629\n",
      "Epoch 1305/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1054e-04 - val_loss: 0.3529\n",
      "Epoch 1306/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.9408e-04 - val_loss: 0.3600\n",
      "Epoch 1307/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.4058e-04 - val_loss: 0.3680\n",
      "Epoch 1308/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.7678e-04 - val_loss: 0.3839\n",
      "Epoch 1309/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2525e-04 - val_loss: 0.3915\n",
      "Epoch 1310/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0030 - val_loss: 0.3164\n",
      "Epoch 1311/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.5912e-04 - val_loss: 0.2512\n",
      "Epoch 1312/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.0642e-04 - val_loss: 0.2022\n",
      "Epoch 1313/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.9289e-04 - val_loss: 0.1564\n",
      "Epoch 1314/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.9169e-04 - val_loss: 0.1212\n",
      "Epoch 1315/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2019e-04 - val_loss: 0.1009\n",
      "Epoch 1316/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0036 - val_loss: 0.0824\n",
      "Epoch 1317/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.4553e-04 - val_loss: 0.0653\n",
      "Epoch 1318/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0013 - val_loss: 0.0493\n",
      "Epoch 1319/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0027 - val_loss: 0.0424\n",
      "Epoch 1320/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.4122e-04 - val_loss: 0.0324\n",
      "Epoch 1321/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8678e-04 - val_loss: 0.0271\n",
      "Epoch 1322/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.8195e-04 - val_loss: 0.0227\n",
      "Epoch 1323/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.1865e-04 - val_loss: 0.0205\n",
      "Epoch 1324/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7776e-04 - val_loss: 0.0190\n",
      "Epoch 1325/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0365 - val_loss: 0.0221\n",
      "Epoch 1326/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.0534e-04 - val_loss: 0.0325\n",
      "Epoch 1327/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.3318e-04 - val_loss: 0.0507\n",
      "Epoch 1328/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.3534e-04 - val_loss: 0.0816\n",
      "Epoch 1329/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.0289e-04 - val_loss: 0.1272\n",
      "Epoch 1330/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.0687e-04 - val_loss: 0.1850\n",
      "Epoch 1331/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.1377e-04 - val_loss: 0.2693\n",
      "Epoch 1332/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.3645e-04 - val_loss: 0.3623\n",
      "Epoch 1333/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1607e-04 - val_loss: 0.5067\n",
      "Epoch 1334/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.0469e-04 - val_loss: 0.5987\n",
      "Epoch 1335/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.9669e-04 - val_loss: 0.6736\n",
      "Epoch 1336/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.8526e-04 - val_loss: 0.7419\n",
      "Epoch 1337/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.9118e-04 - val_loss: 0.8215\n",
      "Epoch 1338/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0014 - val_loss: 0.8981\n",
      "Epoch 1339/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.9011e-04 - val_loss: 0.9289\n",
      "Epoch 1340/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.6009e-04 - val_loss: 0.9917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1341/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.5739e-04 - val_loss: 1.0260\n",
      "Epoch 1342/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.1041e-04 - val_loss: 1.0711\n",
      "Epoch 1343/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.4119e-04 - val_loss: 1.1198\n",
      "Epoch 1344/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0019 - val_loss: 1.1353\n",
      "Epoch 1345/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.2458e-04 - val_loss: 1.1340\n",
      "Epoch 1346/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.0372e-04 - val_loss: 1.1671\n",
      "Epoch 1347/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.9013e-04 - val_loss: 1.1650\n",
      "Epoch 1348/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.2591e-04 - val_loss: 1.1867\n",
      "Epoch 1349/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.1571e-04 - val_loss: 1.2171\n",
      "Epoch 1350/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.9574e-04 - val_loss: 1.2620\n",
      "Epoch 1351/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.7485e-04 - val_loss: 1.2647\n",
      "Epoch 1352/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.6196e-04 - val_loss: 1.2828\n",
      "Epoch 1353/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.6154e-04 - val_loss: 1.3075\n",
      "Epoch 1354/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.0600e-04 - val_loss: 1.3327\n",
      "Epoch 1355/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.9211e-04 - val_loss: 1.3237\n",
      "Epoch 1356/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.0315e-04 - val_loss: 1.3263\n",
      "Epoch 1357/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.5684e-04 - val_loss: 1.3430\n",
      "Epoch 1358/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.4943e-04 - val_loss: 1.3738\n",
      "Epoch 1359/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.0008e-04 - val_loss: 1.3762\n",
      "Epoch 1360/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.9754e-04 - val_loss: 1.3921\n",
      "Epoch 1361/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.0307e-04 - val_loss: 1.4323\n",
      "Epoch 1362/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.6385e-04 - val_loss: 1.5371\n",
      "Epoch 1363/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.6452e-04 - val_loss: 1.5620\n",
      "Epoch 1364/2000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.3075e-04 - val_loss: 1.5612\n",
      "Epoch 1365/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 4.1754e-04 - val_loss: 1.5522\n",
      "Epoch 1366/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.3015e-04 - val_loss: 1.5361\n",
      "Epoch 1367/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.9431e-04 - val_loss: 1.5267\n",
      "Epoch 1368/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.9724e-04 - val_loss: 1.5470\n",
      "Epoch 1369/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.8592e-04 - val_loss: 1.5810\n",
      "Epoch 1370/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.7659e-04 - val_loss: 1.6326\n",
      "Epoch 1371/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.0089e-04 - val_loss: 1.6563\n",
      "Epoch 1372/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.9765e-04 - val_loss: 1.6689\n",
      "Epoch 1373/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.1821e-04 - val_loss: 1.6756\n",
      "Epoch 1374/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.8197e-04 - val_loss: 1.7168\n",
      "Epoch 1375/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.0467e-04 - val_loss: 1.7174\n",
      "Epoch 1376/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0019 - val_loss: 1.7084\n",
      "Epoch 1377/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.9810e-04 - val_loss: 1.7012\n",
      "Epoch 1378/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1342e-04 - val_loss: 1.6895\n",
      "Epoch 1379/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.2394e-04 - val_loss: 1.6822\n",
      "Epoch 1380/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.7526e-04 - val_loss: 1.6725\n",
      "Epoch 1381/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.1034e-04 - val_loss: 1.6700\n",
      "Epoch 1382/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.9942e-04 - val_loss: 1.6490\n",
      "Epoch 1383/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.5733e-04 - val_loss: 1.6345\n",
      "Epoch 1384/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.6098e-04 - val_loss: 1.6060\n",
      "Epoch 1385/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0015 - val_loss: 1.6276\n",
      "Epoch 1386/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.3967e-04 - val_loss: 1.6748\n",
      "Epoch 1387/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0070 - val_loss: 1.2964\n",
      "Epoch 1388/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.9108e-04 - val_loss: 0.8173\n",
      "Epoch 1389/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.2982e-04 - val_loss: 0.4060\n",
      "Epoch 1390/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0011 - val_loss: 0.1841\n",
      "Epoch 1391/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.5392e-04 - val_loss: 0.0838\n",
      "Epoch 1392/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.8367e-04 - val_loss: 0.0379\n",
      "Epoch 1393/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1654e-04 - val_loss: 0.0198\n",
      "Epoch 1394/2000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 4.1478e-04 - val_loss: 0.0130\n",
      "Epoch 1395/2000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.2738e-04 - val_loss: 0.0091\n",
      "Epoch 1396/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.8451e-04 - val_loss: 0.0069\n",
      "Epoch 1397/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.7909e-04 - val_loss: 0.0056\n",
      "Epoch 1398/2000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.8459e-04 - val_loss: 0.0049\n",
      "Epoch 1399/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.2422e-04 - val_loss: 0.0047\n",
      "Epoch 1400/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.0862e-04 - val_loss: 0.0048\n",
      "Epoch 1401/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0018 - val_loss: 0.0049\n",
      "Epoch 1402/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.7878e-04 - val_loss: 0.0052\n",
      "Epoch 1403/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.7056e-04 - val_loss: 0.0054\n",
      "Epoch 1404/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.9414e-04 - val_loss: 0.0052\n",
      "Epoch 1405/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.6067e-04 - val_loss: 0.0055\n",
      "Epoch 1406/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.4900e-04 - val_loss: 0.0057\n",
      "Epoch 1407/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.6741e-04 - val_loss: 0.0059\n",
      "Epoch 1408/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.0112e-04 - val_loss: 0.0061\n",
      "Epoch 1409/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.6220e-04 - val_loss: 0.0062\n",
      "Epoch 1410/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.9029e-04 - val_loss: 0.0057\n",
      "Epoch 1411/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.8170e-04 - val_loss: 0.0054\n",
      "Epoch 1412/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.6657e-04 - val_loss: 0.0053\n",
      "Epoch 1413/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.8909e-04 - val_loss: 0.0052\n",
      "Epoch 1414/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.8319e-04 - val_loss: 0.0052\n",
      "Epoch 1415/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.3998e-04 - val_loss: 0.0049\n",
      "Epoch 1416/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.9107e-04 - val_loss: 0.0049\n",
      "Epoch 1417/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.8702e-04 - val_loss: 0.0048\n",
      "Epoch 1418/2000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.2252e-04 - val_loss: 0.0046\n",
      "Epoch 1419/2000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.7565e-04 - val_loss: 0.0044\n",
      "Epoch 1420/2000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 5.6412e-04 - val_loss: 0.0043\n",
      "Epoch 1421/2000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 1.8771e-04 - val_loss: 0.0040\n",
      "Epoch 1422/2000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.8804e-04 - val_loss: 0.0039\n",
      "Epoch 1423/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.9616e-04 - val_loss: 0.0038\n",
      "Epoch 1424/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5318e-04 - val_loss: 0.0036\n",
      "Epoch 1425/2000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 1.9066e-04 - val_loss: 0.0034\n",
      "Epoch 1426/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0015 - val_loss: 0.0031\n",
      "Epoch 1427/2000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.3160e-04 - val_loss: 0.0029\n",
      "Epoch 1428/2000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.5972e-04 - val_loss: 0.0028\n",
      "Epoch 1429/2000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 1.6349e-04 - val_loss: 0.0027\n",
      "Epoch 1430/2000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.5258e-04 - val_loss: 0.0025\n",
      "Epoch 1431/2000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 4.0534e-04 - val_loss: 0.0024\n",
      "Epoch 1432/2000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0010 - val_loss: 0.0021\n",
      "Epoch 1433/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.8033e-04 - val_loss: 0.0020\n",
      "Epoch 1434/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.7723e-04 - val_loss: 0.0018\n",
      "Epoch 1435/2000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.0261e-04 - val_loss: 0.0017\n",
      "Epoch 1436/2000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 8.5147e-04 - val_loss: 0.0017\n",
      "Epoch 1437/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.7390e-04 - val_loss: 0.0015\n",
      "Epoch 1438/2000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.0118e-04 - val_loss: 0.0015\n",
      "Epoch 1439/2000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.8133e-04 - val_loss: 0.0014\n",
      "Epoch 1440/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.7296e-04 - val_loss: 0.0013\n",
      "Epoch 1441/2000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.8129e-04 - val_loss: 0.0013\n",
      "Epoch 1442/2000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.8358e-04 - val_loss: 0.0013\n",
      "Epoch 1443/2000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.4964e-04 - val_loss: 0.0012\n",
      "Epoch 1444/2000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.7782e-04 - val_loss: 0.0012\n",
      "Epoch 1445/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.5296e-04 - val_loss: 0.0011\n",
      "Epoch 1446/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.6032e-04 - val_loss: 0.0011\n",
      "Epoch 1447/2000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.5586e-04 - val_loss: 0.0011\n",
      "Epoch 1448/2000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.7007e-04 - val_loss: 0.0011\n",
      "Epoch 1449/2000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.6378e-04 - val_loss: 0.0011\n",
      "Epoch 1450/2000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.7643e-04 - val_loss: 0.0011\n",
      "Epoch 1451/2000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.4644e-04 - val_loss: 0.0011\n",
      "Epoch 1452/2000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.5624e-04 - val_loss: 0.0011\n",
      "Epoch 1453/2000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 1454/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.6379e-04 - val_loss: 0.0011\n",
      "Epoch 1455/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.5990e-04 - val_loss: 0.0011\n",
      "Epoch 1456/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.6349e-04 - val_loss: 0.0011\n",
      "Epoch 1457/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.8514e-04 - val_loss: 0.0011\n",
      "Epoch 1458/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.8430e-04 - val_loss: 0.0011\n",
      "Epoch 1459/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.6734e-04 - val_loss: 0.0012\n",
      "Epoch 1460/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.0387e-04 - val_loss: 0.0012\n",
      "Epoch 1461/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.6210e-04 - val_loss: 0.0012\n",
      "Epoch 1462/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.0211e-04 - val_loss: 0.0012\n",
      "Epoch 1463/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0124 - val_loss: 0.0018\n",
      "Epoch 1464/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.7384e-04 - val_loss: 0.0025\n",
      "Epoch 1465/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0028 - val_loss: 0.0021\n",
      "Epoch 1466/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0010 - val_loss: 0.0018\n",
      "Epoch 1467/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.6525e-04 - val_loss: 0.0017\n",
      "Epoch 1468/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.9691e-04 - val_loss: 0.0017\n",
      "Epoch 1469/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.5789e-04 - val_loss: 0.0017\n",
      "Epoch 1470/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.0436e-04 - val_loss: 0.0017\n",
      "Epoch 1471/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.6264e-04 - val_loss: 0.0018\n",
      "Epoch 1472/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.8980e-04 - val_loss: 0.0019\n",
      "Epoch 1473/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1093e-04 - val_loss: 0.0020\n",
      "Epoch 1474/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.7017e-04 - val_loss: 0.0021\n",
      "Epoch 1475/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.1999e-04 - val_loss: 0.0023\n",
      "Epoch 1476/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.5862e-04 - val_loss: 0.0023\n",
      "Epoch 1477/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.9851e-04 - val_loss: 0.0024\n",
      "Epoch 1478/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.6150e-04 - val_loss: 0.0025\n",
      "Epoch 1479/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1000e-04 - val_loss: 0.0025\n",
      "Epoch 1480/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.6946e-04 - val_loss: 0.0025\n",
      "Epoch 1481/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.9678e-04 - val_loss: 0.0025\n",
      "Epoch 1482/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.5759e-04 - val_loss: 0.0025\n",
      "Epoch 1483/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.6979e-04 - val_loss: 0.0025\n",
      "Epoch 1484/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.3694e-04 - val_loss: 0.0024\n",
      "Epoch 1485/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0014 - val_loss: 0.0024\n",
      "Epoch 1486/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.4917e-04 - val_loss: 0.0024\n",
      "Epoch 1487/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.6626e-04 - val_loss: 0.0023\n",
      "Epoch 1488/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.5306e-04 - val_loss: 0.0023\n",
      "Epoch 1489/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.4959e-04 - val_loss: 0.0023\n",
      "Epoch 1490/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.3018e-04 - val_loss: 0.0022\n",
      "Epoch 1491/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.7163e-04 - val_loss: 0.0022\n",
      "Epoch 1492/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.0666e-04 - val_loss: 0.0021\n",
      "Epoch 1493/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.4229e-04 - val_loss: 0.0020\n",
      "Epoch 1494/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.6799e-04 - val_loss: 0.0020\n",
      "Epoch 1495/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step - loss: 1.6653e-04 - val_loss: 0.0019\n",
      "Epoch 1496/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.5030e-04 - val_loss: 0.0018\n",
      "Epoch 1497/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.6953e-04 - val_loss: 0.0018\n",
      "Epoch 1498/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.1468e-04 - val_loss: 0.0018\n",
      "Epoch 1499/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.4914e-04 - val_loss: 0.0016\n",
      "Epoch 1500/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 1501/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.3134e-04 - val_loss: 0.0013\n",
      "Epoch 1502/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.5553e-04 - val_loss: 0.0012\n",
      "Epoch 1503/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.5052e-04 - val_loss: 0.0011\n",
      "Epoch 1504/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.4239e-04 - val_loss: 0.0010\n",
      "Epoch 1505/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.5700e-04 - val_loss: 9.7965e-04\n",
      "Epoch 1506/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.4456e-04 - val_loss: 9.3742e-04\n",
      "Epoch 1507/2000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.8362e-04 - val_loss: 8.8084e-04\n",
      "Epoch 1508/2000\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 1.5382e-04 - val_loss: 8.5557e-04\n",
      "Epoch 1509/2000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 1.4153e-04 - val_loss: 8.1753e-04\n",
      "Epoch 1510/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8423e-04 - val_loss: 7.8974e-04\n",
      "Epoch 1511/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 5.8377e-04 - val_loss: 7.6260e-04\n",
      "Epoch 1512/2000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 1.8720e-04 - val_loss: 7.5732e-04\n",
      "Epoch 1513/2000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 1.7533e-04 - val_loss: 7.4329e-04\n",
      "Epoch 1514/2000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.3533e-04 - val_loss: 7.2627e-04\n",
      "Epoch 1515/2000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.3282e-04 - val_loss: 7.1929e-04\n",
      "Epoch 1516/2000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 1.4444e-04 - val_loss: 7.1328e-04\n",
      "Epoch 1517/2000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.6381e-04 - val_loss: 6.9908e-04\n",
      "Epoch 1518/2000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0012 - val_loss: 6.6853e-04\n",
      "Epoch 1519/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.0859e-04 - val_loss: 6.5726e-04\n",
      "Epoch 1520/2000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.4841e-04 - val_loss: 6.3856e-04\n",
      "Epoch 1521/2000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.3449e-04 - val_loss: 6.1116e-04\n",
      "Epoch 1522/2000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.3078e-04 - val_loss: 5.9313e-04\n",
      "Epoch 1523/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.0944e-04 - val_loss: 5.7523e-04\n",
      "Epoch 1524/2000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.6712e-04 - val_loss: 5.6388e-04\n",
      "Epoch 1525/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.4090e-04 - val_loss: 5.5570e-04\n",
      "Epoch 1526/2000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 7.4920e-04 - val_loss: 5.4502e-04\n",
      "Epoch 1527/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.6153e-04 - val_loss: 5.4033e-04\n",
      "Epoch 1528/2000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.7560e-04 - val_loss: 5.3438e-04\n",
      "Epoch 1529/2000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.5067e-04 - val_loss: 5.2326e-04\n",
      "Epoch 1530/2000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.6466e-04 - val_loss: 5.2128e-04\n",
      "Epoch 1531/2000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.6240e-04 - val_loss: 5.0987e-04\n",
      "Epoch 1532/2000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9723e-04 - val_loss: 5.0611e-04\n",
      "Epoch 1533/2000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.4277e-04 - val_loss: 5.0060e-04\n",
      "Epoch 1534/2000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.4199e-04 - val_loss: 4.9062e-04\n",
      "Epoch 1535/2000\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 1.5698e-04 - val_loss: 4.8154e-04\n",
      "Epoch 1536/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.0033e-04 - val_loss: 4.8073e-04\n",
      "Epoch 1537/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.2511e-04 - val_loss: 4.8153e-04\n",
      "Epoch 1538/2000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 1.3049e-04 - val_loss: 4.7897e-04\n",
      "Epoch 1539/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.3701e-04 - val_loss: 4.8079e-04\n",
      "Epoch 1540/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0025 - val_loss: 5.7239e-04\n",
      "Epoch 1541/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.2570e-04 - val_loss: 6.7733e-04\n",
      "Epoch 1542/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.6815e-04 - val_loss: 7.7734e-04\n",
      "Epoch 1543/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5455e-04 - val_loss: 8.6142e-04\n",
      "Epoch 1544/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.4934e-04 - val_loss: 9.5998e-04\n",
      "Epoch 1545/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.9385e-04 - val_loss: 0.0011\n",
      "Epoch 1546/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.7446e-04 - val_loss: 0.0011\n",
      "Epoch 1547/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4883e-04 - val_loss: 0.0012\n",
      "Epoch 1548/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.8607e-04 - val_loss: 0.0013\n",
      "Epoch 1549/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.5595e-04 - val_loss: 0.0014\n",
      "Epoch 1550/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.7291e-04 - val_loss: 0.0015\n",
      "Epoch 1551/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 1552/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.3450e-04 - val_loss: 0.0017\n",
      "Epoch 1553/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.8620e-04 - val_loss: 0.0017\n",
      "Epoch 1554/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.6704e-04 - val_loss: 0.0018\n",
      "Epoch 1555/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.4688e-04 - val_loss: 0.0019\n",
      "Epoch 1556/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.3386e-04 - val_loss: 0.0019\n",
      "Epoch 1557/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.0696e-04 - val_loss: 0.0020\n",
      "Epoch 1558/2000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.8524e-04 - val_loss: 0.0020\n",
      "Epoch 1559/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.5007e-04 - val_loss: 0.0021\n",
      "Epoch 1560/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0048 - val_loss: 8.4912e-04\n",
      "Epoch 1561/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.7702e-04 - val_loss: 4.8507e-04\n",
      "Epoch 1562/2000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 6.7275e-04 - val_loss: 3.5747e-04\n",
      "Epoch 1563/2000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.3878e-04 - val_loss: 3.1648e-04\n",
      "Epoch 1564/2000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.6464e-04 - val_loss: 3.0344e-04\n",
      "Epoch 1565/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.6749e-04 - val_loss: 3.0041e-04\n",
      "Epoch 1566/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.5578e-04 - val_loss: 3.1255e-04\n",
      "Epoch 1567/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.5442e-04 - val_loss: 3.4217e-04\n",
      "Epoch 1568/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.5567e-04 - val_loss: 3.9270e-04\n",
      "Epoch 1569/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.3764e-04 - val_loss: 4.6278e-04\n",
      "Epoch 1570/2000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.5695e-04 - val_loss: 5.4718e-04\n",
      "Epoch 1571/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.3687e-04 - val_loss: 6.3937e-04\n",
      "Epoch 1572/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0751 - val_loss: 0.0023\n",
      "Epoch 1573/2000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.4363e-04 - val_loss: 0.0082\n",
      "Epoch 1574/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.6781e-04 - val_loss: 0.0170\n",
      "Epoch 1575/2000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.6815e-04 - val_loss: 0.0442\n",
      "Epoch 1576/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.7603e-04 - val_loss: 0.0919\n",
      "Epoch 1577/2000\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.7322e-04 - val_loss: 0.1471\n",
      "Epoch 1578/2000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.8232e-04 - val_loss: 0.2100\n",
      "Epoch 1579/2000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0047 - val_loss: 0.2756\n",
      "Epoch 1580/2000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.0757e-04 - val_loss: 0.3420\n",
      "Epoch 1581/2000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.9882e-04 - val_loss: 0.3880\n",
      "Epoch 1582/2000\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 9.0586e-04 - val_loss: 0.3920\n",
      "Epoch 1583/2000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.5475e-04 - val_loss: 0.4022\n",
      "Epoch 1584/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.7415e-04 - val_loss: 0.4272\n",
      "Epoch 1585/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.1292e-04 - val_loss: 0.4349\n",
      "Epoch 1586/2000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.7765e-04 - val_loss: 0.4238\n",
      "Epoch 1587/2000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.4769e-04 - val_loss: 0.4239\n",
      "Epoch 1588/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.0881e-04 - val_loss: 0.3954\n",
      "Epoch 1589/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.9656e-04 - val_loss: 0.3886\n",
      "Epoch 1590/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.4080e-04 - val_loss: 0.3789\n",
      "Epoch 1591/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.5146e-04 - val_loss: 0.3658\n",
      "Epoch 1592/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.4189e-04 - val_loss: 0.3463\n",
      "Epoch 1593/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.7581e-04 - val_loss: 0.3147\n",
      "Epoch 1594/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.6510e-04 - val_loss: 0.3064\n",
      "Epoch 1595/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.7018e-04 - val_loss: 0.2916\n",
      "Epoch 1596/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2866e-04 - val_loss: 0.2696\n",
      "Epoch 1597/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.7694e-04 - val_loss: 0.2647\n",
      "Epoch 1598/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.7385e-04 - val_loss: 0.2408\n",
      "Epoch 1599/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.0051e-04 - val_loss: 0.2307\n",
      "Epoch 1600/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.7794e-04 - val_loss: 0.2292\n",
      "Epoch 1601/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2324e-04 - val_loss: 0.2062\n",
      "Epoch 1602/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.6465e-04 - val_loss: 0.1813\n",
      "Epoch 1603/2000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.9220e-04 - val_loss: 0.1731\n",
      "Epoch 1604/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 6.9110e-04 - val_loss: 0.1678\n",
      "Epoch 1605/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.8311e-04 - val_loss: 0.1623\n",
      "Epoch 1606/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.5771e-04 - val_loss: 0.1541\n",
      "Epoch 1607/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.7532e-04 - val_loss: 0.1488\n",
      "Epoch 1608/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.4840e-04 - val_loss: 0.1356\n",
      "Epoch 1609/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0014 - val_loss: 0.1368\n",
      "Epoch 1610/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.8361e-04 - val_loss: 0.1309\n",
      "Epoch 1611/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.7359e-04 - val_loss: 0.1259\n",
      "Epoch 1612/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.4309e-04 - val_loss: 0.1239\n",
      "Epoch 1613/2000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.7846e-04 - val_loss: 0.1227\n",
      "Epoch 1614/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.6145e-04 - val_loss: 0.1202\n",
      "Epoch 1615/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.0083e-04 - val_loss: 0.1187\n",
      "Epoch 1616/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.1757e-04 - val_loss: 0.1139\n",
      "Epoch 1617/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0023 - val_loss: 0.1021\n",
      "Epoch 1618/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.8917e-04 - val_loss: 0.0963\n",
      "Epoch 1619/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.2793e-04 - val_loss: 0.0878\n",
      "Epoch 1620/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.1592e-04 - val_loss: 0.0822\n",
      "Epoch 1621/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.7971e-04 - val_loss: 0.0768\n",
      "Epoch 1622/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.6012e-04 - val_loss: 0.0726\n",
      "Epoch 1623/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.7120e-04 - val_loss: 0.0684\n",
      "Epoch 1624/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.9711e-04 - val_loss: 0.0659\n",
      "Epoch 1625/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.3014e-04 - val_loss: 0.0597\n",
      "Epoch 1626/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.1706e-04 - val_loss: 0.0556\n",
      "Epoch 1627/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.6688e-04 - val_loss: 0.0539\n",
      "Epoch 1628/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.0347e-04 - val_loss: 0.0531\n",
      "Epoch 1629/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.0931e-04 - val_loss: 0.0527\n",
      "Epoch 1630/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4087e-04 - val_loss: 0.0515\n",
      "Epoch 1631/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0861 - val_loss: 0.0488\n",
      "Epoch 1632/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.3808e-04 - val_loss: 0.0706\n",
      "Epoch 1633/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.7540e-04 - val_loss: 0.0945\n",
      "Epoch 1634/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.3043e-04 - val_loss: 0.3319\n",
      "Epoch 1635/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.3761e-04 - val_loss: 0.8610\n",
      "Epoch 1636/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.5132e-04 - val_loss: 1.4822\n",
      "Epoch 1637/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.2800e-04 - val_loss: 2.0414\n",
      "Epoch 1638/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.5033e-04 - val_loss: 2.5370\n",
      "Epoch 1639/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2619e-04 - val_loss: 3.1189\n",
      "Epoch 1640/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0010 - val_loss: 3.5894\n",
      "Epoch 1641/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.3781e-04 - val_loss: 3.8230\n",
      "Epoch 1642/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.3518e-04 - val_loss: 4.0364\n",
      "Epoch 1643/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.9679e-04 - val_loss: 4.1954\n",
      "Epoch 1644/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.5758e-04 - val_loss: 4.3012\n",
      "Epoch 1645/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.7066e-04 - val_loss: 4.4006\n",
      "Epoch 1646/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.5540e-04 - val_loss: 4.4907\n",
      "Epoch 1647/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1649e-04 - val_loss: 4.5912\n",
      "Epoch 1648/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.2136e-04 - val_loss: 4.7398\n",
      "Epoch 1649/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.7022e-04 - val_loss: 4.8652\n",
      "Epoch 1650/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.3271e-04 - val_loss: 4.9747\n",
      "Epoch 1651/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.3924e-04 - val_loss: 5.0667\n",
      "Epoch 1652/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.5325e-04 - val_loss: 5.1494\n",
      "Epoch 1653/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.4235e-04 - val_loss: 5.2153\n",
      "Epoch 1654/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.6282e-04 - val_loss: 5.2726\n",
      "Epoch 1655/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.4845e-04 - val_loss: 5.3640\n",
      "Epoch 1656/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0215 - val_loss: 6.0223\n",
      "Epoch 1657/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.3897e-04 - val_loss: 6.3486\n",
      "Epoch 1658/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.7278e-04 - val_loss: 6.4508\n",
      "Epoch 1659/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.4837e-04 - val_loss: 6.5316\n",
      "Epoch 1660/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.3839e-04 - val_loss: 6.4821\n",
      "Epoch 1661/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.6548e-04 - val_loss: 6.4615\n",
      "Epoch 1662/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.6477e-04 - val_loss: 6.4660\n",
      "Epoch 1663/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.4179e-04 - val_loss: 6.4867\n",
      "Epoch 1664/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.0227e-04 - val_loss: 6.5134\n",
      "Epoch 1665/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.6628e-04 - val_loss: 6.5155\n",
      "Epoch 1666/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1710e-04 - val_loss: 6.5258\n",
      "Epoch 1667/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0014 - val_loss: 6.5213\n",
      "Epoch 1668/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.5303e-04 - val_loss: 6.5205\n",
      "Epoch 1669/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.3751e-04 - val_loss: 6.5274\n",
      "Epoch 1670/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0020 - val_loss: 6.5625\n",
      "Epoch 1671/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.1591e-04 - val_loss: 6.5994\n",
      "Epoch 1672/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4301e-04 - val_loss: 6.6337\n",
      "Epoch 1673/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2140e-04 - val_loss: 6.6730\n",
      "Epoch 1674/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.3258e-04 - val_loss: 6.6806\n",
      "Epoch 1675/2000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.5729e-04 - val_loss: 6.7004\n",
      "Epoch 1676/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.0591e-04 - val_loss: 6.7280\n",
      "Epoch 1677/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.3774e-04 - val_loss: 6.7294\n",
      "Epoch 1678/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.4365e-04 - val_loss: 6.7393\n",
      "Epoch 1679/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.8862e-04 - val_loss: 6.7488\n",
      "Epoch 1680/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4477e-04 - val_loss: 6.7585\n",
      "Epoch 1681/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.8689e-04 - val_loss: 6.7680\n",
      "Epoch 1682/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.4695e-04 - val_loss: 6.7497\n",
      "Epoch 1683/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.4930e-04 - val_loss: 6.7389\n",
      "Epoch 1684/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.8653e-04 - val_loss: 6.7277\n",
      "Epoch 1685/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.3978e-04 - val_loss: 6.7318\n",
      "Epoch 1686/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.8580e-04 - val_loss: 6.7237\n",
      "Epoch 1687/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.2306e-04 - val_loss: 6.7232\n",
      "Epoch 1688/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.7953e-04 - val_loss: 6.7201\n",
      "Epoch 1689/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.3925e-04 - val_loss: 6.7243\n",
      "Epoch 1690/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.6312e-04 - val_loss: 6.7130\n",
      "Epoch 1691/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.0739e-04 - val_loss: 6.7025\n",
      "Epoch 1692/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0246 - val_loss: 6.7014\n",
      "Epoch 1693/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.6267e-04 - val_loss: 6.6515\n",
      "Epoch 1694/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.7187e-04 - val_loss: 6.5878\n",
      "Epoch 1695/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.1997e-04 - val_loss: 6.5291\n",
      "Epoch 1696/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.4474e-04 - val_loss: 6.4519\n",
      "Epoch 1697/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.6438e-04 - val_loss: 6.3727\n",
      "Epoch 1698/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0027 - val_loss: 6.2640\n",
      "Epoch 1699/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.9858e-04 - val_loss: 6.0420\n",
      "Epoch 1700/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4767e-04 - val_loss: 5.6832\n",
      "Epoch 1701/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.5927e-04 - val_loss: 5.3683\n",
      "Epoch 1702/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.3921e-04 - val_loss: 5.1720\n",
      "Epoch 1703/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.3246e-04 - val_loss: 5.0377\n",
      "Epoch 1704/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.4080e-04 - val_loss: 4.9545\n",
      "Epoch 1705/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.3522e-04 - val_loss: 4.8884\n",
      "Epoch 1706/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.8072e-04 - val_loss: 4.8325\n",
      "Epoch 1707/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.4737e-04 - val_loss: 4.7723\n",
      "Epoch 1708/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.2673e-04 - val_loss: 4.7292\n",
      "Epoch 1709/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.9176e-04 - val_loss: 4.6908\n",
      "Epoch 1710/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2634e-04 - val_loss: 4.6596\n",
      "Epoch 1711/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0207 - val_loss: 4.6300\n",
      "Epoch 1712/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.4379e-04 - val_loss: 4.8026\n",
      "Epoch 1713/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2250e-04 - val_loss: 4.9415\n",
      "Epoch 1714/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.6170e-04 - val_loss: 5.0494\n",
      "Epoch 1715/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1313e-04 - val_loss: 4.9539\n",
      "Epoch 1716/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.0201e-04 - val_loss: 4.7143\n",
      "Epoch 1717/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4449e-04 - val_loss: 4.3825\n",
      "Epoch 1718/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.3907e-04 - val_loss: 4.1062\n",
      "Epoch 1719/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.0729e-04 - val_loss: 3.9028\n",
      "Epoch 1720/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0267 - val_loss: 3.6754\n",
      "Epoch 1721/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.6357e-04 - val_loss: 3.4542\n",
      "Epoch 1722/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.9847e-04 - val_loss: 3.2572\n",
      "Epoch 1723/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1683e-04 - val_loss: 3.0932\n",
      "Epoch 1724/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.9074e-04 - val_loss: 2.9427\n",
      "Epoch 1725/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.1411e-04 - val_loss: 2.8955\n",
      "Epoch 1726/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.2781e-04 - val_loss: 2.8700\n",
      "Epoch 1727/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.2353e-04 - val_loss: 2.8529\n",
      "Epoch 1728/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0078 - val_loss: 2.8401\n",
      "Epoch 1729/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.2382e-04 - val_loss: 2.8490\n",
      "Epoch 1730/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.8047e-04 - val_loss: 2.8697\n",
      "Epoch 1731/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.1008e-04 - val_loss: 2.8899\n",
      "Epoch 1732/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.2836e-04 - val_loss: 2.9059\n",
      "Epoch 1733/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.2449e-04 - val_loss: 2.9419\n",
      "Epoch 1734/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.2878e-04 - val_loss: 2.9858\n",
      "Epoch 1735/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.5185e-04 - val_loss: 3.0178\n",
      "Epoch 1736/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.4885e-04 - val_loss: 3.0589\n",
      "Epoch 1737/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.1316e-04 - val_loss: 3.1023\n",
      "Epoch 1738/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.2976e-04 - val_loss: 3.1475\n",
      "Epoch 1739/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.1579e-04 - val_loss: 3.1795\n",
      "Epoch 1740/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.1414e-04 - val_loss: 3.2075\n",
      "Epoch 1741/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.9420e-04 - val_loss: 3.2487\n",
      "Epoch 1742/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.6841e-04 - val_loss: 3.2778\n",
      "Epoch 1743/2000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.4408e-04 - val_loss: 3.3129\n",
      "Epoch 1744/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.1683e-04 - val_loss: 3.3514\n",
      "Epoch 1745/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.2569e-04 - val_loss: 3.3625\n",
      "Epoch 1746/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4560e-04 - val_loss: 3.3792\n",
      "Epoch 1747/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.7433e-04 - val_loss: 3.3966\n",
      "Epoch 1748/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.6639e-04 - val_loss: 3.4125\n",
      "Epoch 1749/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.5297e-04 - val_loss: 3.4325\n",
      "Epoch 1750/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.4241e-04 - val_loss: 3.4479\n",
      "Epoch 1751/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.2038e-04 - val_loss: 3.4657\n",
      "Epoch 1752/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.1614e-04 - val_loss: 3.4745\n",
      "Epoch 1753/2000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.2101e-04 - val_loss: 3.5096\n",
      "Epoch 1754/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.1414e-04 - val_loss: 3.5232\n",
      "Epoch 1755/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.9705e-04 - val_loss: 3.5255\n",
      "Epoch 1756/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 9.9837e-05 - val_loss: 3.5282\n",
      "Epoch 1757/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.0944e-04 - val_loss: 3.5365\n",
      "Epoch 1758/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0375e-04 - val_loss: 3.5529\n",
      "Epoch 1759/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.4156e-04 - val_loss: 3.5669\n",
      "Epoch 1760/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0989e-04 - val_loss: 3.5701\n",
      "Epoch 1761/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1030e-04 - val_loss: 3.5787\n",
      "Epoch 1762/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.1429e-04 - val_loss: 3.5862\n",
      "Epoch 1763/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.4413e-04 - val_loss: 3.5852\n",
      "Epoch 1764/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0270e-04 - val_loss: 3.5964\n",
      "Epoch 1765/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.2735e-04 - val_loss: 3.5937\n",
      "Epoch 1766/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.3158e-04 - val_loss: 3.6033\n",
      "Epoch 1767/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.7557e-04 - val_loss: 3.6106\n",
      "Epoch 1768/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.1641e-04 - val_loss: 3.6152\n",
      "Epoch 1769/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.6603e-04 - val_loss: 3.6027\n",
      "Epoch 1770/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.8021e-04 - val_loss: 3.6129\n",
      "Epoch 1771/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.5098e-04 - val_loss: 3.6132\n",
      "Epoch 1772/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.4612e-04 - val_loss: 3.6084\n",
      "Epoch 1773/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.9580e-04 - val_loss: 3.6016\n",
      "Epoch 1774/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.2033e-04 - val_loss: 3.6040\n",
      "Epoch 1775/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2325e-04 - val_loss: 3.6007\n",
      "Epoch 1776/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.1018e-04 - val_loss: 3.5910\n",
      "Epoch 1777/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2514e-04 - val_loss: 3.5922\n",
      "Epoch 1778/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2525e-04 - val_loss: 3.5947\n",
      "Epoch 1779/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0777e-04 - val_loss: 3.5937\n",
      "Epoch 1780/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0030 - val_loss: 3.6150\n",
      "Epoch 1781/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.1783e-04 - val_loss: 3.6394\n",
      "Epoch 1782/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 9.4921e-05 - val_loss: 3.6602\n",
      "Epoch 1783/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1855e-04 - val_loss: 3.6720\n",
      "Epoch 1784/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.3738e-04 - val_loss: 3.6782\n",
      "Epoch 1785/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0375e-04 - val_loss: 3.6868\n",
      "Epoch 1786/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0265e-04 - val_loss: 3.6963\n",
      "Epoch 1787/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0935e-04 - val_loss: 3.7029\n",
      "Epoch 1788/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.4523e-04 - val_loss: 3.7081\n",
      "Epoch 1789/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.3567e-05 - val_loss: 3.7137\n",
      "Epoch 1790/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.8729e-05 - val_loss: 3.7145\n",
      "Epoch 1791/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.1052e-04 - val_loss: 3.7153\n",
      "Epoch 1792/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1752e-04 - val_loss: 3.7204\n",
      "Epoch 1793/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.1807e-04 - val_loss: 3.7236\n",
      "Epoch 1794/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0298e-04 - val_loss: 3.7354\n",
      "Epoch 1795/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2015e-04 - val_loss: 3.7315\n",
      "Epoch 1796/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0852e-04 - val_loss: 3.7282\n",
      "Epoch 1797/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.1348e-04 - val_loss: 3.7275\n",
      "Epoch 1798/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.1672e-04 - val_loss: 3.7213\n",
      "Epoch 1799/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0992 - val_loss: 2.9188\n",
      "Epoch 1800/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.2817e-04 - val_loss: 2.6101\n",
      "Epoch 1801/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step - loss: 9.3327e-05 - val_loss: 2.4763\n",
      "Epoch 1802/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0508e-04 - val_loss: 2.1271\n",
      "Epoch 1803/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.8272e-04 - val_loss: 1.3251\n",
      "Epoch 1804/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0097 - val_loss: 0.6795\n",
      "Epoch 1805/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.4713e-04 - val_loss: 0.3162\n",
      "Epoch 1806/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0035 - val_loss: 0.2603\n",
      "Epoch 1807/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1649e-04 - val_loss: 0.2304\n",
      "Epoch 1808/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.3345e-04 - val_loss: 0.2029\n",
      "Epoch 1809/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.9940e-04 - val_loss: 0.2062\n",
      "Epoch 1810/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2237e-04 - val_loss: 0.2331\n",
      "Epoch 1811/2000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.6889e-04 - val_loss: 0.2629\n",
      "Epoch 1812/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.4991e-04 - val_loss: 0.2969\n",
      "Epoch 1813/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0022 - val_loss: 0.3198\n",
      "Epoch 1814/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0164 - val_loss: 0.3241\n",
      "Epoch 1815/2000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.0185e-04 - val_loss: 0.3330\n",
      "Epoch 1816/2000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.3585e-04 - val_loss: 0.3318\n",
      "Epoch 1817/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.2941e-04 - val_loss: 0.3426\n",
      "Epoch 1818/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.9307e-04 - val_loss: 0.3390\n",
      "Epoch 1819/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.5317e-04 - val_loss: 0.3435\n",
      "Epoch 1820/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.3156e-04 - val_loss: 0.3484\n",
      "Epoch 1821/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.5706e-04 - val_loss: 0.3590\n",
      "Epoch 1822/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0939e-04 - val_loss: 0.3689\n",
      "Epoch 1823/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.3926e-04 - val_loss: 0.3687\n",
      "Epoch 1824/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.8988e-04 - val_loss: 0.3747\n",
      "Epoch 1825/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0018 - val_loss: 0.3803\n",
      "Epoch 1826/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.7407e-04 - val_loss: 0.3851\n",
      "Epoch 1827/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.4056e-04 - val_loss: 0.3877\n",
      "Epoch 1828/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.4361e-04 - val_loss: 0.3870\n",
      "Epoch 1829/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.5281e-04 - val_loss: 0.3962\n",
      "Epoch 1830/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.6322e-04 - val_loss: 0.3855\n",
      "Epoch 1831/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.8965e-04 - val_loss: 0.3866\n",
      "Epoch 1832/2000\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2027e-04"
     ]
    }
   ],
   "source": [
    "model = hyperopti_on_user(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab9012ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9db353c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 8088 (pid 6792), started 0:01:05 ago. (Use '!kill 6792' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-72febe4e1a8a7f69\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-72febe4e1a8a7f69\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8088;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir tuner_runs/tb_logs --max_reload_threads=20 --reload_multifile=true --host localhost --port 8088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0fa91880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the users' data into global variables so that it is not repeated in subsequent runs\n",
    "user_datas = []\n",
    "user_datas.append(None) # so that the index in the list matches the user id from 1 to 40\n",
    "for id_ in range(1,41):\n",
    "    (X_train,y_train), (X_valid,y_valid), (X_test,y_test) = prepare_data_of_user(id_)\n",
    "    splits = dict()\n",
    "    splits[\"X_train\"] = X_train\n",
    "    splits[\"y_train\"] = y_train\n",
    "    splits[\"X_valid\"] = X_valid\n",
    "    splits[\"y_valid\"] = y_valid\n",
    "    splits[\"X_test\"] = X_test\n",
    "    splits[\"y_test\"] = y_test\n",
    "    user_datas.append(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "269bce38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training On user: 1 with learning rate 5e-06\n",
      "Epoch 1/10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1972/3040330821.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0muser_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_datas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"X_train\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muser_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"y_train\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"X_valid\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muser_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"y_valid\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_and_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_valid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muser_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"X_test\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muser_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"y_test\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1972/2758217149.py\u001b[0m in \u001b[0;36mcreate_and_train_model\u001b[1;34m(user_id, X_train, y_train, X_valid, y_valid, learning_rate)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mearly_stopping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     history = model.fit(X_train,y_train, validation_data=(X_valid,y_valid),epochs = 10000,batch_size=8,verbose = 1,\n\u001b[1;32m----> 9\u001b[1;33m                        callbacks=[early_stopping])\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marci\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marci\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1307\u001b[0m                 _r=1):\n\u001b[0;32m   1308\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1310\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marci\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marci\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marci\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    973\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 975\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    976\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marci\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2957\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2959\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marci\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1854\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marci\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    505\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\marci\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 55\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1332x756 with 42 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "subdir = \"learning_rate_tuning_final_10k_epochs\"\n",
    "Path(subdir).mkdir(parents=True, exist_ok=True)\n",
    "#test the learning rate after hyperparameter optimization\n",
    "for lr in (0.000005,):\n",
    "    dir_to_save = os.path.join(subdir,\"lr_\" + str(lr))\n",
    "    Path(dir_to_save).mkdir(parents=True, exist_ok=True)\n",
    "    fig, axs = plt.subplots(6,7) #6rows,7cols for 40 diagrams\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    errs = []\n",
    "    \n",
    "    n_p = 0\n",
    "    n_n = 0\n",
    "    n_fp = 0\n",
    "    n_fn = 0\n",
    "    n_errs = 0\n",
    "    \n",
    "    for id_ in range(1,41):\n",
    "        print(\"Training On user:\",id_,\"with learning rate\",lr)\n",
    "        user_data = user_datas[id_]\n",
    "        X_train,y_train,X_valid,y_valid = user_data[\"X_train\"],user_data[\"y_train\"], user_data[\"X_valid\"],user_data[\"y_valid\"]\n",
    "        model, history = create_and_train_model(id_, X_train,y_train,X_valid,y_valid,learning_rate = lr)\n",
    "                                                                         \n",
    "        (fp,fn,p,n) = evaluate_model(id_,model,user_data[\"X_test\"],user_data[\"y_test\"],history,axs)\n",
    "        n_errs+=fp+fn\n",
    "        errs.append(fp+fn)\n",
    "        \n",
    "        n_p+=p\n",
    "        n_n+=n\n",
    "        n_fp+=fp\n",
    "        n_fn+=fn\n",
    "        \n",
    "    print(\"false positive rate:\",n_fp/(n_fp+n_n))\n",
    "    print(\"false negative rate:\",n_fn/(n_fn+n_p))\n",
    "    print(\"accuracy:\",1-n_errs/(40*6))\n",
    "    print(\"F-score\", (n_p) / (n_p + 0.5*(n_fp + n_fn)))\n",
    "    print(n_p,n_n,n_fp,n_fn)\n",
    "    plt.savefig(os.path.join(dir_to_save,\"loss_\" + str(lr) + \".png\"))\n",
    "    plt.show()\n",
    "    fig = plt.hist(errs,bins=6)\n",
    "    plt.savefig(os.path.join(dir_to_save,\"errs_\" + str(lr) + \".png\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a809866f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1972/2931468266.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#run it after the previous cell\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#printing statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"false positive rate:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_n\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_fp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mn_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"false negative rate:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_p\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_fn\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mn_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"accuracy:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mn_errs\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "#run it after the previous cell\n",
    "#printing statistics\n",
    "print(\"false positive rate:\",n_n/(n_fp+n_n))\n",
    "print(\"false negative rate:\",n_p/(n_fn+n_p))\n",
    "print(\"accuracy:\",1-n_errs/(40*6))\n",
    "print(\"F-score\", (n_p) / (n_p + 0.5*(n_fp + n_fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bd613c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAygUlEQVR4nO3deXgV5fn/8fcnCWENhCXsCQFkkX2JuLHZuiEu1aLVumtF/Wm/Vr/V1tqqrV/bqm2trbbuW1v3lSqIS1UUsQoaVgGRfQchEES25P79MRM9xhMIyTmZ5OR+Xde5MmfmmZl7Ejj3meeZuUdmhnPOOVdeWtQBOOecq508QTjnnIvLE4Rzzrm4PEE455yLyxOEc865uDxBOOeci8sThKsWSVdL+oekhP9bknSepHfjzG8jqVBSQQL2sVTSkdXdTm0gKV+SScqo4vom6YBEx+XqLk8QrkKStsW8SiV9GfP+TEljgKHAeWZWWkMxNQAeAf6fmU2viX1WhaSHJf1fkveRMsnN1U5V+qbh6gcza1Y2LWkp8CMze71cs0k1HNNuYGxN7tO5+srPINx+k5Qm6eeSPpP0uaSnJLUKl02SdHm59jMlnRJO95b0mqRNkhZIOi2mXWtJEyRtlfQB0L3cdva27nGS5kkqlrRK0k9jlh0fdkkVSXpP0oAKjutASUsknVHB8jskrQjjmyFpRAXtxgNnAteEZ1v/Dud3lPSspA3hfv4nZp1hkqaH214n6U/h/DvLncntkXSjpH8AecC/w/nXxIRwpqTlkjZKuq7cPqaFv4c14bYzKziGhyX9Lfx7bpM0VVJ7SX+WtFnSfEmDY9qX/XsoDv8OJ8csm1nuGEzS6HDZiZLmhjG9JenAmPWWSvqppFmStkh6UlKjePG6JDEzf/lrny9gKXBkOH0F8D7QGWgI3AM8Hi47B5gas14foChs1xRYAZxPcPY6GNgI9AnbPgE8FbbrB6wC3g2X7WvdNcCIcLolMCScHgysBw4G0oFzw2NpGHtcwBBgOXD8Xn4HZwGtw/3/L7AWaFRB24eB/4t5nwbMAK4HMoFuwGLgmHD5NODscLoZcEicbQ4CNgCDy/9Nwvf5gAH3AY2BgcBO4MBw+VDgkDD+fOAT4Ccx6xtwQEz8G8N1GgH/AZaEf9904P+AN2PWPRXoGB7nD4AvgA5xjmE8MB9oDvQM2x0FNACuARYBmTHH90G43VZhvJdE/X+hPr0iD8BfdePFNxPEJ8B3Y5Z1AHaHHzxZ4X/6LuGym4EHw+kfAO+U2+49wA3hh85uoHfMst/ydYKocN1wejlwMdC8XJu/AzeVm7cAGBVzXL8GVgKj9/N3shkYWMGyh/lmgjgYWF6uzbXAQ+H0lDCONhVsLyeM9fR4f5PwfX74Id85Zt4HseuU2+ZPgOdj3pdPEPfFLPsx8EnM+/5A0V5+N4XASeXmDSdI1j3D978CnopZnkbwpWB0zPGdFbP8VuDuqP8v1KeXdzG5qugCPB92CxQRJIwSoJ2ZFQMvA6eHbc8A/hWz3sFl64Xrngm0J/gAzCA4SyizrNw+K1oX4PvAccAySW9LOjRmvf8tt14uwbfSMpcA75nZW3s76LC745Owu6MIaAG02ds65eLvWC6OXwDtwuUXEnyjni/pQ0nHx+y3AfAM8JiZPVGJfa2Nmd5OcEaCpJ6SXpK0VtJWggS8t/jXxUx/Ged97BjVOTHdeEUEZ4BtYpbnEpwdnmtmC8PZHYn5G1twocMKoNO+jsXVDE8QripWAGPMLDvm1cjMVoXLHwfOCD+kGwFvxqz3drn1mpnZpQRdJ3sIPrzL5JXbZ0XrYmYfmtlJQFvgBYIPo7L1bi63XhMzezxm25cAeZJur+iAw/GGa4DTgJZmlg1sAVTBKuXLJK8AlpSLI8vMjgvj/9TMzgjjvwV4RlLTcN2/AluBX+5jH/vyd4LunR5m1pwgQVUUf6VJ6kLQrXU50Dr83cwp27akxgR/kz+bWexFDasJEmfZdkTw91+FqxU8QbiquBu4OfxgQFKOpJNilk8k+I//G+BJ+/oS2JeAnpLOltQgfB0k6UAzKwGeA26U1ERSH4LxAva1rqRMBZfdtrDgKqetQNk+7wMukXSwAk0ljZWUFbPtYuBYYKSk31dwzFkECWwDkCHpeoJ+9IqsIxhnKPMBUCzpZ5IaS0qX1E/SQeHv8CxJOeHvqihcp1TSxcAo4Ez79qXE5fexL1kEv5ttknoDl+7HunvTlCBZbQCQdD7BGUSZB4H5ZnZrufWeAsZK+m54lvS/BGMm7yUoLldNniBcVdwBTABelVRMMGB9cNlCM9tJ8GF/JPBYzPxi4GiC7qfVBN0HtxAMYEPwDbRZOP9h4KH9WPdsYGnYdXIJQfcTFtwrcRFwJ8GYwSLgvPIHZGZFBIOlYyTdFOeYJwOvAAsJukV28M3usPIeAPqEXS4vhAnweIKB5iUEA8D3E3RTQZCg5kraRvD7Pd3MviToousGrI65CugX4Tq/A34Z7uOrq7b24qfADwkS4n3Ak5VYZ5/MbB7wR4KB9nUE4xNTY5qcDpxc7kqmEWa2gGDg/68Ev48TgBPMbFci4nLVJzN/YJBzzrlv8zMI55xzcXmCcM45F5cnCOecc3F5gnDOORdXShXra9OmjeXn50cdhnPO1RkzZszYaGY58ZalVILIz89n+vRaWwHaOedqHUnLKlrmXUzOOefi8gThnHMuLk8Qzjnn4vIE4ZxzLi5PEM455+LyBOGccy4uTxDOOefiqvcJYsfuEu6bspj3F38edSjOOVerpNSNclUhwf3vLuaAts04pFvrqMNxzrlao96fQTTMSOeCw7syddHnzF65JepwnHOu1qj3CQLgjIPzyGqYwd1TPos6FOecqzWSliAkPShpvaQ5MfOelFQYvpZKKqxg3aWSZoftkl5cqXmjBpx5SBcmzV7Dss+/SPbunHOuTkjmGcTDBM/Z/YqZ/cDMBpnZIOBZgucWV+SIsG1B8kL82vmH55ORlsb97yypid0551ytl7QEYWZTgE3xlkkScBrweLL2v7/aNW/EyYM78dT0FWzctjPqcJxzLnJRjUGMANaZ2acVLDfgVUkzJI3f24YkjZc0XdL0DRs2VCuoi0Z2Y1dJKY++t7Ra23HOuVQQVYI4g72fPQw3syHAGOAySSMramhm95pZgZkV5OTEfeZFpR3QthlHHdiOR6Yt44ude6q1Leecq+tqPEFIygBOAZ6sqI2ZrQp/rgeeB4bVTHRw8ajubPlyN09NX1FTu3TOuVopijOII4H5ZrYy3kJJTSVllU0DRwNz4rVNhqFdWnJQfkvuf2cJu0tKa2q3zjlX6yTzMtfHgWlAL0krJV0YLjqdct1LkjpKmhi+bQe8K2km8AHwspm9kqw447l4ZHdWFX3Jy7PW1ORunXOuVpGZRR1DwhQUFFginkldWmoc8+cppKeJSVeMILjoyjnnUo+kGRXdTuB3UseRlibGj+zG/LXFTPl0Y9ThOOdcJDxBVOCkQZ1o17whd7/l5Tecc/WTJ4gKZGakceHwrkxb/DkzVxRFHY5zztU4TxB7ccawPLIaZXDvlMVRh+KcczXOE8ReZDVqwFmHdGHSnDUs3ehF/Jxz9YsniH04/7CgiN997/hZhHOufvEEsQ9tmzfilCGdeHrGSjYUexE/51z94QmiEi4a2Y3dJaU8Om1p1KE451yN8QRRCd1zmnF0n3Y86kX8nHP1iCeISior4vfEh17EzzlXP3iCqKQheS0Zlt+KB95Z7EX8nHP1gieI/XDJ6G6s3rKDl2atjjoU55xLOk8Q+2F0z7b0bNeMe95eTCoVOXTOuXg8QeyHoIhfd+avLeathdV7vKlzztV2niD204kDO9KhRSPueduL+DnnUpsniP1UVsTv/cWbKPQifs65FOYJogpO/6qIn59FOOdSlyeIKmjWMIOzD+nCpDlrWeJF/JxzKSqZz6R+UNJ6SXNi5t0oaZWkwvB1XAXrHitpgaRFkn6erBir47zD82mQ7kX8nHOpK5lnEA8Dx8aZf7uZDQpfE8svlJQO3AWMAfoAZ0jqk8Q4q6RtViO+P6Qzz3gRP+dcikpagjCzKcCmKqw6DFhkZovNbBfwBHBSQoNLkItGdGV3SSmPvLc06lCccy7hohiDuFzSrLALqmWc5Z2A2IJHK8N5cUkaL2m6pOkbNtTsvQndcppxTJ/2PDptKdu8iJ9zLsXUdIL4O9AdGASsAf5Y3Q2a2b1mVmBmBTk5OdXd3H67eFQ3tu7YwxMfLK/xfTvnXDLVaIIws3VmVmJmpcB9BN1J5a0CcmPedw7n1UqD81pycNdWPPDuEi/i55xLKTWaICR1iHl7MjAnTrMPgR6SukrKBE4HJtREfFV1yajurNmyg3/P9CJ+zrnUkczLXB8HpgG9JK2UdCFwq6TZkmYBRwBXhm07SpoIYGZ7gMuBycAnwFNmNjdZcSbC6F459GqX5UX8nHMpJSNZGzazM+LMfqCCtquB42LeTwS+dQlsbSWJi0d146qnZvLWgg0c0btt1CE551y1+Z3UCXLCwI50bNGIu72In3MuRXiCSJAG6WlcMLwr/12yiY+Xb446HOecqzZPEAl0+rA8mjfK4J63vfyGc67u8wSRQM0aZnDOoflMnreWxRu2RR2Oc85ViyeIBDv3sLIifkuiDsU556rFE0SC5WQ1ZNzQzjz70UrWF++IOhznnKsyTxBJcNGIbuwuKeXhqUujDsU556rME0QSdG3TlDH92vOP95d5ET/nXJ3lCSJJLh7ZnWIv4uecq8M8QSTJwNxsDukWFPHbtceL+Dnn6h5PEEl0cVjEb4IX8XPO1UGeIJJodM8cerfP4t4pn1Fa6kX8nHN1iyeIJCor4rdw3TbeWrg+6nCcc26/eIJIsuMHlBXx8/Ibzrm6xRNEkjVIT+PCEd34YMkmPvIifs65OsQTRA04/aBcWjRuwD1eCtw5V4d4gqgBTRtmcM6hXXh13jo+8yJ+zrk6whNEDTn3sHwy09O4/x0fi3DO1Q3JfCb1g5LWS5oTM+82SfMlzZL0vKTsCtZdGj67ulDS9GTFWJPaNGvIqQWdeXbGKtZv9SJ+zrnaL5lnEA8Dx5ab9xrQz8wGAAuBa/ey/hFmNsjMCpIUX4370fBu7Ckt5aH3lkYdinPO7VPSEoSZTQE2lZv3qpmVVa97H+icrP3XRvltmjKmXwf++f4yinfsjjoc55zbqyjHIC4AJlWwzIBXJc2QNL4GY0q6i0d1C4v4rYg6FOec26tIEoSk64A9wL8qaDLczIYAY4DLJI3cy7bGS5ouafqGDRuSEG1iDeiczWHdW3sRP+dcrVfjCULSecDxwJlmFrdAkZmtCn+uB54HhlW0PTO718wKzKwgJycnCREn3sWjurN26w5eLFwVdSjOOVehGk0Qko4FrgFONLPtFbRpKimrbBo4GpgTr21dNbJHm7CI32Iv4uecq7WSeZnr48A0oJeklZIuBO4EsoDXwktY7w7bdpQ0MVy1HfCupJnAB8DLZvZKsuKMgiQuGdWdT9dv480FXsTPOVc7qYJenjqpoKDApk+vG7dN7C4pZfRtb9ExuxFPX3JY1OE45+opSTMqup1gn2cQkg6R9KGkbZJ2SSqRtDXxYdYvDdLT+NGIrny4dDMzlm3a9wrOOVfDKtPFdCdwBvAp0Bj4EXBXMoOqL35wUC7ZTRpwj5cCd87VQpUagzCzRUC6mZWY2UN8+w5pVwVNMjM455AuvPbJOhat9yJ+zrnapTIJYrukTKBQ0q2Srqzkeq4Syor43TfFzyKcc7VLZT7ozwbSgcuBL4Bc4PvJDKo+ad2sIacV5PL8x6tY50X8nHO1yD4ThJktM7MvzWyrmf3azK4Ku5xcgvxoRNegiN/UpVGH4pxzX6nMVUzHS/pY0iZJWyUV+1VMidWldVPG9O/Av7yIn3OuFqlMF9OfgXOB1mbW3MyyzKx5csOqfy4Z2Z3inXt47L/Low7FOeeAyiWIFcCciuomucTo37kFhx/QmgenLmHnnpKow3HOuUoliGuAiZKulXRV2SvZgdVHF4/szrqtO3mxcHXUoTjnXKUSxM3AdqARQR2lspdLsBE92tCnQ3Mv4uecqxUyKtGmo5n1S3okDklcPKobVzxRyBvz13NUn3ZRh+Scq8cqcwYxUdLRSY/EATC2fwc6ZTfmnrc/izoU51w9V5kEcSnwiqQv/TLX5MtIT+OiEV2Zvmwz05d6ET/nXHQqc6NclpmlmVljv8y1Zpx2UC4tmzTgHi+/4ZyLUKVqKklqKWmYpJFlr2QHVp81yczgnEPzeW3eOhatL446HOdcPRU3QUjqKyktnL4IeBd4GfgNMBm4saYCrK/OObQLjRqkca+fRTjnIlLRGUQX4AVJbYErgAJgrpmNBgYDRTUSXT3mRfycc1GLmyDMbCLwY4Jk8KWZfQlkSEo3s/lArxqMsd760fBulJQaD05dEnUozrl6qMIxiLCK62RgpaRsYBLwhqQXCcpv7JOkByWtlzQnZl4rSa9J+jT82bKCdc8N23wq6dz9OqoUkde6CWMHdOSx95ez1Yv4OedqWGWuYjrZzIrM7GbgeuBB4MRKbv9hvv30uZ8Db5hZD+CN8P03SGoF3AAcDAwDbqgokaS6i0d28yJ+zrlIVKbc9wOSBgGY2RQzexH4RWU2bmZTgPIX858EPBJOPwJ8L86qxwCvmdkmM9sMvEY9fcxpv04tGH5AGx5814v4OedqVmUucz0GeKRcN09lzyDiaWdma8LptUC8ehKd+GY31spw3rdIGi9puqTpGzZsqEZYtdfFo7qxvngnL37sRfycczWnMgliPTASGCfpLkkZgBKx87CEeLWq0pnZvWZWYGYFOTk5iQir1hl+QBv6dmzO3VM+8yJ+zrkaU5kEITPbYmYnABuAt4AW1djnOkkdAMKf6+O0WUXw7OsyncN59VJQxK87izd8weufrIs6HOdcPVGZBDGhbMLMbgRuAZZWY58TCJ5QR/jzxThtJgNHh3dwtwSODufVW8f1a09uq8ZefsM5V2MqkyBujbmruidB99Ixldm4pMeBaUAvSSslXQj8HjhK0qfAkeF7JBVIuh/AzDYBNwEfhq/fhPPqraCIXzdmeBE/51wN0b6eJCppBjACaAlMJfjA3mlmZyU/vP1TUFBg06dPjzqMpPlyVwmH/f4NhnZpyf3nHhR1OM65FCBphpkVxFtW2TGI7cApwN/M7FSgfyIDdJXTODOdcw/L5/VP1vPpOi/i55xLrkolCEmHAmcSFOyr7HouCc45NN+L+DnnakRlPuivAK4FnjezuZK6AW8mNyxXkVZNMzn9oDxeKFzF2i1exM85lzyVKbUxxcxONLNbwveLzex/kh+aq8iFw7tSangRP+dcUnlXUR2U26oJY/t34LH/LmfLl6lVxG/91h1MnruWW16Zz+8nzfcbA52LUEbUAbiqGT+yGxNmruax/y7n0tHdow6nSnbsLmHOqi0Uriji4xVFFC4vYlXRlwCkp4mSUqN984acd3jXiCN1rn7aZ4KQ1Kr8PQiSupqZ929EqF+nFozo0YYHpy7hguH5NMxIjzqkvTIzlmz8gsIVRUFCWF7EJ2u2sic8Q+iU3ZjBedmcf3g+g/Oy6duxBZf8cwa3vLKA7x7YjtxWTSI+Aufqn8rcBzEVGGNmW8P3fYCnzKxfDcS3X1L9Pojypi7ayJn3/5ffn9Kf04flRR3ONxRt3/WNZDBzZRFF24PusKaZ6QzonM3gvGwG5WYzKC+btlmNvrWN1UVfcvTtUxjQuQX/+tHBSAkpAeaci7G3+yAq08X0W+DfksYSPEnuUYJLXl3EDuvemn6dmnPvlMWcVpBLWlo0H6C7S0qZv6aYwhWb+Xh5kBQWb/wCAAl6ts3i2L7tGZSbzeC8lhzQthnplYi1Y3Zjrht7INc+N5vHPljOmQd3SfahOOdi7DNBmNnLkhoArwJZwMlmtjDpkbl9ksTFI7vz48c/5rVP1nFM3/ZJ36eZsXrLDgqXF32VEGav2sLOPaUAtGnWkMF52Xx/aGcG52UzoHM2zRpWfajr9INyeWnWan43cT6je7WlU3bjRB2Kc24fKuxikvRXvi7FLeA7wGeEhfpq46Wu9a2LCWBPSSnf+ePbtG6WyXOXHpbwbpgvdu5h1spwIHn5ZgpXFLG+eCcAmRlp9O/UIjwzCLqLOmU3TngMKzZt55g/T2Fol5Y8esEw72pyLoGq2sVU/pO2fn3y1hFBEb+u/OrFuXy4dDPDuraq8rZKS41FG7ZRuLyIj8Ozg4Xriim70rRrm6YcfkCbr5JB7/bNycxI/pXSua2a8PMxvbn+xbk8PX0lpx2Uu++VnHPVVmGCMLOyx4IiqTGQZ2YLaiQqt1/GDc3l9tc/5Z63P9uvBLFx286wqyhICLNWbKF45x4AmjfKYFBeS47p255BedkM6pxNy6aZyTqEfTrr4C68PGsNN708j5E9c2jf4tuD2s65xKrMZa4nAH8AMoGu4fOpf2Nm1XnsqEugxpnpnHtoPre/vpCF64rp2S7rW2127ilh7uqt4dlBMH6wYlNwz0FGmujdIYuTBndkcG5LBuVl07V108gGveNJSxO3fH8Ax94xhV88P5sHzi3wribnkqwyo4c3AsMIniSHmRWG9ZhcLXLOoV24++3PuHfKYm4bN4Dlm7Z/dYnpxyuKmLd6C7tLgr6iji0aMSgvm3MOyWdQXjb9OragcWbtvo8CIL9NU64+pjc3vTSP5z9exSlDOkcdknMprTIJYreZbSn3ba00SfG4KmrZNJMfHJTLP99fxn/mr2fTF7sAaJKZTv9OLbhweLevBpPbNa+73TPnHZbPxNlr+PW/5zH8gDa0rcPH4lxtV5kEMVfSD4F0ST2A/wHeS25YriouHtWNheuK6dyyMYNyWzI4L5sebZuRkZ46JbfS08St4wZw3B3v8MsX5nDP2UO9q8m5JKlMgvgxcB2wE3iM4NnQNyUzKFc1HVo05rGLDok6jKTrntOMq47qye8mzeffs9Zw4sCOUYfkXEqqzFfLsWZ2nZkdFL5+CVR5gFpSL0mFMa+tkn5Srs1oSVti2lxf1f251PSjEd0YmJvNDS/OYeO2nVGH41xKqkyCuLaS8yrFzBaY2SAzGwQMBbYDz8dp+k5ZOzP7TVX351JTepr4w7gBfLGzhBsmzI06HOdSUoVdTJLGAMcBnST9JWZRc2BPgvb/XeAzM1uWoO25eqRHuyyuOLIHt01ewAkD1nBsvw5Rh+RcStnbGcRqgrundwAzYl4TgGMStP/TgccrWHaopJmSJknqm6D9uRQzfmQ3+nZszi9fmMPm8Mot51xiVKbc99nAC2ZWHDPveDN7qVo7ljIJklBfM1tXbllzoNTMtkk6DrjDzHpUsJ3xwHiAvLy8ocuW+clIfTNv9VZOvPNdjh/QgT+fPjjqcJyrU/ZWi6kyYxB/Bd6RdGDMvESMCYwBPiqfHADMbKuZbQunJwINJLWJtxEzu9fMCsysICcnJwFhubqmT8fmXHbEAbxQuJrX533rn5NzrooqkyCWABcAz0g6NZyXiAvPz6CC7iVJ7RVe3C5pWBjn5wnYp0tRlx1xAL3bZ/GL52ezZXtqPafbuahUJkGYmX0EjALGS/oDUK26DJKaAkcBz8XMu0TSJeHbccAcSTOBvwCn2776wly9lpmRxh9OHcjnX+zippfnRR2OcymhMgliDYCZbSQYnDagWo8bNbMvzKy1mW2JmXe3md0dTt9pZn3NbKCZHWJmfue226d+nVpwyahuPDNjJW8uWB91OM7VeftMEGY2ttysm8wsdWo3uJTyP9/tQY+2zfjFc7PZusO7mpyrjn1+0Et6TFLzsFtoDjBP0tXJD825/dcwI53bTh3Iuq07+N3ET6IOx7k6rTJnAn3MbCvwPWAS0BU4O5lBOVcdg3KzuWhENx7/YAXvfrox6nCcq7MqkyAaSGpAkCAmmNluvn5WtXO10pVH9aRbm6b87NlZbNuZqBv/natfKpMg7gGWAk2BKZK6AFuTGZRz1dWoQTq3nTqA1Vu+5JZJ86MOx7k6qTKD1H8xs05mdpwFlgFH1EBszlXL0C6tOP+wrvzj/WVM+8xvo3Fuf1VmkLqdpAckTQrf9wHOTXpkziXAT4/pSV6rJvzs2Vls3+VdTc7tj8p0MT1M8JCgsqeyLAR+kqR4nEuoJpkZ3PL94Bndf5i8MOpwnKtTKkwQkspKgbcxs6cIn0NtZnuAkhqIzbmEOLR7a84+pAsPvbeE6Us3RR2Oc3XG3s4gPgh/fiGpNeGVS5IOAbZUuJZztdDPx/SmY4vGXPPMLHbs9u83zlXG3hJEWUG+qwieAdFd0lTgUYLnVDtXZzRtGHQ1Ld74Bbe/5l1NzlVGhU+UA3IkXRVOPw9MJEgaO4EjgVlJjs25hBreow1nDMvlvncWc2y/9gzOaxl1SM7Vans7g0gHmgFZBPdAZITzmoTznKtzrj3uQNo1b8Q1z8xi5x7vanJub/Z2BrHGzBLxYCDnao3mjRrwu1P6c95DH/KXNz7l6mN6Rx2Sc7VWZcYgnEspo3u1ZdzQztz99mJmr/TrLZyryN4SxHdrLArnativxvahddNMrn5mJrv2lEYdjnO1UoUJwsz8gnGXslo0acBvT+7P/LXF3PXmoqjDca5W8gf/uHrryD7t+N6gjtz15iLmrfb6k86V5wnC1Ws3nNCX7CYNuPqZmewu8a4m52JFliAkLZU0W1KhpOlxlkvSXyQtkjRL0pAo4nSprWXTTG46qR9zV2/lnrc/izoc52qVqM8gjjCzQWZWEGfZGKBH+BoP/L1GI3P1xpj+HRjbvwN/eWMRC9cVRx2Oc7VG1Alib04CHg2fQfE+kC2pQ9RBudT065P60qxRBlc/M4s93tXkHBBtgjDgVUkzJI2Ps7wTsCLm/cpw3jdIGi9puqTpGzZsSFKoLtW1adaQG0/sy8wVRTzw7pKow3GuVogyQQw3syEEXUmXSRpZlY2Y2b1mVmBmBTk5OYmN0NUrJwzowNF92vHH1xby2YZtUYfjXOQiSxBmtir8uZ6gGOCwck1WAbkx7zuH85xLCkn838n9aNwgnWuemUVJqUUdknORiiRBSGoqKatsGjgamFOu2QTgnPBqpkOALWa2poZDdfVM26xG3HBCH2Ys28zD7y2NOhznIhXVGUQ74F1JMwkeTPSymb0i6RJJl4RtJgKLgUXAfcD/iyZUV9+cPLgT3+ndltsmz2fpxi+iDse5yMgsdU6jCwoKbPr0b91S4dx+W7tlB0fd/jYHdmjOExcdQlqa1650qUnSjApuNajVl7k6F5n2LRrxq7F9+GDJJv7532VRh+NcJDxBOFeBUws6M7JnDr+fNJ8Vm7ZHHY5zNc4ThHMVkMTvTumPgJ8/N4tU6o51rjI8QTi3F52yG3PtcQcyddHnPP7Bin2v4FwK8QTh3D78cFgeh3ZrzW8nfsKqoi+jDse5GuMJwrl9SEsTt3x/ACWlxi+em+1dTa7e8AThXCXktW7Cz47txdsLN/DMjJVRh+NcjfAE4VwlnXNoPsPyW3HTS/NYt3VH1OE4l3SeIJyrpLQ0ccu4AezcU8p1z3tXk0t9niCc2w9d2zTl6mN68fon63mxcHXU4TiXVJ4gnNtP5x/elSF52dz477msL/auJpe6PEE4t5/S08St4wayfVcJ178w17uaXMryBOFcFRzQthlXHtmTV+au5eXZXoXepSZPEM5V0UUjujKwcwuuf3Eun2/bGXU4ziWcJwjnqigjPY1bxw2keMdubpgwN+pwnEu4jKgDcK4u69U+ix9/pwd/em0hxw9Yy7H92kcdUsKt3bKDj5dvpnBFEfPXFnN033accVCePyOjHvAE4Vw1XTq6O6/MWcsvX5jDwV1b0bJpZtQhVdn2XXuYvXILhSuKKFxRxMfLi1gb3hSYmZ5GuxYNue75DUwoXM3vvz+Arm2aRhyxSyZ/opxzCTB39RZOunMqJwzsyO0/GBR1OJVSWmos3riNj5d/nQwWrCumpDT4TMhr1YTBedkMyg1efTo2JzM9jaenr+Sml+exa08pVx3VkwuHdyUj3Xur66q9PVGuxhOEpFzgUYLnUhtwr5ndUa7NaOBFYEk46zkz+82+tu0JwkXpT68u4C//WcQD5xbw3QPbRR3Ot2z6YheFKzZTuLyIj8MzhOIdewDIapjBwNzsbySE1s0aVritdVt38KsX5vDqvHUM6NyCW74/gAM7NK+pQ3EJVNsSRAegg5l9JCkLmAF8z8zmxbQZDfzUzI7fn217gnBR2rWnlBP++i5FX+7i1StH0aJxg0hjmbdmK4XLN3+VDJZ9HjwVL03Qq33zr5LBkLxsurVptt9jCmbGxNlruWHCHIq27+b/je7OZd85gIYZ6ck4JJcke0sQNT4GYWZrgDXhdLGkT4BOwLy9ruhcLZeZkcZtpw7g5L+9x80vz+PWcQNrZL9mxsrNX/LxiqKvBpPnrtrKrpJSANo1b8jg3JacMSyPQbnZ9O/UgqYNq/9fXxJjB3TgsO6tuenlefzlP4uYNGctt4wbwJC8ltXevotepGMQkvKBKUA/M9saM3808CywElhNcDYR9zpCSeOB8QB5eXlDly3zB8y7aN3yynz+/tZnPHLBMEb1zEn49ot37GbWyi1fJYPCFUVs3LYLgEYN0ujfqQWD81oyKOwy6tCiccJjiOfNBeu57rnZrNm6g/MP68pPj+lJk0y/Dqa2q1VdTF/tWGoGvA3cbGbPlVvWHCg1s22SjgPuMLMe+9qmdzG52mDH7hKO/+u7bN+5h8lXjiSrUdW7mkpKjYXrisOB5CAhfLp+G2X/bbvlNGVwbksG5WUzODebXu2zaBDhgPG2nXu49ZX5PDptGbmtGvO7kwcwvEebyOJx+1brEoSkBsBLwGQz+1Ml2i8FCsxs497aeYJwtcVHyzcz7u/vcfqwPH57cv9Kr7d+6w4+Cq8qKlyxmVkrt7B9VwkA2U0aMDg3m0FhQhjUOZsWTaIb59ibD5Zs4mfPzmLJxi84raAz143tE+mYjKtYrRqDkCTgAeCTipKDpPbAOjMzScMI7vj+vAbDdK5ahuS15MLhXbnvnSUc378Dhx3w7W/RO3aXMGfVlpjLTDezektwz0FGmujTsTmnDu0cnh20pEvrJgT/fWq/YV1bMemKEdzxxqfcO2Uxby3YwE3f68cxfVPvRsJUFsVVTMOBd4DZQGk4+xdAHoCZ3S3pcuBSYA/wJXCVmb23r237GYSrTXbsLmHMHe+wu6SUV34ykvVbd3ydDFZsZv6aYvaE9xx0ym781VVFg/Oy6duxBY0apMbVQHNWbeGaZ2Yxb81WxvbvwI0n9iUnq+JLaF3NqnVdTMniCcLVNh8u3cRp90yjQXoau/YE34eaZqYzoHPMPQd52bTNahRxpMm1u6SUe6cs5o7XP6VJw3SuP74PJw/uVGfOiFKZJwjnIvTw1CXMX1scnh205IC2zUivp3WMFq3fxs+encWMZZsZ3SuHm0/uT6fsmrnKysXnCcI5V2uUlhqPTlvKrZMXIODnY3pz5sFdvPhfRPaWILyAinOuRqWlifMO78rkn4xkSJeW/OrFufzg3ml8tmFb1KG5cjxBOOcikduqCY9eMIzbxg1gwdpixtzxDn97axF7Skr3vbKrEZ4gnHORkcSpBbm8/r+j+G7vttz6ygK+97epzF29JerQHJ4gnHO1QNusRvz9rKH8/cwhrN2ykxPvnMptk+ezY3dJ1KHVa54gnHO1xpj+HXj9qpGcPLgTd735GWP/8g7Tl26KOqx6yxOEc65WyW6SyR9OHcijFwxjx+5STr1nGjdOmMsXO/dEHVq94wnCOVcrjeyZw6tXjuTcQ/N5ZNpSjr59ClMWbog6rHrFE4RzrtZq2jCDG0/sy9MXH0rDBmmc8+AH/PTpmRRt3xV1aPWCJwjnXK1XkN+Kif8zgsuO6M7zH6/iyD9NYdLsNVGHlfI8QTjn6oRGDdK5+pjeTLj8cNo1b8il//qIS/85g/XFO6IOLWV5gnDO1Sl9O7bgxcsO52fH9uaN+es58o9v8/T0FaRS2aDawhOEc67OyUhP49LR3Zl0xQh6tc/i6mdmcc6DH7Bi0/aoQ0spniCcc3VW95xmPDn+UG46qS8fLdvMMX+ewsNTl1Ba6mcTieAJwjlXp6WlibMPzWfylSM5KL8VN/57HqfeM41F64ujDq3O8wThnEsJnVs24eHzD+JPpw3ksw3bOO6Od7nrzUXs9uJ/VeYJwjmXMiRxypDOvHblKI7q047bJi/gxDunMmeVF/+rCk8QzrmUk5PVkLvOHMLdZw1l47adnHTXVH4/yYv/7a9IEoSkYyUtkLRI0s/jLG8o6clw+X8l5UcQpnOujju2X3tev3IU44Z05u63P+O4O97hgyVe/K+yajxBSEoH7gLGAH2AMyT1KdfsQmCzmR0A3A7cUrNROudSRYsmDbhl3AD+eeHB7Cop5bR7pvGrF+awzYv/7VNGBPscBiwys8UAkp4ATgLmxbQ5CbgxnH4GuFOSzO+Ecc5V0fAebXj1ypH8YfJCHnpvCS/NWk2bZg2jDishWjbJ5KlLDk34dqNIEJ2AFTHvVwIHV9TGzPZI2gK0BjaW35ik8cB4gLy8vGTE65xLEU0yM7j+hD6MHdCBf0xbyq4UucKpeaMGSdluFAkioczsXuBegIKCAj/DcM7t09AuLRnapWXUYdR6UQxSrwJyY953DufFbSMpA2gBfF4j0TnnnAOiSRAfAj0kdZWUCZwOTCjXZgJwbjg9DviPjz8451zNqvEupnBM4XJgMpAOPGhmcyX9BphuZhOAB4B/SFoEbCJIIs4552pQJGMQZjYRmFhu3vUx0zuAU2s6Luecc1/zO6mdc87F5QnCOedcXJ4gnHPOxeUJwjnnXFxKpatHJW0AllVx9TbEuVO7jkqVY0mV4wA/ltooVY4DqncsXcwsJ96ClEoQ1SFpupkVRB1HIqTKsaTKcYAfS22UKscByTsW72JyzjkXlycI55xzcXmC+Nq9UQeQQKlyLKlyHODHUhulynFAko7FxyCcc87F5WcQzjnn4vIE4ZxzLq56nyAkHStpgaRFkn4edTxVJelBSeslzYk6luqSlCvpTUnzJM2VdEXUMVWVpEaSPpA0MzyWX0cdU3VISpf0saSXoo6lOiQtlTRbUqGk6VHHUx2SsiU9I2m+pE8kJezZo/V6DEJSOrAQOIrg0acfAmeY2by9rlgLSRoJbAMeNbN+UcdTHZI6AB3M7CNJWcAM4Ht19O8ioKmZbZPUAHgXuMLM3o84tCqRdBVQADQ3s+OjjqeqJC0FCsyszt8oJ+kR4B0zuz98xk4TMytKxLbr+xnEMGCRmS02s13AE8BJEcdUJWY2heDZGXWema0xs4/C6WLgE4LnlNc5FtgWvm0QvurktzJJnYGxwP1Rx+ICkloAIwmeoYOZ7UpUcgBPEJ2AFTHvV1JHP4hSlaR8YDDw34hDqbKwW6YQWA+8ZmZ19Vj+DFwDlEYcRyIY8KqkGZLGRx1MNXQFNgAPhV1/90tqmqiN1/cE4WoxSc2AZ4GfmNnWqOOpKjMrMbNBBM9fHyapznUBSjoeWG9mM6KOJUGGm9kQYAxwWdhFWxdlAEOAv5vZYOALIGFjqfU9QawCcmPedw7nuYiF/fXPAv8ys+eijicRwlP/N4FjIw6lKg4HTgz77p8AviPpn9GGVHVmtir8uR54nqC7uS5aCayMOSt9hiBhJER9TxAfAj0kdQ0Hd04HJkQcU70XDuw+AHxiZn+KOp7qkJQjKTucbkxwQcT8SIOqAjO71sw6m1k+wf+T/5jZWRGHVSWSmoYXPxB2xxwN1Mmr/8xsLbBCUq9w1neBhF3MEckzqWsLM9sj6XJgMpAOPGhmcyMOq0okPQ6MBtpIWgncYGYPRBtVlR0OnA3MDvvuAX4RPsu8rukAPBJeMZcGPGVmdfoS0RTQDng++B5CBvCYmb0SbUjV8mPgX+GX3MXA+YnacL2+zNU551zF6nsXk3POuQp4gnDOOReXJwjnnHNxeYJwzjkXlycIl3IkdZR0dpL38UNJecncx/6Q1EzSZVHH4VJLvb7M1dVdkloDb4Rv2wMlBCUHILjU7/IE7ScfeCm2AKKkC4EsM1texW0uJfGF4m4GXk7g9pzzBOHqJjP7HBgEIOlGYJuZ/aGG9l2r7i+R1AR418xejToWl1q8i8mlDElDJb0dFmCbLKmDpN6SPohpky9pdkXtY+bPlDQTuCxm3XRJt0n6UNIsSReH8ztImhI+W2COpBHh/KMlTZP0kaSnw9pSsfE2ljRJ0kXl5qdLejjc1mxJV4bdZoUxrxJJXSSdQFC+4zpJr0tqF27jRkmPSHpH0jJJp0i6NdzeK2EpEyRdHx7PHEn3hnexOwd4gnCpQ8BfgXFmNhR4ELjZzOYDmZK6hu1+ADwZfkB+q33Y5iHgx2Y2sNw+LgS2mNlBwEHAReF2fwhMDgvyDQQKJbUBfgkcGRaFmw5cFbOtZsC/gcfN7L5y+xkEdDKzfmbWH3jIzFab2aBwH/cBz5rZMoLnSxwSzn+aoNpqme7Ad4ATgX8Cb4bb+5KgbDfAnWZ2UNiF1hios894cInnXUwuVTQE+gGvhV+C04E14bKnCBLD78OfPwB6xWsf1k3KDp+vAfAPgoqfENTsGSBpXPi+BdCDoKbXg2HSecHMCiWNAvoAU8PtZwLTYuJ9EbjVzP4V51gWA90k/ZVgXOGrriNJhwMXAcPDWR2BR8O4m/D1OAzAJDPbHZ4xpQNl5SRmA/nh9BGSrgnXbQXMJUhcznmCcClDwFwzi/e4xSeBpyU9R/AMn08l9Y/Xvqyw3l728WMzm/ytBUG56LHAw5L+BGwmePbDGRVsaypwrKTHrFy9GzPbLGkgcAxwCXAacEHYBfYAcGLMQ4juBG4xs1ckHQHcELOpneH2SiXtjtlPKZAhqRHwN4IB8xXhWE6jvRy/q2e8i8mlip1AjsLn8UpqIKkvgJl9RnCV068IkgXAgnjtw5LcRZLKvqGfGbOPycClMf33PRVUBu0CrAu7iu4nKLf8PnC4pAPCtk0l9YzZ1vUESeSu8gcSdk+lmdmzBN1UQ8J9Pg38zMwWxjRvyddnDefux+8Lvk4GG8PxkXF7a+zqH08QLlWUEnzA3RIOLhcCh8UsfxI4i6C7ifARsxW1Px+4S0El2dhB2/sJSil/JGkOcA/BWfhoYKakjwm6r+4wsw3AecDjkmYRdC/1LhfzFUBjSbeWm98JeCvc/z+Ba8PYCoBfxwxUdwR+AzwjaQbf7F7apzAZ3kdQ6noyQVeZc1/xaq7OOefi8jMI55xzcXmCcM45F5cnCOecc3F5gnDOOReXJwjnnHNxeYJwzjkXlycI55xzcf1/rl7/x/OmmbkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plots the number of errors accross the dataset (i.e. 40 user)\n",
    "y = [errs[::2].count(0),errs[::2].count(1),errs[::2].count(2),errs[::2].count(3),errs[::2].count(4),errs[::2].count(5),errs[::2].count(6)]\n",
    "plt.plot([0,1,2,3,4,5,6],y)\n",
    "plt.title('Tvedsek a teszthalmazon')\n",
    "plt.xlabel('Tvedsek szma')\n",
    "plt.ylabel('Tesztek szma')\n",
    "plt.savefig(os.path.join(dir_to_save,\"plot_errs\" + str(lr) + \".png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b7fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
